@article{Aggarwal2015,
abstract = {Tactile sensors, because of their intrinsic insensitivity to lighting conditions and water turbidity, provide promis- ing opportunities for perception and object recognition in underwater and deep-sea environments. However, the limited availability of tactile sensors for underwater use has led to limited research in this domain. Recently, wehave developed a deep-sea-capable tactile sensing system,with high spatial and force resolutions,which has made underwater haptic exploration possible for the first time. This paper presents a tactile sensor-based object recognition and localizationmethodology for structured underwater and deep-sea applications. Our approach is based on databasematching using a local feature-based Random Sampling and Consensus (RANSAC) algo- rithm, and sequentially evolving the resulting hypotheses over the course of object exploration. It can handle a large database of three-dimensional objects of complex shapes, and it performs a complete six degree of freedom localization of a static object. An approach to utilize both contact and free-space measurements is presented. Extensive experimentation is performed in underwater environments for validating both the sensor system and the algorithms. To our knowledge, this is the first instance of haptic object recognition and localization in underwater and deep-sea environments.},
annote = {Uses RANSAC to evaluate matches between sets of tactile features to a number of running hypotheses. They also combine this method with voxel respresentations to keep track of non-object space. Finally it is merged with a form of ICP. The system achieves $\backslash$80{\%} recognition with fewer than 10 touches, from a database of 45 objects. Object models need to be known in advance, which in their context (deep sea object recognition) is not a major limitiation.},
author = {Aggarwal, Achint and Kampmann, Peter and Lemburg, Johannes and Kirchner, Frank},
doi = {10.1002/rob.21538},
isbn = {9783902661623},
issn = {15564967},
journal = {Journal of Field Robotics},
keywords = {Folder - TOR by BOF},
language = {en},
mendeley-tags = {Folder - TOR by BOF},
month = {aug},
number = {1},
pages = {167--185},
title = {{Haptic object recognition in underwater and deep-sea environments}},
url = {http://onlinelibrary.wiley.com/doi/10.1002/rob.21538/abstract},
volume = {32},
year = {2015}
}
@inproceedings{Alcantarilla2012,
abstract = {In this paper, we introduce the concept of dense scene flow for visual SLAM applications. Traditional visual SLAM methods assume static features in the environment and that a dominant part of the scene changes only due to camera egomotion. These assumptions make traditional visual SLAM methods prone to failure in crowded real-world dynamic environments with many independently moving objects, such as the typical environments for the visually impaired. By means of a dense scene flow representation, moving objects can be detected. In this way, the visual SLAM process can be improved considerably, by not adding erroneous measurements into the estimation, yielding more consistent and improved localization and mapping results. We show large-scale visual SLAM results in challenging indoor and outdoor crowded environments with real visually impaired users. In particular, we performed experiments inside the Atocha railway station and in the city-center of Alcalá de Henares, both in Madrid, Spain. Our results show that the combination of visual SLAM and dense scene flow allows to obtain an accurate localization, improving considerably the results of traditional visual SLAM methods and GPS-based approaches. View full abstract},
annote = {Uses a measure of motion likelihood to detect moving points and removes them from the SLAM.---------- Relative camera motion detected using matching of Harris(det)-MUSURF(desc).----------Improves stereo visual slam to include dense scene flow. 
---------- 2D flow using Farneback.},
author = {Alcantarilla, Pablo F. and Yebes, Jos{\'{e}} J. and Almaz{\'{a}}n, Javier and Bergasa, Luis M.},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2012.6224690},
isbn = {9781467314039},
issn = {10504729},
keywords = {Alcal{\'{a}} de Henares,Atocha railway station,Cameras,Feature extraction,Folder - scene flow - dense scene flow,GPS-based approach,Global Positioning System,Madrid,Optical imaging,Robustness,Spain,Vectors,Visualization,camera egomotion,crowded real-world dynamic environments,dense scene flow representation,handicapped aids,image motion analysis,localization-mapping robustness,moving object detection,object detection,simultaneous localization and mapping,static features,visual SLAM applications,visually impaired users},
mendeley-tags = {Alcal{\'{a}} de Henares,Atocha railway station,Cameras,Feature extraction,Folder - scene flow - dense scene flow,GPS-based approach,Global Positioning System,Madrid,Optical imaging,Robustness,Spain,Vectors,Visualization,camera egomotion,crowded real-world dynamic environments,dense scene flow representation,handicapped aids,image motion analysis,localization-mapping robustness,moving object detection,object detection,simultaneous localization and mapping,static features,visual SLAM applications,visually impaired users},
month = {may},
pages = {1290--1297},
pmid = {6224690},
title = {{On combining visual SLAM and dense scene flow to increase the robustness of localization and mapping in dynamic environments}},
url = {http://ieeexplore.ieee.org/ielx5/6215071/6224548/06224690.pdf?tp={\&}arnumber=6224690{\&}isnumber=6224548 http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6224690},
year = {2012}
}
@article{Allen1988,
abstract = {A robotic system for object recognition is described that uses passive stereo vision and active exploratory tactile sensing. The complementary nature of these sensing modalities allows the system to discover the underlying 3-D structure of the objects to be recognized. This structure is embodied in rich, hierarchical, viewpoint-independent 3-D models of the objects which include curved surfaces, concavities and holes. The vision processing provides sparse 3-D data about regions of interest that are then actively explored by the tactile sensor mounted on the end of a six-degree-of-freedom manipulator. A robust, hierarchical procedure has been developed to integrate the visual and tactile data into accurate 3-D surface and feature primitives. This integration of vision and touch provides geometric measures of the surfaces and features that are used in a matching phase to find model objects that are consistent with the sensory data. Methods for verification of the hypothesis are presented, including the sensing of visually occluded areas with the tactile sensor. A number of experiments have been performed using real sensors and real, noisy data to demonstrate the utility of these methods and the ability of such a system to recognize objects that would be difficult for a system using vision alone.},
annote = {model based----------Early attempts used vision to guide a series of EBs and combines vision and touch to create a modular geometry-based model of an object. Once possible matchings are identified, the robot proceeds to verify a hypothesis by sensing parts of the object which are yet unseen.},
author = {Allen, Peter K.},
issn = {02783649},
journal = {International Journal of Robotics Research},
keywords = {Computer science,Folder - Haptic-Visual integration},
mendeley-tags = {Computer science,Folder - Haptic-Visual integration},
pages = {15--33},
title = {{Integrating vision and touch for object recognition tasks}},
url = {http://academiccommons.columbia.edu/item/ac:141203 http://academiccommons.columbia.edu/download/fedora{\_}content/download/ac:141204/CONTENT/CUCS-240-86.pdf},
volume = {7},
year = {1988}
}
@article{Allen1988a,
abstract = {A robotic system for object recognition is described that uses passive stereo vision and active exploratory tactile sensing. The complementary nature of these sensing modalities allows the system to discover the underlying 3-D structure of the objects to be recognized. This structure is embodied in rich, hierarchical, viewpoint-independent 3-D models of the objects which include curved surfaces, concavities and holes. The vision processing provides sparse 3-D data about regions of interest that are then actively explored by the tactile sensor mounted on the end of a six-degree-of-freedom manipulator. A robust, hierarchical procedure has been developed to inte grate the visual and tactile data into accurate 3-D surface and feature primitives. This integration of vision and touch provides geometric measures of the surfaces and features that are used in a matching phase to find model objects that are consistent with the sensory data. Methods for verification of the hypothesis are presented, including the sensing of visually occluded areas with the tactile sensor. A number of experi ments have been performed using real sensors and real, noisy data to demonstrate the utility of these methods and the ability of such a system to recognize objects that would be difficult for a system using vision alone.},
author = {Allen, Peter K.},
doi = {10.1177/027836498800700603},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
language = {en},
month = {dec},
number = {6},
pages = {15--33},
title = {{Integrating Vision and Touch for Object Recognition Tasks}},
url = {http://ijr.sagepub.com/content/7/6/15 http://ijr.sagepub.com/content/7/6/15.short http://ijr.sagepub.com/cgi/doi/10.1177/027836498800700603},
volume = {7},
year = {1988}
}
@article{Allen1999,
abstract = {Most robotic hands are either sensorless or lack the ability to report accurate position and force information relating to contact. This paper describes a robotic hand system that uses a limited set of native joint position and force sensing along with custom designed tactile sensors and real-time vision modules to accurately compute finger contacts and applied forces for grasping tasks. A number of experiments are described that show how these sensors can be combined to increase the capabilities of a robotic hand for grasping and manipulation tasks.},
annote = {use vision together with haptic sensors to estimate parameters of a kinematic model for hand-object interactions.},
author = {Allen, Peter K. and Miller, Andrew T. and Oh, Paul Y. and Leibowitz, Brian S.},
journal = {International Journal of Intelligent Machines},
keywords = {Folder - Haptic-Visual integration},
mendeley-tags = {Folder - Haptic-Visual integration},
number = {October 2015},
pages = {129--149},
title = {{Integration of vision, force and tactile sensing for grasping}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.118.6441{\&}rep=rep1{\&}type=pdf http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.118.6441},
volume = {4},
year = {1999}
}
@article{Arbel??ez2011,
abstract = {This paper investigates two fundamental problems in computer vision: contour detection and image segmentation. We present state-of-the-art algorithms for both of these tasks. Our contour detector combines multiple local cues into a globalization framework based on spectral clustering. Our segmentation algorithm consists of generic machinery for transforming the output of any contour detector into a hierarchical region tree. In this manner, we reduce the problem of image segmentation to that of contour detection. Extensive experimental evaluation demonstrates that both our contour detection and segmentation methods significantly outperform competing algorithms. The automatically generated hierarchical segmentations can be interactively refined by user-specified annotations. Computation at multiple image resolutions provides a means of coupling our system to recognition applications.},
annote = {Create a hierarchical graph representation of an image by performing segmentation and joining regions into macro regions at multiple layers.},
author = {Arbel??ez, Pablo and Maire, Michael and Fowlkes, Charless and Malik, Jitendra},
doi = {10.1109/TPAMI.2010.161},
isbn = {0162-8828},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Algorithms,Animals,Benchmark testing,Cluster Analysis,Computer vision,Contour detection,Detectors,Histograms,Humans,Image Processing- Computer-Assisted,Image edge detection,Image segmentation,Pixel,computer vision.,contour detection,edge detection,hierarchical image segmentation,hierarchical region tree,image segmentation,object detection,pattern clustering,spectral clustering,trees (mathematics)},
mendeley-tags = {Algorithms,Animals,Benchmark testing,Cluster Analysis,Computer vision,Detectors,Histograms,Humans,Image Processing- Computer-Assisted,Image edge detection,Image segmentation,Pixel,computer vision.,contour detection,edge detection,hierarchical image segmentation,hierarchical region tree,object detection,pattern clustering,spectral clustering,trees (mathematics)},
month = {may},
number = {5},
pages = {898--916},
pmid = {20733228},
title = {{Contour detection and hierarchical image segmentation}},
url = {http://ieeexplore.ieee.org/ielx5/34/5735600/05557884.pdf?tp={\&}arnumber=5557884{\&}isnumber=5735600 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5557884{\&}tag=1},
volume = {33},
year = {2011}
}
@article{Assaf2014,
abstract = {Effective tactile sensing for artificial platforms remains an open issue in robotics. This study investigates the performance of a soft biologically-inspired artificial fingertip in active exploration tasks. The fingertip sensor replicates the mechanisms within human skin and offers a robust solution that can be used both for tactile sensing and gripping/manipulating objects. The softness of the optical sensor's contact surface also allows safer interactions with objects. High-level tactile features such as edges are extrapolated from the sensor's output and the information is used to generate a tactile image. The work presented in this paper aims to investigate and evaluate this artificial fingertip for 2D shape reconstruction. The sensor was mounted on a robot arm to allow autonomous exploration of different objects. The sensor and a number of human participants were then tested for their abilities to track the raised perimeters of different planar objects and compared. By observing the technique and accuracy of the human subjects, simple but effective parameters were determined in order to evaluate the artificial system's performance. The results prove the capability of the sensor in such active exploration tasks, with a comparable performance to the human subjects despite it using tactile data alone whereas the human participants were also able to use proprioceptive cues. {\textcopyright} 2014 by the authors; licensee MDPI, Basel, Switzerland.},
annote = {Reconstructing 2D shapes from the autonomous exploration and feature extraction.},
author = {Assaf, Tareq and Roke, Calum and Rossiter, Jonathan and Pipe, Tony and Melhuish, Chris},
doi = {10.3390/s140202561},
isbn = {1424-8220},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {Folder - TACTIP,Object features,Optical-based tactile sensor,Real-time processing,Shape recognition,Touch sensor,object features,optical-based tactile sensor,real-time processing,shape recognition,touch sensor},
language = {en},
mendeley-tags = {Folder - TACTIP,object features,optical-based tactile sensor,real-time processing,shape recognition,touch sensor},
month = {feb},
number = {2},
pages = {2561--2577},
pmid = {24514881},
shorttitle = {Seeing by Touch},
title = {{Seeing by touch: Evaluation of a soft biologically-inspired artificial fingertip in real-time active touch}},
url = {http://www.mdpi.com/1424-8220/14/2/2561 http://www.mdpi.com/1424-8220/14/2/2561/pdf},
volume = {14},
year = {2014}
}
@inproceedings{Aubry2015,
abstract = {We introduce an approach for analyzing the variation of features generated by convolutional neural networks (CNNs) with respect to scene factors that occur in natural images. Such factors may include object style, 3D viewpoint, color, and scene lighting configuration. Our approach analyzes CNN feature responses corresponding to different scene factors by controlling for them via rendering using a large database of 3D CAD models. The rendered images are presented to a trained CNN and responses for different layers are studied with respect to the input scene factors. We perform a decomposition of the responses based on knowledge of the input scene factors and analyze the resulting components. In particular, we quantify their relative importance in the CNN responses and visualize them using principal component analysis. We show qualitative and quantitative results of our study on three CNNs trained on large image datasets: AlexNet, Places, and Oxford VGG. We observe important differences across the networks and CNN layers for different scene factors and object categories. Finally, we demonstrate that our analysis based on computer-generated imagery translates to the network representation of natural images.},
annote = {Change illumination, viewpoint, colour, style for same object and see how a Deep NN reacts.},
author = {Aubry, Mathieu and Russell, Bryan C.},
booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2015.329},
file = {:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Aubry, Russell - 2015 - Understanding Deep Features with Computer-Generated Imagery.pdf:pdf;:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Aubry, Russell - 2015 - Understanding Deep Features with Computer-Generated Imagery.html:html},
isbn = {978-1-4673-8391-2},
keywords = {Computer Science - Computer Vision and Pattern Rec,Folder - convolutional - visualise},
mendeley-tags = {Computer Science - Computer Vision and Pattern Rec,Folder - convolutional - visualise},
month = {jun},
pages = {2875--2883},
title = {{Understanding Deep Features with Computer-Generated Imagery}},
url = {http://arxiv.org/abs/1506.01151 http://www.arxiv.org/pdf/1506.01151.pdf http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7410686},
year = {2015}
}
@inproceedings{Barron-Gonzalez2013,
address = {Karlsruhe, Germany},
annote = {recognise actions received such as poking, caressing and slapping, using the sensors of an iCub.},
author = {Barron-Gonzalez, Hector and Prescott, Tony},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-662-43645-5_6},
file = {:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Barron-Gonzalez, Prescott - 2014 - Discrimination of social tactile gestures using biomimetic skin.pdf:pdf},
isbn = {9783662436448},
issn = {16113349},
keywords = {HRI,Social robotics,Tactile gesture recognition},
pages = {46--48},
title = {{Discrimination of social tactile gestures using biomimetic skin}},
url = {file:///home/tadeo/Dropbox/phd/papers/TAROS2013{\_}059{\_}final{\_}v1.pdf},
volume = {8069 LNAI},
year = {2014}
}
@phdthesis{Beale2012,
abstract = {This thesis investigates the problem of visual learning using a robotic platform. Given a set of objects the robots task is to autonomously manipulate, observe, and learn. This allows the robot to recognise objects in a novel scene and pose, or separate them into distinct visual categories. The main focus of the work is in autonomously acquiring object models using robotic manipulation. Autonomous learning is important for robotic systems. In the context of vision, it allows a robot to adapt to new and uncertain environments, updating its internal model of the world. It also reduces the amount of human supervision needed for building visual models. This leads to machines which can operate in environments with rich and complicated visual information, such as the home or industrial workspace; also, in environments which are potentially hazardous for humans. The hypothesis claims that inducing robot motion on objects aids the learning process. It is shown that extra information from the robot sensors provides enough information to localise an object and distinguish it from the background. Also, that decisive planning allows the object to be separated and observed from a variety of dierent poses, giving a good foundation to build a robust classication model. Contributions include a new segmentation algorithm, a new classication model for object learning, and a method for allowing a robot to supervise its own learning in cluttered and dynamic environments.},
author = {Beale, Dan},
month = {mar},
pages = {197},
title = {{Autonomous Visual Learning for Robotic Systems}},
url = {http://opus.bath.ac.uk/32210/ http://opus.bath.ac.uk/32210/1/UnivBath{\_}PhD{\_}2012{\_}D{\_}Beale.pdf},
year = {2012}
}
@article{Behrmann2003,
abstract = {This article explores expertise in tactile object recognition. In one study, participants were trained to differing degrees of accuracy on tactile identification of two-dimensional patterns. Recognition of these patterns, of inverted versions of these patterns, and of sub-parts of these patterns was then tested. The inversion effect (better recognition of upright than inverted patterns) and the part-whole effect (better recognition of the whole than a part pattern), traditionally considered signatures of visual expertise, were observed for tactile experts but not for novices. In a second study, participants were trained as visual or tactile experts and then tested in the trained and non-trained modalities. Whereas expertise effects were observed in the modality of training, cross-modal transfer was asymmetric; visual experts showed generalization to haptic recognition, but tactile experts did not show generalization to visual recognition. Tactile expertise is not obviously attributable to visual mediation and emerges from domain-general principles that operate independently of modality. (PsycINFO Database Record (c) 2012 APA, all rights reserved)(journal abstract)},
annote = {Haptic information may not be used to inform visual expertise.},
author = {Behrmann, Marlene and Ewell, Catherine},
doi = {10.1111/1467-9280.02458},
isbn = {0956-7976},
issn = {09567976},
journal = {Psychological Science},
keywords = {Folder - Haptic-Visual integration - bio motivatio},
language = {en},
mendeley-tags = {Folder - Haptic-Visual integration - bio motivatio},
month = {sep},
number = {5},
pages = {480--486},
pmid = {12930480},
title = {{Expertise in tactile pattern recognition}},
url = {http://pss.sagepub.com/content/14/5/480 http://pss.sagepub.com/content/14/5/480.abstract http://www.ncbi.nlm.nih.gov/pubmed/12930480},
volume = {14},
year = {2003}
}
@inproceedings{Bekiroglu2010,
abstract = {In this paper, the problem of learning grasp stability in robotic object grasping based on tactile measurements is studied. Although grasp stability modeling and estimation has been studied for a long time, there are few robots today able of demonstrating extensive grasping skills. The main contribution of the work presented here is an investigation of probabilistic modeling for inferring grasp stability based on learning from examples. The main objective is classification of a grasp as stable or unstable before applying further actions on it, e.g. lifting. The problem cannot be solved by visual sensing which is typically used to execute an initial robot hand positioning with respect to the object. The output of the classification system can trigger a regrasping step if an unstable grasp is identified. An off-line learning process is implemented and used for reasoning about grasp stability for a three-fingered robotic hand using Hidden Markov models. To evaluate the proposed method, experiments are performed both in simulation and on a real robot system.},
author = {Bekiroglu, Yasemin and Kragic, Danica and Kyrki, Ville},
booktitle = {Proceedings - IEEE International Workshop on Robot and Human Interactive Communication},
doi = {10.1109/ROMAN.2010.5598659},
isbn = {9781424479917},
issn = {1944-9445},
keywords = {Grasping,Shape,Stability analysis,dexterous manipulators,hidden Markov models,learning (artificial intelligence),learning grasp stability,offline learning process,probabilistic modeling,probability,robot hand positioning,robotic object grasping,stability,tactile data,tactile measurements,tactile sensors,three-fingered robotic hand},
mendeley-tags = {Grasping,Shape,Stability analysis,dexterous manipulators,hidden Markov models,learning (artificial intelligence),learning grasp stability,offline learning process,probabilistic modeling,probability,robot hand positioning,robotic object grasping,stability,tactile data,tactile measurements,tactile sensors,three-fingered robotic hand},
month = {sep},
pages = {132--137},
title = {{Learning grasp stability based on tactile data and HMMs}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp={\&}arnumber=5598659{\&}queryText{\%}3DLearning+grasp+stability+based+on+tactile+data+and+HMMs},
year = {2010}
}
@article{Bekiroglu2011,
abstract = {An important ability of a robot that interacts with the environment and manipulates objects is to deal with the uncertainty in sensory data. Sensory information is necessary to, for example, perform online assessment of grasp stability. We present methods to assess grasp stability based on haptic data and machine-learning methods, including AdaBoost, support vector machines (SVMs), and hidden Markov models (HMMs). In particular, we study the effect of different sensory streams to grasp stability. This includes object information such as shape; grasp information such as approach vector; tactile measurements from fingertips; and joint configuration of the hand. Sensory knowledge affects the success of the grasping process both in the planning stage (before a grasp is executed) and during the execution of the grasp (closed-loop online control). In this paper, we study both of these aspects. We propose a probabilistic learning framework to assess grasp stability and demonstrate that knowledge about grasp stability can be inferred using information from tactile sensors. Experiments on both simulated and real data are shown. The results indicate that the idea to exploit the learning approach is applicable in realistic scenarios, which opens a number of interesting venues for the future research.},
annote = {tactile databse},
author = {Bekiroglu, Yasemin and Laaksonen, Janne and J{\o}rgensen, Jimmy Alison and Kyrki, Ville and Kragic, Danica},
doi = {10.1109/TRO.2011.2132870},
isbn = {1552-3098 VO - 27},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {AdaBoost,Force and tactile sensing,Grasping,HMM,SVM,Stability analysis,dexterous manipulators,force and tactile sensing,grasp information,grasp stability,grasping,haptic data,haptic interfaces,hidden Markov models,learning (artificial intelligence),learning and adaptive systems,machine learning methods,object information,probabilistic learning framework,robot,sensory data,sensory information,support vector machines,tactile measurements,tactile sensors},
mendeley-tags = {AdaBoost,Grasping,HMM,SVM,Stability analysis,dexterous manipulators,force and tactile sensing,grasp information,grasp stability,haptic data,haptic interfaces,hidden Markov models,learning (artificial intelligence),learning and adaptive systems,machine learning methods,object information,probabilistic learning framework,robot,sensory data,sensory information,support vector machines,tactile measurements,tactile sensors},
month = {jun},
number = {3},
pages = {616--629},
title = {{Assessing grasp stability based on learning and haptic data}},
url = {http://ieeexplore.ieee.org/ielx5/8860/5784192/05759756.pdf?tp={\&}arnumber=5759756{\&}isnumber=5784192 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5759756},
volume = {27},
year = {2011}
}
@article{Bengio1994,
abstract = {Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered.},
annote = {NN suffer from the vanishing gradient problem},
archivePrefix = {arXiv},
arxivId = {arXiv:1211.5063v2},
author = {Bengio, Yoshua and Simard, Patrice and Frasconi, Paolo},
doi = {10.1109/72.279181},
eprint = {arXiv:1211.5063v2},
isbn = {1045-9227 VO  - 5},
issn = {19410093},
journal = {IEEE Transactions on Neural Networks},
month = {mar},
number = {2},
pages = {157--166},
pmid = {18267787},
title = {{Learning Long-Term Dependencies with Gradient Descent is Difficult}},
url = {http://dx.doi.org/10.1109/72.279181},
volume = {5},
year = {1994}
}
@article{Bezdek1973,
abstract = {Abstract Given a finite, unlabelled set of real vectors X, one often presumes the existence of (c) subsets (clusters) in X, the members of which somehow bear more similarity to each other than to members of adjoining clusters. In this paper, we use membership function matrices associated with fuzzy c-partitions of X, together with their values in the Euclidean (matrix) norm, to formulate an a posteriori method for evaluating algorithmically suggested clusterings of X. Several numerical examples are offered in support of the proposed technique. Given a finite, unlabelled set of real vectors X, one often presumes the existence of (c) subsets (clusters) in X, the members of which somehow bear more similarity to each other than to members of adjoining clusters. In this paper, we use membership function matrices associated with fuzzy c-partitions of X, together with their values in the Euclidean (matrix) norm, to formulate an a posteriori method for evaluating algorithmically suggested clusterings of X. Several numerical examples are offered in support of the proposed technique.},
author = {Bezdek, James C.},
doi = {10.1080/01969727308546047},
isbn = {0022-0280},
issn = {0022-0280},
journal = {Journal of Cybernetics},
keywords = {Folder - Clustering},
mendeley-tags = {Folder - Clustering},
month = {jan},
number = {March 2015},
pages = {58--73},
title = {{Cluster Validity with Fuzzy Sets}},
url = {http://dx.doi.org/10.1080/01969727308546047 http://www.tandfonline.com/doi/abs/10.1080/01969727308546047{\#}.U{\_}7whGOookk},
volume = {3},
year = {1974}
}
@inproceedings{Bierbaum2007,
abstract = {In order for humanoid robots to enter human-centered environments, it is indispensable to equip them with the ability to recognize and classify objects in such an environment. A promising way to acquire the object models necessary for object manipulation appears in the supplement of the information gathered by computer vision techniques with data from haptic exploration. In this paper we present a framework for haptic exploration which is intended for use with both, a five-finger humanoid robot hand as well as with a human hand. We describe experiments and results on haptic exploration and shape estimation of 2D and 3D objects by the human hand. Volumetric shape data is acquired by a human operator hand using a data glove. The exploring human hand is located by a stereo camera system, whereas the finger configuration is calculated from the glove data.},
annote = {Uses vision to track a human operated glove which is sensing and following the contour of an object, to create a volumetric representation of it.},
author = {Bierbaum, A. and Welke, K. and Burger, D. and Asfour, T. and Dillmann, R.},
booktitle = {Proceedings of the 2007 7th IEEE-RAS International Conference on Humanoid Robots, HUMANOIDS 2007},
doi = {10.1109/ICHR.2007.4813935},
isbn = {9781424418626},
keywords = {3D shape reconstruction,Computer science,Computer vision,Data gloves,Fingers,Folder - Haptic-Visual integration,Humans,Shape,Stereo vision,computer vision techniques,five-finger hands,five-finger humanoid robot,glove data,haptic exploration,haptic interfaces,human-centered environments,humanoid robots,image reconstruction,manipulators,object classification,object manipulation,object recognition,robot vision,stereo camera system},
mendeley-tags = {3D shape reconstruction,Computer science,Computer vision,Data gloves,Fingers,Folder - Haptic-Visual integration,Humans,Shape,Stereo vision,computer vision techniques,five-finger hands,five-finger humanoid robot,glove data,haptic exploration,haptic interfaces,human-centered environments,humanoid robots,image reconstruction,manipulators,object classification,object manipulation,object recognition,robot vision,stereo camera system},
pages = {616--621},
title = {{Haptic exploration for 3D shape reconstruction using five-finger hands}},
url = {http://ieeexplore.ieee.org/ielx5/4807937/4813825/04813935.pdf?tp={\&}arnumber=4813935{\&}isnumber=4813825 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4813935},
year = {2008}
}
@inproceedings{Bierbaum2008,
abstract = {3D shape reconstruction of objects from tactile exploration data acquired by a multi-fingered robot hand is an important skill for a humanoid robot system. Tactile exploration data captured using current robot technology is naturally sparse and noisy, therefore a satisfying shape estimate is difficult to achieve. In this paper we describe a robust approach for 3D shape recovery using superquadric functions, which makes use of both contact location and normal information. We present two quality measures and compare to other relevant estimation techniques using representative synthetic contact data.},
author = {Bierbaum, A and Gubarev, I and Dillmann, R.},
booktitle = {2008 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS},
doi = {10.1109/IROS.2008.4650982},
isbn = {9781424420582},
keywords = {3D shape reconstruction,Estimation,Fingers,Gallium,Shape,Three dimensional displays,dexterous manipulators,haptic exploration,haptic interfaces,humanoid robot system,humanoid robots,multifingered robot hand,robots,robust 3D shape recovery,sparse contact location,superquadric functions,tactile exploration data,tactile sensors},
mendeley-tags = {3D shape reconstruction,Estimation,Fingers,Gallium,Shape,Three dimensional displays,dexterous manipulators,haptic exploration,haptic interfaces,humanoid robot system,humanoid robots,multifingered robot hand,robots,robust 3D shape recovery,sparse contact location,superquadric functions,tactile exploration data,tactile sensors},
month = {sep},
pages = {3200--3205},
title = {{Robust shape recovery for sparse contact location and normal data from haptic exploration}},
url = {http://ieeexplore.ieee.org/ielx5/4637508/4650570/04650982.pdf?tp={\&}arnumber=4650982{\&}isnumber=4650570 http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=4650982{\&}navigation=1},
year = {2008}
}
@article{Bierbaum2007a,
author = {Bierbaum, A and Welke, K and Burger, D},
journal = {Proceedings of the {\ldots}},
title = {{A framework for visually guided haptic exploration with five finger hands}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=AF83D0D84C01D1717D892E8AAEBFA852?doi=10.1.1.378.157{\&}rep=rep1{\&}type=pdf http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.378.157 http://www.researchgate.net/publication/228757094{\_}A{\_}Framework{\_}fo},
year = {2007}
}
@article{Blei2003,
abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
annote = {LDA, first introduced by},
author = {Blei, D and Ng, A and {and M. Jordan}},
issn = {1532-4435},
journal = {Jmlr},
month = {mar},
pages = {993�1022},
title = {{Latent Dirichlet Allocation}},
url = {http://dl.acm.org/citation.cfm?id=944919.944937},
volume = {3},
year = {2003}
}
@inproceedings{Blum2012,
abstract = {In this work we address the problem of feature extraction for object recognition in the context of cameras providing RGB and depth information (RGB-D data). We consider this problem in a bag of features like setting and propose a new, learned, local feature descriptor for RGB-D images, the convolutional k-means descriptor. The descriptor is based on recent results from the machine learning community. It automatically learns feature responses in the neighborhood of detected interest points and is able to combine all avail- able information, such as color and depth into one, concise representation. To demonstrate the strength of this approach we show its applicability to different recognition problems. We evaluate the quality of the descriptor on the RGB-D Object Dataset where it is competitive with previously published results and propose an embedding into an image processing pipeline for object recognition and pose estimation.},
annote = {CKM (Convolutional K-Means). A learned local feature descriptor which is then used across on interest points (SURF used here) to create a descriptor. Unclear if is it invariant to scale or rotation.},
author = {Blum, Manuel and Springenberg, Jost Tobias and W{\"{u}}lfing, Jan and Riedmiller, Martin},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2012.6225188},
isbn = {9781467314039},
issn = {10504729},
keywords = {Accuracy,Feature extraction,Histograms,RGB-D image,RGB-D object dataset,Training,Vectors,concise representation,convolutional k-means descriptor,depth information,image colour analysis,image processing pipeline,image representation,interest point detection,learned feature descriptor,learning (artificial intelligence),machine learning community,object recognition,pose estimation,unsupervised learning},
mendeley-tags = {Accuracy,Feature extraction,Histograms,RGB-D image,RGB-D object dataset,Training,Vectors,concise representation,convolutional k-means descriptor,depth information,image colour analysis,image processing pipeline,image representation,interest point detection,learned feature descriptor,learning (artificial intelligence),machine learning community,object recognition,pose estimation,unsupervised learning},
pages = {1298--1303},
title = {{A learned feature descriptor for object recognition in RGB-D data}},
url = {http://ieeexplore.ieee.org/ielx5/6215071/6224548/06225188.pdf?tp={\&}arnumber=6225188{\&}isnumber=6224548 http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6225188},
year = {2012}
}
@article{Bo2011,
abstract = {Consumer depth cameras, such as the Microsoft Kinect, are capable of providing frames of dense depth values at real time. One fundamental question in utilizing depth cameras is how to best extract features from depth frames. Motivated by local descriptors on images, in particular kernel descriptors, we develop a set of kernel features on depth images that model size, 3D shape, and depth edges in a single framework. Through extensive experiments on object recognition, we show that (1) our local features capture different aspects of cues from a depth frame/view that complement one another; (2) our kernel features signiﬁcantly outperform traditional 3D features (e.g. Spin images); and (3) we signiﬁcantly improve the capabilities of depth and RGB-D (color+depth) recognition, achieving 10−15{\%} improvement in accuracy over the state of the art.},
annote = {Modifies kernel descriptors to be used in depth images encoding information about shape, size and edges.},
author = {Bo, Liefeng and Ren, Xiaofeng and Fox, Dieter},
doi = {10.1109/IROS.2011.6095119},
isbn = {978-1-61284-456-5},
issn = {21530858},
journal = {Int. Symp. on Experimental Robotics (ISER)},
keywords = {Feature extraction,Kernel,Principal component analysis,Shape,Three dimensional displays,Vectors,object recognition},
mendeley-tags = {Feature extraction,Kernel,Principal component analysis,Shape,Three dimensional displays,Vectors,object recognition},
pages = {1--15},
title = {{Unsupervised Feature Learning for RGB-D Based Object Recognition}},
url = {http://ieeexplore.ieee.org/ielx5/6034548/6094399/06095119.pdf?tp={\&}arnumber=6095119{\&}isnumber=6094399 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6095119 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6095119$\backslash$nhttp://homes.cs.washing},
year = {2012}
}
@article{Bo2012,
abstract = {Recently introduced RGB-D cameras are capable of providing high quality synchronized videos of both color and depth. With its advanced sensing capabilities, this technology represents an opportunity to dramatically increase the capabilities of object recognition. It also raises the problem of developing expressive features for the color and depth channels of these sensors. In this paper we introduce hierarchical matching pursuit (HMP) for RGB-D data. HMP uses sparse coding to learn hierarchical feature representations from raw RGB-D data in an unsupervised way. Extensive experiments on various datasets indicate that the features learned with our approach enable superior object recognition results using linear support vector machines.},
annote = {HMP (Hierarchical Matched Pursuit).
Uses RGBD plus normal information to learn features, unsupervised. these are then used together with a learnt dictionary to perform what is termed Hierarchical Matching Pursuit, which involves combining local features to produce higher level features which describe larger scale properties.----------Classification 87.5{\%}. (82 and 81 with rgb and d alone).},
author = {Bo, Liefeng and Ren, Xiaofeng and Fox, Dieter},
doi = {10.1007/978-3-319-00065-7},
editor = {Desai, Jaydev P. and Dudek, Gregory and Khatib, Oussama and Kumar, Vijay},
file = {:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bo, Ren, Fox - 2013 - Unsupervised Feature Learning for RGB-D Based Object Recognition.pdf:pdf;:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bo, Ren, Fox - 2013 - Unsupervised Feature Learning for RGB-D Based Object Recognition.html:html},
isbn = {978-3-319-00064-0},
journal = {ISER, June},
keywords = {Artificial Intelligence (incl. Robotics),Robotics and automation},
mendeley-tags = {Artificial Intelligence (incl. Robotics),Robotics and automation},
pages = {387--402},
publisher = {Springer International Publishing},
series = {Springer Tracts in Advanced Robotics},
title = {{Unsupervised Feature Learning for RGB-D Based Object Recognition}},
url = {http://link.springer.com/chapter/10.1007/978-3-319-00065-7{\_}27 http://link.springer.com/chapter/10.1007{\%}2F978-3-319-00065-7{\_}27 http://link.springer.com/content/pdf/10.1007{\%}2F978-3-319-00065-7{\_}27.pdf http://link.springer.com/10.1007/978-3-319-00065-7$\backslash$nhttp:},
volume = {88},
year = {2013}
}
@article{Bordes2005,
abstract = {Very high dimensional learning systems become theoretically possible when training examples are abundant. The computing cost then becomes the limiting factor. Any efficient learning algorithm should at least take a brief look at each example. But should all examples be given equal attention? This contribution proposes an empirical answer. We first present an online SVM algorithm based on this premise. LASVM yields competitive misclassification rates after a single pass over the training examples, outspeeding state-of-the-art SVM solvers. Then we show how active example selection can yield faster training, higher accuracies, and simpler models, using only a fraction of the training example labels.},
annote = {Size of the SVM QP problem grows at least like n{\^{}}2, as a consequence of [Steinwart 03]},
author = {Bordes, Antoine and Ertekin, Şeyda and Weston, Jason and Bottou, L{\'{e}}on},
doi = {10.1.1.60.9676},
file = {:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bordes et al. - 2005 - Fast Kernel Classifiers with Online and Active Learning.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
month = {dec},
pages = {1579--1619},
title = {{Fast Kernel Classifiers with Online and Active Learning}},
url = {http://dl.acm.org/citation.cfm?id=1046920.1194898 http://dl.acm.org/ft{\_}gateway.cfm?id=1194898{\&}type=pdf},
volume = {6},
year = {2005}
}
@article{Caimo2010,
abstract = {Exponential Random Graph models are an important tool in network analysis for describing complicated dependency structures. However, Bayesian parameter estimation for these models is extremely challenging, since evaluation of the posterior distribution typically involves the calculation of an intractable normalizing constant. This barrier motivates the consideration of tractable approximations to the likelihood function, such as pseudolikelihoods, which offer a principled approach to constructing such an approximation. Naive implementation of a posterior from a misspecified model is likely to give misleading inferences. We provide practical guidelines to calibrate in a quick and efficient manner samples coming from an approximated posterior and discuss the efficiency of this approach. The exposition of the methodology is accompanied by the analysis of real-world graphs. Comparisons against the Approximate Exchange algorithm of Caimo and Friel (2011) are provided, followed by concluding remarks.},
annote = {Comment: 29 pages; Accepted to appear in Social Networks},
archivePrefix = {arXiv},
arxivId = {1510.00934},
author = {Bouranis, Lampros and Friel, Nial and Maire, Florian},
eprint = {1510.00934},
file = {:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bouranis, Friel, Maire - 2015 - Bayesian inference for misspecified exponential random graph models.pdf:pdf;:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bouranis, Friel, Maire - 2015 - Bayesian inference for misspecified exponential random graph models.html:html},
journal = {arXiv:1007.5192 [stat]},
keywords = {Statistics - Applications,exponential random graph models,intractable normalizing constants,large,logistic regression,networks,pseudolikelihood,tractable approximation},
mendeley-tags = {Statistics - Applications},
month = {jul},
pages = {1--21},
title = {{Bayesian inference for misspecified exponential random graph models}},
url = {http://arxiv.org/abs/1007.5192 http://www.arxiv.org/pdf/1007.5192.pdf http://arxiv.org/abs/1510.00934},
year = {2015}
}
@inproceedings{Browatzki2012,
abstract = {Interaction with its environment is a key requisite for a humanoid robot. Especially the ability to recognize and manipulate unknown objects is crucial to successfully work in natural environments. Visual object recognition, however, still remains a challenging problem, as three-dimensional objects often give rise to ambiguous, two-dimensional views. Here, we propose a perception-driven, multisensory exploration and recognition scheme to actively resolve ambiguities that emerge at certain viewpoints. We define an efficient method to acquire two-dimensional views in an object-centered task space and sample characteristic views on a view sphere. Information is accumulated during the recognition process and used to select actions expected to be most beneficial in discriminating similar objects. Besides visual information we take into account proprioceptive information to create more reliable hypotheses. Simulation and real-world results clearly demonstrate the efficiency of active, multisensory exploration over passive, visiononly recognition methods.},
annote = {{\~{}}80{\%} accuracy
Active resolution of ambiguities by choosing a viewing angle that is likely to produce discerning evidence (e.g. identical cups with different inside base tape colour)
Works with a typically small number of rival hypotheses
Only action is choosing a viewpoint},
author = {Browatzki, Bj??rn and Tikhanoff, Vadim and Metta, Giorgio and B??lthoff, Heinrich H. and Wallraven, Christian},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2012.6225218},
isbn = {9781467314039},
issn = {10504729},
keywords = {Entropy,Folder - Active Object Recognition,Joints,Robot sensing systems,Visualization,active object recognition,environment Interaction,humanoid robot,humanoid robots,multisensory exploration,object recognition,object-centered task space,perception-driven multisensory exploration,robot vision,vision-only recognition methods,visual information,visual object recognition},
mendeley-tags = {Entropy,Folder - Active Object Recognition,Joints,Robot sensing systems,Visualization,active object recognition,environment Interaction,humanoid robot,humanoid robots,multisensory exploration,object recognition,object-centered task space,perception-driven multisensory exploration,robot vision,vision-only recognition methods,visual information,visual object recognition},
month = {may},
pages = {2021--2028},
title = {{Active object recognition on a humanoid robot}},
url = {http://ieeexplore.ieee.org/ielx5/6215071/6224548/06225218.pdf?tp={\&}arnumber=6225218{\&}isnumber=6224548 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6225218},
year = {2012}
}
@article{Cai2015,
abstract = {The cross-depiction problem is that of recognising visual objects regardless of whether they are photographed, painted, drawn, etc. It is a potentially significant yet under-researched problem. Emulating the remarkable human ability to recognise objects in an astonishingly wide variety of depictive forms is likely to advance both the foundations and the applications of Computer Vision. In this paper we benchmark classification, domain adaptation, and deep learning methods; demonstrating that none perform consistently well in the cross-depiction problem. Given the current interest in deep learning, the fact such methods exhibit the same behaviour as all but one other method: they show a significant fall in performance over inhomogeneous databases compared to their peak performance, which is always over data comprising photographs only. Rather, we find the methods that have strong models of spatial relations between parts tend to be more robust and therefore conclude that such information is important in modelling object classes regardless of appearance details.},
archivePrefix = {arXiv},
arxivId = {1505.00110},
author = {Cai, Hongping and Wu, Qi and Corradi, Tadeo and Hall, Peter},
eprint = {1505.00110},
journal = {CoRR},
number = {1},
title = {{The Cross-Depiction Problem: Computer Vision Algorithms for Recognising Objects in Artwork and in Photographs}},
url = {http://arxiv.org/abs/1505.00110},
year = {2015}
}
@article{Caimo2011,
abstract = {Exponential random graph models are extremely difficult models to handle from a statistical viewpoint, since their normalising constant, which depends on model parameters, is available only in very trivial cases. We show how inference can be carried out in a Bayesian framework using a MCMC algorithm, which circumvents the need to calculate the normalising constants. We use a population MCMC approach which accelerates convergence and improves mixing of the Markov chain. This approach improves performance with respect to the Monte Carlo maximum likelihood method of Geyer and Thompson (1992). ?? 2010 Elsevier B.V.},
archivePrefix = {arXiv},
arxivId = {arXiv:1007.5192v2},
author = {Caimo, Alberto and Friel, Nial},
doi = {10.1016/j.socnet.2010.09.004},
eprint = {arXiv:1007.5192v2},
file = {:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Caimo, Friel - 2011 - Bayesian inference for exponential random graph models.pdf:pdf;:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Caimo, Friel - 2011 - Bayesian inference for exponential random graph models.html:html},
isbn = {0378-8733},
issn = {03788733},
journal = {Social Networks},
keywords = {Exponential random graph models,Folder - models - bayesian,Markov chain Monte Carlo,Social network analysis},
mendeley-tags = {Exponential random graph models,Folder - models - bayesian,Markov chain Monte Carlo,Social network analysis},
month = {jan},
number = {1},
pages = {41--55},
title = {{Bayesian inference for exponential random graph models}},
url = {http://www.sciencedirect.com/science/article/pii/S0378873310000493 http://www.sciencedirect.com/science/article/pii/S0378873310000493/pdfft?md5=e1e6d390086b522b55c26dc8a8be5788{\&}pid=1-s2.0-S0378873310000493-main.pdf},
volume = {33},
year = {2011}
}
@inproceedings{Caselli1994,
abstract = {A haptic object recognition methodology suitable for application with a multifingered robot hand is presented. The methodology exploits peculiar features of robot hands such as their distributed sensoriality and parallel kinematics for fast acquisition of contact data, and is based on volumetric representation models for efficient dynamic integration of the perceived information. Experimental results demonstrate the effectiveness and applicability of the methodology to non-trivial examples. I. Introduction The potential for tactile sensing and haptic perception in general in robotics is well known. Haptics can provide general {\{}3-D{\}} perceptual information to strengthen or substitute machine vision (if needed) as well as local, relevant knowledge to be exploited in carrying out complex manipulation tasks. In 1984 Ruzena Bajcsy, in an inspiring paper titled "What can we learn from one finger experiments?" [3], investigated the properties that could be inferred from the exploration of an o...},
annote = {recognition of objects can be performed using touch by creating a polyhedral 3D representation of the object},
author = {Caselli, Stefano and Magnanini, Corrado and Zanichelli, Francesco},
booktitle = {IEEE Int. Conf. on Multisensor Fusion and Integration, Las Vegas, NV},
doi = {10.1109/MFI.1994.398442},
isbn = {0-7803-2072-7},
keywords = {Fingers,Kinematics,Robot sensing systems,Sensor arrays,Sensor phenomena and characterization,Shape,contact data acquisition,data acquisition,dextrous hand,distributed sensoriality,haptic interfaces,haptic object recognition,manipulators,multifingered robot hand,object recognition,object{\_}recognition,parallel kinematics,sensor fusion,tactile sensing,tactile sensors,volumetric shape representations},
mendeley-tags = {Fingers,Kinematics,Robot sensing systems,Sensor arrays,Sensor phenomena and characterization,Shape,contact data acquisition,data acquisition,dextrous hand,distributed sensoriality,haptic interfaces,haptic object recognition,manipulators,multifingered robot hand,object recognition,parallel kinematics,sensor fusion,tactile sensing,tactile sensors,volumetric shape representations},
pages = {2--5},
title = {{Haptic Object Recognition with a Dextrous Hand Based on Volumetric Shape Representations}},
url = {http://ieeexplore.ieee.org/ielx2/3176/9001/00398442.pdf?tp={\&}arnumber=398442{\&}isnumber=9001 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=398442{\&}tag=1 http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.51.1953},
year = {1994}
}
@article{Cauwenberghs2000,
abstract = {An on-line recursive algorithm for training support vector machines, one vector at a time, is presented. Adiabatic increments retain the KuhnTucker conditions on all previously seen training data, in a number of steps each computed analytically. The incremental procedure is reversible, and decremental "unlearning" offers an efficient method to exactly evaluate leave-one-out generalization performance. Interpretation of decremental unlearning in feature space sheds light on the relationship between generalization and geometry of the data. 1},
annote = {Given the 1-Norm Soft Margin SVM solution for a dataset, and a new datum to be added to this set, it is possible to modify the solution by virtue of maintaining in memory the matrix inverse of the kernel matrix, and updating the inverse directly to calculate the adjustments to be made to the Lagrange coeffiecients in such a way that the KKT conditions are not violated. New datum which would be misclassified by the previous solution are thus added to the support vectors or to the error vectors, and "adiabatic" perturbations are made to the Lagrange coefficients and to the bias to obtain a new solution.},
author = {Cauwenberghs, Gert and Poggio, Tomaso},
doi = {10.1.1.21.1720},
file = {:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cauwenberghs, Poggio - 2001 - Incremental and decremental support vector machine learning.pdf:pdf;:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cauwenberghs, Poggio - 2001 - Incremental and decremental support vector machine learning.html:html},
journal = {in Advances in Neural Information Processing Systems 13, T. K. Leen, T. G. Dietterich, and V. Tresp, Eds. MIT Press},
keywords = {Folder - Incremental},
mendeley-tags = {Folder - Incremental},
pages = {409--415},
title = {{Incremental and decremental support vector machine learning}},
url = {http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.21.1720 http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=A96AA7CF228E16B2A5CE86775BF05C6B?doi=10.1.1.21.1720{\&}rep=rep1{\&}type=pdf},
year = {2001}
}
@misc{CenterforHistoryandNewMedia,
annote = {Welcome to Zotero!View the Quick Start Guide to learn how to begin collecting, managing, citing, and sharing your research sources.Thanks for installing Zotero.},
author = {{Center for History and New Media}},
booktitle = {Journal of Bacteriology},
doi = {10.1128/JB.186.10.3097-3107.2004},
issn = {0021-9193},
number = {10},
pages = {3097--3107},
title = {{Zotero Quick Start Guide}},
url = {http://zotero.org/support/quick{\_}start{\_}guide},
volume = {186},
year = {2004}
}
@inproceedings{Chitta2010,
abstract = {Tactile information is valuable in determining properties of objects that are inaccessible from visual perception. In this work, we present a tactile perception strategy that allows any mobile robot with tactile sensors in its gripper to measure a set of generic tactile features while grasping an object. We propose a hybrid velocity-force controller, that grasps an object safely and reveals at the same time its deformation properties. As an application, we show that a robot can use these features to distinguish the open/closed and fill state of bottles and cans - purely from tactile sensing - from a small training set. To prove that this is a hard recognition problem, we also conducted a comperative study with 17 human test subjects. We found that the recognition rate of the human subjects were comparable to our robotic gripper.},
annote = {define a set of generic tactile features the usefulness of which is tested by designing a system that can distinguish between full and empty bottles by touch},
author = {Chitta, Sachin and Piccoli, Matthew and Sturm, J{\"{u}}rgen},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ROBOT.2010.5509923},
isbn = {9781424450381},
issn = {10504729},
keywords = {Feedback,Grippers,Humans,Robot sensing systems,Sensor arrays,State estimation,Testing,Visual Perception,deformation,deformation properties,force control,gripper,internal state recognition,mobile manipulation,mobile robot,mobile robots,tactile information,tactile object class,tactile sensors,velocity control,velocity-force controller},
mendeley-tags = {Feedback,Grippers,Humans,Robot sensing systems,Sensor arrays,State estimation,Testing,Visual Perception,deformation,deformation properties,force control,gripper,internal state recognition,mobile manipulation,mobile robot,mobile robots,tactile information,tactile object class,tactile sensors,velocity control,velocity-force controller},
month = {may},
pages = {2342--2348},
title = {{Tactile object class and internal state recognition for mobile manipulation}},
url = {http://ieeexplore.ieee.org/xpls/icp.jsp?arnumber=5509923},
year = {2010}
}
@inproceedings{Cho2013,
abstract = {Many tasks in computer vision are formulated as graph matching $\backslash$nproblems. Despite the NP-hard nature of the problem, fast and accurate $\backslash$napproximations have led to significant progress in a wide range of applications. $\backslash$nLearning graph models from observed data, however, still remains a challenging $\backslash$nissue. This paper presents an effective scheme to parameterize a graph model, $\backslash$nand learn its structural attributes for visual object matching. For this, we $\backslash$npropose a graph representation with histogram-based attributes, and optimize $\backslash$nthem to increase the matching accuracy. Experimental evaluations on synthetic $\backslash$nand real image datasets demonstrate the effectiveness of our approach, and show $\backslash$nsignificant improvement in matching accuracy over graphs with pre-defined $\backslash$nstructures.},
annote = {Introduces a way of encoding graph match learning as a SSVM. The graph learnt is the one that best repsents the seen data. The SSVM is of the form min 0.5||w||{\^{}}2 + C/n Sum(Delta(y{\_}i, y{\^{}}{\_}i(G{\_}i,w)). Where y{\^{}}(G,w) = argmax w.psi(G,y), and psi(G,y) = [...;a{\_}pi(i);...;a{\_}pi(i)pi(j);...]. Where a{\_}pi(i) represents the vertex assigned to vertex i, and a{\_}pi(i)pi(j) represents the edge assigned to edge i-j.
Here w encodes the parameters of the learnt graph G*. w = beta.theta(G*), where beta represents weighing on the various parameters of the graph, and theta(G*) = [...;a{\^{}}*{\_}i;...a{\^{}}*{\_}ij;...].
Where a{\^{}}*{\_}i, and a{\^{}}*{\_}ij represents the node and edge attributes of the learnt graph G*, respectively. These need not be 1-D.},
author = {Cho, Minsu and Alahari, Karteek and Ponce, Jean},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2013.11},
file = {:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cho, Alahari, Ponce - 2013 - Learning graphs to match.pdf:pdf;:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cho, Alahari, Ponce - 2013 - Learning graphs to match.html:html},
isbn = {9781479928392},
issn = {1550-5499},
keywords = {Computational modeling,Computer vision,Context,Folder - SSVMs,Graph Matching,Histograms,NP-hard problem,Optimization,Vectors,feature correspondence,graph learning,graph matching,graph matching problems,graph model,graph representation,graph theory,histogram-based attributes,image matching,learning systems,object recognition,visual object matching},
mendeley-tags = {Computational modeling,Computer vision,Context,Folder - SSVMs,Graph Matching,Histograms,NP-hard problem,Optimization,Vectors,feature correspondence,graph learning,graph matching problems,graph model,graph representation,graph theory,histogram-based attributes,image matching,learning systems,object recognition,visual object matching},
month = {dec},
pages = {25--32},
title = {{Learning graphs to match}},
url = {http://ieeexplore.ieee.org/ielx7/6750807/6751100/06751112.pdf?tp={\&}arnumber=6751112{\&}isnumber=6751100 http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6751112},
year = {2013}
}
@article{Chorley2009,
abstract = {AbstractHere we present a novel biologically inspired tactile sensor based on the structure of the human fingertip. Unlike previous biologically inspired sensors, it is based on new theories of the functional morphology of the fingertip skin features and the Meissners Corpuscles mechanoreceptors pervading them, particularly in the encoding of tactile edge information. Through mimicking the layered macro-structure of fingertip skin the sensor is highly conformable and very sensitive, as well as a strong and practical gripping tool. The tactile sensor is composed of a thin flexible rubber skin with structural details emulating those of the glabrous epidermis. This encases a clear, highly compliant polymer melt blend with similar mechanical properties to the dermis and subcutaneous fat. A camera is used to track markers on the internal structural details of the rubber skin enabling remote, detailed, direct and sensitive detection of surface deflections. Initial results presented here show the design to be a very capable, highly sensitive sensor as well as a very practical, affordable and scalable robotic fingertip.},
annote = {Introduces TACTIP, a biologically inspired touch sensor which comprises of a rubber hemisphere with inward pointing papillae whose tip is distinctively coloured. Internal LEDs provide illumination. a CCD camera records the distribution of the papillae. Deformation of the rubber through touch results in different images of the locations of the papillae's tips.},
author = {Chorley, Craig and Melhuish, Chris and Pipe, Tony and Rossiter, Jonathan},
isbn = {978-1-4244-4855-5},
journal = {Design},
keywords = {Biological information theory,Biosensors,Encoding,Folder - TACTIP,Humans,Mechanical sensors,Meissner's Corpuscles mechanoreceptors,Morphology,Rubber,Sensor phenomena and characterization,Skin,TACTIP,biologically inspired edge encoding,functional morphology,gripping tool,highly compliant polymer melt blend,human fingertip structure,manipulator dynamics,scalable robotic fingertip,tactile edge information,tactile sensor,tactile sensors,thin flexible rubber skin},
mendeley-tags = {Biological information theory,Biosensors,Encoding,Folder - TACTIP,Humans,Mechanical sensors,Meissner's Corpuscles mechanoreceptors,Morphology,Rubber,Sensor phenomena and characterization,Skin,TACTIP,biologically inspired edge encoding,functional morphology,gripping tool,highly compliant polymer melt blend,human fingertip structure,manipulator dynamics,scalable robotic fingertip,tactile edge information,tactile sensor,tactile sensors,thin flexible rubber skin},
pages = {1--6},
title = {{Development of a Tactile Sensor Based on Biologically Inspired Edge Encoding}},
url = {http://ieeexplore.ieee.org/ielx5/5166725/5174665/05174720.pdf?tp={\&}arnumber=5174720{\&}isnumber=5174665 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5174720 http://www.brl.ac.uk/projects/InspiredEdgeEncoding.pdf},
year = {2009}
}
@inproceedings{Chorley2010,
abstract = {The mechanics of the skin and subcutaneous tissues are as central to the sense of touch as the optics of the eye is to vision and the acoustics of the ear is to hearing. Recent published theories of the functional morphology of glabrous skin micro structures and their associated nerve endings are developed in this paper to form a new concept, that of direct tactile edge encoding by the skin. Through exploiting key mechanical properties and structures of human glabrous skin, it has been possible to create a prototype edge-encoding tactile sensor with an artificial skin capable of high levels of compliance, conformability and strength. The sensor has proved to have an impressive level of sensitivity, accuracy and detail within a skin design that promises to be a capable and practical end-effector. This paper presents and discusses the edge-encoding function of the sensor design and presents results that illustrate the capabilities of the sensor concept and the importance of edge sensitivity in tactile sensing.},
annote = {Track the papillae using standard MATLAB toolboxes, to determine point displacement vectors, which are interpreted as edge gradients (linear interpolation used).
 ----------Impressive results on accuracy, yet some artifacts appear in the images (perhaps as a result of the hemispherical shape of the sensor, detecting an 'edge' between the flat surface and air, or maybe due to secondary deformations)----------Unclear how efficient the algorithm is [Matt Lane]. Better apprach by Matt Lane's final year project (unpublished).},
author = {Chorley, Craig and Melhuish, Chris and Pipe, Tony and Rossiter, Jonathan},
booktitle = {Proceedings of IEEE Sensors},
doi = {10.1109/ICSENS.2010.5690181},
isbn = {9781424481682},
issn = {02628856},
keywords = {Skin,biomechanics,biomedical measurement,edge detection,edge-encoding tactile sensor,functional morphology,human glabrous skin,neurophysiology,subcutaneous tissues,tactile edge detection,tactile sensors,touch (physiological)},
mendeley-tags = {Skin,biomechanics,biomedical measurement,edge detection,edge-encoding tactile sensor,functional morphology,human glabrous skin,neurophysiology,subcutaneous tissues,tactile edge detection,tactile sensors,touch (physiological)},
pages = {2593--2598},
pmid = {20055546},
title = {{Tactile edge detection}},
url = {http://ieeexplore.ieee.org/ielx5/5678851/5689839/05690181.pdf?tp={\&}arnumber=5690181{\&}isnumber=5689839 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5690181{\&}tag=1},
year = {2010}
}
@inproceedings{Chu2013,
abstract = {Delivering on the promise of real-world robotics will require robots that can communicate with humans through natural language by learning new words and concepts through their daily experiences. Our research strives to create a robot that can learn the meaning of haptic adjectives by directly touching objects. By equipping the PR2 humanoid robot with state-of-the-art biomimetic tactile sensors that measure temperature, pressure, and fingertip deformations, we created a platform uniquely capable of feeling the physical properties of everyday objects. The robot used five exploratory procedures to touch 51 objects that were annotated by human participants with 34 binary adjective labels. We present both static and dynamic learning methods to discover the meaning of these adjectives from the labeled objects, achieving average F1 scores of 0.57 and 0.79 on a set of eight previously unfelt items.},
address = {Karlsruhe, Germany},
annote = {Uses 5 EPs together with feedback (adjectives given by humans) to categorise objects. Generalises well to unseen objects.},
author = {Chu, Vivian and McMahon, Ian and Riano, Lorenzo and McDonald, Craig G. and He, Qin and Perez-Tejada, Jorge Martinez and Arrigo, Michael and Fitter, Naomi and Nappo, John C. and Darrell, Trevor and Kuchenbecker, Katherine J.},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2013.6631000},
isbn = {9781467356411},
issn = {10504729},
pages = {3048--3055},
title = {{Using robotic exploratory procedures to learn the meaning of haptic adjectives}},
url = {file:///home/tadeo/x-drive/conferences/ICRA2013/media/files/papers{\_}videos/1663.pdf},
year = {2013}
}
@article{Colavita1974,
abstract = {Human Ss matched an auditory and a visual stimulus for subjective magnitude. Then each stimulus was used as a cue in a reaction time task. On occasions when both stimuli were presented simultaneously, Ss' responding was seen to be dominated by the visual stimulus. Of further interest was the finding that on some occasions of simultaneous light-tone presentation Ss were unaware that the tone had been presented. This apparent prepotency of the visual over the auditory stimulus was seen to persist across a variety of experimental conditions, which included giving Ss verbal instructions to respond to the tone when both stimuli were presented simultaneously.},
annote = {it is largely believed that in humans vision is the dominant sense (as exemplified by the Colavita effect )},
author = {Colavita, Francis B.},
doi = {10.3758/BF03203962},
isbn = {0031-5117},
issn = {0031-5117},
journal = {Perception {\&} Psychophysics},
keywords = {*Auditory Perception,*Discrimination Learning,*Time Perception,Animals,Avoidance Learning,Brain Mapping,Cats,Cerebral Cortex/*physiology,Cognitive Psychology,Electroshock,Folder - Haptic-Visual integration - bio motivatio,Temporal Lobe/*physiology},
language = {en},
mendeley-tags = {Cognitive Psychology,Folder - Haptic-Visual integration - bio motivatio},
month = {mar},
number = {2},
pages = {409--412},
title = {{Human Sensory Dominance}},
url = {http://link.springer.com/article/10.3758/BF03203962 http://link.springer.com/content/pdf/10.3758{\%}2FBF03203962.pdf},
volume = {16},
year = {1974}
}
@inproceedings{Corradi2015,
abstract = {We present a Bayesian approach to tactile object recognition that improves on state-of-the-art in using single-touch events in two ways. First by improving recognition accuracy from about 90{\%} to about 95{\%}, using about half the number of touches. Second by reducing the number of touches needed for training from about 200 to about 60. In addition, we use a new tactile sensor that is less than one tenth of the cost of widely available sensors. The paper describes the sensor, the likelihood function used with the Naive Bayes classifier, and experiments on a set of ten real objects. We also provide preliminary results to test our approach for its ability to generalise to previously unencountered objects. {\textcopyright} 2015 IEEE.},
author = {Corradi, Tadeo and Hall, Peter and Iravani, Pejman},
booktitle = {2015 IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2015.7139744},
isbn = {978-1-4799-6923-4},
keywords = {Bayes methods,Bayesian,Bayesian approaches,Bayesian networks,Bayesian tactile object recognition,Classifiers,Likelihood functions,Naive Bayes classifiers,Real objects,Recognition accuracy,Tactile sensors,Testing,Training,Two ways,accuracy,likelihood function,naive Bayes classifier,object learning,object recognition,robotics,tactile sensor},
pages = {3909--3914},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Bayesian tactile object recognition: Learning and recognising objects using a new inexpensive tactile sensor}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7139744},
volume = {2015-June},
year = {2015}
}
@inproceedings{Corradi2014,
abstract = {A simple and cost effective new tactile sensor is presented, based on a camera capturing images of the shading of a deformable rubber membrane. In Computer Vision, the issue of information encoding and classification is well studied. In this paper we explore different ways of encoding tactile images, including: Hu moments, Zernike Moments, Principal Component Analysis (PCA), Zernike PCA, and vectorized scaling. These encodings are tested by performing tactile shape recognition using a number of supervised approaches (Nearest Neighbor, Artificial Neural Networks, Support Vector Machines, Naive Bayes). In conclusion: the most effective way of representing tactile information is achieved by combining Zernike Moments and PCA, and the most accurate classifier is Nearest Neighbor, with which the system achieves a high degree (96.4{\%}) of accuracy at recognising seven basic shapes. {\textcopyright} 2014 Springer International Publishing.},
author = {Corradi, Tadeo and Hall, Peter and Iravani, Pejman},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Haptic recognition,supervised learning,tactile features,tactile sensors},
pages = {163--172},
publisher = {Springer Verlag},
title = {{Tactile features: Recognising touch sensations with a novel and inexpensive tactile sensor}},
volume = {8717 LNAI},
year = {2014}
}
@inproceedings{Cretu2015,
abstract = {The paper discusses signal processing and mechatronics solutions for tactile object recognition using robotic hands. The two important aspects analyzed are the recognition of objects from tactile displacement profiles obtained by force sensing transducers and the recognition of textures using a rubbing motion executed by a robotic finger equipped with a dynamic tactile fingertip. Neural network solutions are used as an intelligent approach to recognize symbols, such as embossed numbers and letters, as well as for the recognition of different texture profiles. The conception issues, the challenges as well as the experimental results obtained are discussed from both a biological and a technological perspective.},
annote = {Tactile exploration used to construct a global tactile image of reliefs (digits). Two stacked neural networks are then used for classification, obtaining over 92{\%} accuracy.},
author = {Cretu, Ana Maria and {De Oliveira}, Thiago Eustaquio Alves and {Prado Da Fonseca}, Vinicius and Tawbe, Bilal and Petriu, Emil M. and Groza, Voicu Z.},
booktitle = {WISP 2015 - IEEE International Symposium on Intelligent Signal Processing, Proceedings},
doi = {10.1109/WISP.2015.7139165},
file = {:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cretu et al. - 2015 - Computational intelligence and mechatronics solutions for robotic tactile object recognition.pdf:pdf;:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cretu et al. - 2015 - Computational intelligence and mechatronics solutions for robotic tactile object recognition.html:html},
isbn = {9781479972524},
keywords = {Arrays,Fingers,Image processing,Mechatronics,Neural networks,Principal component analysis,Robot hand,Shape,haptic perception,image processing,mechatronics,model-based recognition,neural networks,principal component analysis,robot hand,tactile sensor,tactile sensors},
mendeley-tags = {Arrays,Fingers,Image processing,Mechatronics,Neural networks,Principal component analysis,Robot hand,Shape,haptic perception,model-based recognition,tactile sensor,tactile sensors},
month = {may},
pages = {1--6},
title = {{Computational intelligence and mechatronics solutions for robotic tactile object recognition}},
url = {http://ieeexplore.ieee.org/ielx7/7126340/7139146/07139165.pdf?tp={\&}arnumber=7139165{\&}isnumber=7139146 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=7139165{\&}tag=1},
year = {2015}
}
@article{Csurka2004,
abstract = {We present a novel method for generic visual categorization: the problem of identifying the object content of natural images while generalizing across variations inherent to the object class. This bag of keypoints method is based on vector quantization of affine invariant descriptors of image patches. We propose and compare two alternative implementations using different classifiers: Na{\"{i}}ve Bayes and SVM. The main advantages of the method are that it is simple, computationally efficient and intrinsically invariant. We present results for simultaneously classifying seven semantic visual categories. These results clearly demonstrate that the method is robust to background clutter and produces good categorization accuracy even without exploiting geometric information.},
annote = {Introduces bag of features},
author = {Csurka, Gabriella and Dance, Christopher R. and Fan, Lixin and Willamowski, Jutta and Bray, C{\'{e}}dric},
doi = {10.1234/12345678},
isbn = {9780335226375},
issn = {18703453},
journal = {Proceedings of the ECCV International Workshop on Statistical Learning in Computer Vision},
keywords = {Folder - TOR by BOF},
mendeley-tags = {Folder - TOR by BOF},
pages = {59--74},
title = {{Visual categorization with bags of keypoints}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=7220F593C55029FBC7C0F6D85212C7B6?doi=10.1.1.72.604{\&}rep=rep1{\&}type=pdf http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.72.604},
year = {2004}
}
@article{Dahiya2013,
abstract = {A wide variety of tactile (touch) sensors exist today for robotics and related applications. They make use of various transduction methods, smart materials and engineered structures, complex electronics, and sophisticated data processing. While highly useful in themselves, effective utilization of tactile sensors in robotics applications has been slow to come and largely remains elusive today. This paper surveys the state of the art and the research issues in this area, with the emphasis on effective utilization of tactile sensors in robotic systems. One specific with the use of tactile sensing in robotics is that the sensors have to be spread along the robot body, the way the human skin is—thus dictating varied 3-D spatio–temporal requirements, decentralized and distributed control, and handling of multiple simultaneous tactile contacts. Satisfying these requirements pose challenges to making tactile sensor modality a reality. Overcoming these challenges requires dealing with issues such as sensors placement, electronic/mechanical hardware, methods to access and acquire signals, automatic calibration techniques, and algorithms to process and interpret sensing data in real time. We survey this field from a system perspective, recognizing the fact that the system performance tends to depend on how its various components are put together. It is hoped that the survey will be of use to practitioners designing tactile sensing hardware (whole-body or large-patch sensor coverage), and to researchers working on cognitive robotics involving tactile sensing.},
annote = {
an extensive review of tactile and haptic sening technologies and approaches to analyses of sensed data,

----------

unlike visual information, haptic information is distributed over a potentially unknown geometry [], so the equivalent to 'camera calibration' is a significantly more difficult task.

----------

there is no consensus on the best approach to encoding of tactile sensing information
},
author = {Dahiya, R.S. and Mittendorfer, P. and Valle, M. and Cheng, G. and Lumelsky, V.J.},
doi = {10.1109/JSEN.2013.2279056},
issn = {1530-437X},
journal = {IEEE Sensors Journal},
keywords = {Skin,Substrates,Tactile skin,bendable electronics,embedded systems,interface electronics,modular,sensor fusion,tactile sensors},
mendeley-tags = {Skin,Substrates,Tactile skin,bendable electronics,embedded systems,interface electronics,modular,sensor fusion,tactile sensors},
number = {11},
pages = {4121--4138},
shorttitle = {Directions Toward Effective Utilization of Tactile},
title = {{Directions Toward Effective Utilization of Tactile Skin: A Review}},
url = {http://ieeexplore.ieee.org/ielx7/7361/6609078/06583342.pdf?tp={\&}arnumber=6583342{\&}isnumber=6609078 http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6583342{\&}navigation=1},
volume = {13},
year = {2013}
}
@inproceedings{Dahiya2009,
abstract = {... In this way, a force in the range of 2.1 gmf - 17.6 gmf (0.021 N - 0.176 N) is reported to be controlled by pressure in the ... The tactile sensing chip consists of a 2-D array of 32 microelectrodes epoxy adhered with thin piezoelectric polymer film . ... [17] for the development of ultrasonic ... $\backslash$n},
annote = {unread},
author = {Dahiya, Ravinder S. and Metta, G. and VaIle, M.},
booktitle = {IEEE 2009 International Conference on Mechatronics, ICM 2009},
doi = {10.1109/ICMECH.2009.4957166},
isbn = {9781424441952},
keywords = {Communication system control,Ethernet networks,Hardware,Humans,MEA,Mechatronics,Tactile Sensing Arrays,Tactile sensing,Tactile sensing arrays,Testing,Universal Serial Bus,Virtual environment,fingertip tactile sensing chip,haptic interfaces,humanoid robot,humanoid robots,microelectrode array,microelectrodes,microprocessor chips,piezoelectric devices,piezoelectric polymer,polymers,tactile sensing,tactile sensor,tactile sensors,touch (physiological)},
mendeley-tags = {Communication system control,Ethernet networks,Hardware,Humans,MEA,Mechatronics,Tactile Sensing Arrays,Testing,Universal Serial Bus,Virtual environment,fingertip tactile sensing chip,haptic interfaces,humanoid robot,humanoid robots,microelectrode array,microelectrodes,microprocessor chips,piezoelectric devices,piezoelectric polymer,polymers,tactile sensing,tactile sensor,tactile sensors,touch (physiological)},
pages = {1--6},
title = {{Development of fingertip tactile sensing chips for humanoid robots}},
url = {http://ieeexplore.ieee.org/ielx5/4914928/4957110/04957166.pdf?tp={\&}arnumber=4957166{\&}isnumber=4957110 http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=4957166{\&}navigation=1},
year = {2009}
}
@article{Damen2012,
abstract = {We present a method for the learning and detection of multiple rigid texture-less 3D objects intended to operate at frame rate speeds for video input. The method is geared for fast and scalable learning and detection by combining tractable extraction of edgelet constellations with library lookup based on rotation- and scale-invariant descriptors. The approach learns object views in real-time, and is generative - enabling more objects to be learnt without the need for re-training. During testing, a random sample of edgelet constellations is tested for the presence of known objects. We perform testing of single and multi-object detection on a 30 objects dataset showing detections of any of them within milliseconds from the object's visibility. The results show the scalability of the approach and its framerate performance.},
annote = {Constellations of edges and edge segments have been used to achieve online learning and detection of objects. They use a novel algorithm for looking up objects in a database, whereby many possible paths between the parts are considered, encoded, and their similarity scored, based on relative angles and distances.
During learning, all paths are recorded onto a database. During testing, a search time limit is imposed, so the number of checked paths varies.
Achieve a precision score of 70 to 80$\backslash${\%}. The best trade-off between precision and recall is achieved at the lower end of the scale, where fewer than 5 frames per second can be processed. However the authors argue that recall is not that important in an online system where if an object is missed in a frame it can be detected soon in the next few frames.
Method scales well, due to intelligent hash table lookups, the distinctiveness of objects learnt is more important than their number.----------Uses paths (vector of angles and distances) between edgelets. Records all paths onto a hashtable.
 
During learning, all paths are recorded onto a database. During testing, a search time limit is imposed, so the number of checked paths varies.
Achieve a precision score of 70 to 80$\backslash${\%}. The best trade-off between precision and recall is achieved at the lower end of the scale, where fewer than 5 frames per second can be processed. However the authors argue that recall is not that important in an online system where if an object is missed in a frame it can be detected soon in the next few frames.
Method scales well, due to intelligent hash table lookups, the distinctiveness of objects learnt is more important than their number.},
author = {Damen, Dima and Bunnun, Pished},
doi = {10.5244/C.26.23},
isbn = {1-901725-46-4},
journal = {Bmvc},
keywords = {Folder - constellation-star models},
mendeley-tags = {Folder - constellation-star models},
pages = {1--12},
publisher = {British Machine Vision Association},
shorttitle = {Real-time Learning and Detection of 3D Texture-les},
title = {{Real-time Learning and Detection of 3D Texture-less Objects: A Scalable Approach.}},
url = {http://www.bmva.org/bmvc/2012/BMVC/paper023/index.html http://www.cs.bris.ac.uk/{~}damen/bmvc2012{\_}scalable{\_}textureless.pdf http://research-information.bris.ac.uk/explore/files/7483923/bmvc2012{\_}scalable{\_}textureless{\_}3{\_}.pdf},
year = {2012}
}
@inproceedings{Danielsson2009,
abstract = {In this paper we introduce a new kind of feature - the multi-local feature, so named as each one is a collection of local features, such as oriented edgels, in a very specific spatial arrangement. A multi-local feature has the ability to capture underlying constant shape properties of exemplars from an object class. Thus it is particularly suited to representing and detecting visual classes that lack distinctive local structures and are mainly defined by their global shape. We present algorithms to automatically learn an ensemble of these features to represent an object class from weakly labelled training images of that class, as well as procedures to detect these features efficiently in novel images. The power of multi-local features is demonstrated by using the ensemble in a simple voting scheme to perform object category detection on a standard database. Despite its simplicity, this scheme yields detection rates matching state-of-the-art object detection systems.},
author = {Danielsson, Oscar and Carlsson, Stefan and Sullivan, Josephine},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2009.5459338},
file = {:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Danielsson, Carlsson, Sullivan - 2009 - Automatic learning and extraction of multi-local features.html:html},
isbn = {9781424444205},
issn = {1550-5499},
keywords = {Computer science,Computer vision,Feature extraction,Folder - constellation-star models - geometry over,Image databases,Robustness,Shape,Spatial databases,Voting,automatic learning,edge detection,learning (artificial intelligence),multilocal feature extraction,object category detection,object detection,oriented edgels,spatial arrangement,visual databases},
mendeley-tags = {Computer science,Computer vision,Feature extraction,Folder - constellation-star models - geometry over,Image databases,Robustness,Shape,Spatial databases,Voting,automatic learning,edge detection,learning (artificial intelligence),multilocal feature extraction,object category detection,object detection,oriented edgels,spatial arrangement,visual databases},
month = {sep},
pages = {917--924},
title = {{Automatic learning and extraction of multi-local features}},
url = {http://ieeexplore.ieee.org/xpls/icp.jsp?arnumber=5459338{\&}tag=1{\#}fig{\_}4},
year = {2009}
}
@article{Davies1979,
abstract = {A measure is presented which indicates the similarity of clusters which are assumed to have a data density which is a decreasing function of distance from a vector characteristic of the cluster. The measure can be used to infer the appropriateness of data partitions and can therefore be used to compare relative appropriateness of various divisions of the data. The measure does not depend on either the number of clusters analyzed nor the method of partitioning of the data and can be used to guide a cluster seeking algorithm.},
author = {Davies, David L. and Bouldin, Donald W.},
doi = {10.1109/TPAMI.1979.4766909},
isbn = {0162-8828},
issn = {0162-8828},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Algorithm design and analysis,Cluster,Clustering algorithms,Data analysis,Density measurement,Dispersion,Folder - Clustering,Humans,Missiles,Multidimensional systems,Partitioning algorithms,Performance analysis,data partitions,multidimensional data analysis,parametric clustering,partitions,similarity measure},
mendeley-tags = {Algorithm design and analysis,Cluster,Clustering algorithms,Data analysis,Density measurement,Dispersion,Folder - Clustering,Humans,Missiles,Multidimensional systems,Partitioning algorithms,Performance analysis,data partitions,multidimensional data analysis,parametric clustering,partitions,similarity measure},
month = {apr},
number = {2},
pages = {224--227},
pmid = {21868852},
title = {{A cluster separation measure.}},
url = {http://ieeexplore.ieee.org/ielx5/34/4766893/04766909.pdf?tp={\&}arnumber=4766909{\&}isnumber=4766893 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4766909{\&}tag=1},
volume = {1},
year = {1979}
}
@article{Decherchi2011,
annote = {use SVMs, RLS and RELM to classify materials such as wood, brass and lead},
author = {Decherchi, Sergio and Gastaldo, Paolo and Dahiya, Ravinder S. and Valle, Maurizio and Zunino, Rodolfo},
doi = {10.1109/TRO.2011.2130030},
file = {:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Decherchi et al. - 2011 - Tactile-Data Classification of Contact Materials Using Computational Intelligence.pdf:pdf},
issn = {1552-3098},
journal = {IEEE Transactions on Robotics},
month = {jun},
number = {3},
pages = {635--639},
title = {{Tactile-Data Classification of Contact Materials Using Computational Intelligence}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5756252},
volume = {27},
year = {2011}
}
@article{Diehl2003,
abstract = {The objective of machine learning is to identify a model that yields good generalization performance. This involves repeatedly selecting a hypothesis class, searching the hypothesis class by minimizing a given objective function over the model's parameter space, and evaluating the generalization performance of the resulting model. This search can be computationally intensive as training data continuously arrives, or as one needs to tune hyperparameters in the hypothesis class and the objective function. In this paper, we present a framework for exact incremental learning and adaptation of support vector machine (SVM) classifiers. The approach is general and allows one to learn and unlearn individual or multiple examples, adapt the current SVM to changes in regularization and kernel parameters, and evaluate generalization performance through exact leave-one-out error estimation.},
annote = {Same as [Cauwenberghs 00] but it also propose methods for perturbing the hyperparameter C, which regulates the relative importance of constraint violation versus maximum marginal distance to support vectors.},
author = {Diehl, C.P. and Cauwenberghs, G.},
doi = {10.1109/IJCNN.2003.1223991},
file = {:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Diehl, Cauwenberghs - 2003 - SVM incremental learning, adaptation and optimization.pdf:pdf;:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Diehl, Cauwenberghs - 2003 - SVM incremental learning, adaptation and optimization.html:html},
isbn = {0-7803-7898-9},
issn = {1098-7576},
journal = {Proceedings of the International Joint Conference on Neural Networks, 2003.},
keywords = {Folder - Incremental,Kernel,Laboratories,Optimization,Risk management,Statistical learning,Support vector machine classification,Training data,adaptation,error analysis,exact leave-one-out error estimation,generalisation (artificial intelligence),generalization,hyperparameters,hypothesis class,incremental learning,kernel parameters,learning (artificial intelligence),machine learning,optimisation,physics,regularization,support vector machine classifiers,support vector machines},
mendeley-tags = {Folder - Incremental,Kernel,Laboratories,Optimization,Risk management,Statistical learning,Support vector machine classification,Training data,adaptation,error analysis,exact leave-one-out error estimation,generalisation (artificial intelligence),generalization,hyperparameters,hypothesis class,incremental learning,kernel parameters,learning (artificial intelligence),machine learning,optimisation,physics,regularization,support vector machine classifiers,support vector machines},
month = {jul},
number = {x},
pages = {2685--2690},
title = {{SVM incremental learning, adaptation and optimization}},
url = {http://ieeexplore.ieee.org/xpls/icp.jsp?arnumber=1223991 http://www.cpdiehl.org/PDF/IJCNN03{\_}final.pdf},
volume = {4},
year = {2003}
}
@article{Donahue2013,
abstract = {We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be re-purposed to novel generic tasks. Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks. We investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges. We compare the efficacy of relying on various network levels to define a fixed feature, and report novel results that significantly outperform the state-of-the-art on several important vision challenges. We are releasing DeCAF, an open-source implementation of these deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms.},
author = {Donahue, Jeff and Jia, Yangqing and Vinyals, Oriol and Hoffman, Judy and Zhang, Ning and Tzeng, Eric and Darrell, Trevor},
journal = {CoRR},
keywords = {Computer Science - Computer Vision and Pattern Rec},
mendeley-tags = {Computer Science - Computer Vision and Pattern Rec},
month = {oct},
shorttitle = {DeCAF},
title = {{DeCAF: A deep convolutional activation feature for generic visual recognition.}},
url = {http://arxiv.org/abs/1310.1531 http://www.arxiv.org/pdf/1310.1531.pdf},
year = {2013}
}
@article{Drimus2014,
abstract = {For both humans and robots, tactile sensing is important for interaction with the environment: it is the core sensing used for exploration and manipulation of objects. In this paper, we present a novel tactile-array sensor based on flexible piezoresistive rubber. We describe the design of the sensor and data acquisition system. We evaluate the sensitivity and robustness of the sensor, and show that it is consistent over time with little relaxation. Furthermore, the sensor has the benefit of being flexible, having a high resolution, it is easy to mount, and simple to manufacture. We demonstrate the use of the sensor in an active object-classification system. A robotic gripper with two sensors mounted on its fingers performs a palpation procedure on a set of objects. By squeezing an object, the robot actively explores the material properties, and the system acquires tactile information corresponding to the resulting pressure. Based on a k nearest neighbor classifier and using dynamic time warping to calculate the distance between different time series, the system is able to successfully classify objects. Our sensor demonstrates similar classification performance to the Weiss Robotics tactile sensor, while having additional benefits. ?? 2012 Elsevier B.V. All rights reserved.},
annote = {Perform recognition of objects via grasping and time series comparison to detect the compliance of objects.},
author = {Drimus, Alin and Kootstra, Gert and Bilberg, Arne and Kragic, Danica},
doi = {10.1016/j.robot.2012.07.021},
isbn = {978-1-4577-1159-6},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Active perception,Piezoresistive rubber,Tactile object classification,Tactile sensing,tactile sensing},
mendeley-tags = {Active perception,Piezoresistive rubber,Tactile object classification,tactile sensing},
month = {jan},
number = {1},
pages = {3--15},
series = {New Boundaries of Robotics},
title = {{Design of a flexible tactile sensor for classification of rigid and deformable objects}},
url = {http://www.sciencedirect.com/science/article/pii/S092188901200125X http://www.sciencedirect.com/science/article/pii/S092188901200125X/pdf?md5=d94c1b9a78445f71e8d3946329aaf533{\&}pid=1-s2.0-S092188901200125X-main.pdf},
volume = {62},
year = {2014}
}
@inproceedings{Drost2010,
abstract = {This paper addresses the problem of recognizing free-form 3D objects in point clouds. Compared to traditional approaches based on point descriptors, which depend on local information around points, we propose a novel method that creates a global model description based on oriented point pair features and matches that model locally using a fast voting scheme. The global model description consists of all model point pair features and represents a mapping from the point pair feature space to the model, where similar features on the model are grouped together. Such representation allows using much sparser object and scene point clouds, resulting in very fast performance. Recognition is done locally using an efficient voting scheme on a reduced two-dimensional search space. We demonstrate the efficiency of our approach and show its high recognition performance in the case of noise, clutter and partial occlusions. Compared to state of the art approaches we achieve better recognition rates, and demonstrate that with a slight or even no sacrifice of the recognition performance our method is much faster then the current state of the art approaches.},
annote = {unread},
author = {Drost, Bertram and Ulrich, Markus and Navab, Nassir and Ilic, Slobodan},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2010.5540108},
isbn = {9781424469840},
issn = {10636919},
keywords = {Cameras,Clouds,Computer Graphics,Computer science,Layout,Robustness,Sensor systems,Stereo vision,Voting,fast voting scheme,global model description,image representation,mapping representation,object detection,object recognition,point descriptors,point pair feature space model,reduced two-dimensional search space,robust 3D object recognition,scene point clouds},
mendeley-tags = {Cameras,Clouds,Computer Graphics,Computer science,Layout,Robustness,Sensor systems,Stereo vision,Voting,fast voting scheme,global model description,image representation,mapping representation,object detection,object recognition,point descriptors,point pair feature space model,reduced two-dimensional search space,robust 3D object recognition,scene point clouds},
month = {jun},
pages = {998--1005},
shorttitle = {Model globally, match locally},
title = {{Model globally, match locally: Efficient and robust 3D object recognition}},
url = {http://ieeexplore.ieee.org/ielx5/5521876/5539770/05540108.pdf?tp={\&}arnumber=5540108{\&}isnumber=5539770 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5540108},
year = {2010}
}
@inproceedings{Duchenne2011,
abstract = {This paper addresses the problem of category-level image classification. The underlying image model is a graph whose nodes correspond to a dense set of regions, and edges reflect the underlying grid structure of the image and act as springs to guarantee the geometric consistency of nearby regions during matching. A fast approximate algorithm for matching the graphs associated with two images is presented. This algorithm is used to construct a kernel appropriate for SVM-based image classification, and experiments with the Caltech 101, Caltech 256, and Scenes datasets demonstrate performance that matches or exceeds the state of the art for methods using a single type of features.},
author = {Duchenne, Olivier and Joulin, Armand and Ponce, Jean},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2011.6126445},
file = {:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Duchenne, Joulin, Ponce - 2011 - A graph-matching kernel for object categorization.html:html},
isbn = {9781457711015},
issn = {1550-5499},
keywords = {Approximation algorithms,Folder - models - MRF-kernel-SVM,Geometry,Image edge detection,Kernel,Optimization,SVM-based image classification,Vectors,category-level image classification,fast approximate algorithm,geometric consistency,graph theory,graph-matching kernel,grid structure,image classification,image matching,image model,image retrieval,object categorization,support vector machines},
mendeley-tags = {Approximation algorithms,Folder - models - MRF-kernel-SVM,Geometry,Image edge detection,Kernel,Optimization,SVM-based image classification,Vectors,category-level image classification,fast approximate algorithm,geometric consistency,graph theory,graph-matching kernel,grid structure,image classification,image matching,image model,image retrieval,object categorization,support vector machines},
month = {nov},
pages = {1792--1799},
title = {{A graph-matching kernel for object categorization}},
url = {http://ieeexplore.ieee.org/xpls/icp.jsp?arnumber=6126445},
year = {2011}
}
@article{Dunn1973,
abstract = {Two fuzzy versions of the k-means optimal, least squared error partitioning problem are formulated for finite subsets X of a general inner product space. In both cases, the extremizing solutions are shown to be fixed points of a certain operator T on the class of fuzzy, k-partitions of X, and simple iteration of T provides an algorithm which has the descent property relative to the least squared error criterion function. In the first case, the range of T consists largely of ordinary (i.e. non-fuzzy) partitions of X and the associated iteration scheme is essentially the well known ISODATA process of Ball and Hall. However, in the second case, the range of T consists mainly of fuzzy partitions and the associated algorithm is new; when X consists of k compact well separated (CWS) clusters, Xi , this algorithm generates a limiting partition with membership functions which closely approximate the characteristic functions of the clusters Xi . However, when X is not the union of k CWS clusters, the limiting partition is truly fuzzy in the sense that the values of its component membership functions differ substantially from 0 or 1 over certain regions of X. Thus, unlike ISODATA, the “fuzzy” algorithm signals the presence or absence of CWS clusters in X. Furthermore, the fuzzy algorithm seems significantly less prone to the “cluster-splitting” tendency of ISODATA and may also be less easily diverted to uninteresting locally optimal partitions. Finally, for data sets X consisting of dense CWS clusters embedded in a diffuse background of strays, the structure of X is accurately reflected in the limiting partition generated by the fuzzy algorithm. Mathematical arguments and numerical results are offered in support of the foregoing assertions.},
author = {Dunn, J. C.},
doi = {10.1080/01969727308546046},
isbn = {0022-0280},
issn = {0022-0280},
journal = {Journal of Cybernetics},
keywords = {Folder - Clustering,c,fcm,fuzzy,means},
mendeley-tags = {Folder - Clustering,c,fcm,fuzzy,means},
number = {3},
pages = {32--57},
title = {{A Fuzzy Relative of the ISODATA Process and Its Use in Detecting Compact Well-Separated Clusters}},
url = {http://dx.doi.org/10.1080/01969727308546046},
volume = {3},
year = {1974}
}
@article{Eitel2015,
abstract = {Robust object recognition is a crucial ingredient of many robotics applications and a prerequisite for solving challenging tasks such as manipulating or grasping objects. Recently, convolutional neural networks (CNNs) have been shown to outperform conventional computer vision algorithms for object recognition from images in many large-scale recognition tasks. In this paper, we investigate the potential for using CNNs to solve object recognition from combined RGB and depth images. We present a new network architecture with two processing streams that learns descriptive features from both input modalities and can learn to fuse information automatically before classification. We introduce an effective encoding of depth information for CNNs and report state of the art performance on the challenging RGB-D Object dataset, where we achieve a recognition accuracy of 91.3{\%}. Finally, to facilitate usage of our method "in the wild", we present a novel synthetic data augmentation approach for training with depth images that improves recognition accuracy in real-world environments.},
annote = {Best result (91.3{\%}) for the Washington dataset (51 classes, 300 instances of each, 3 viewpoints) by training separate deep CNNs for depth and RGB and fusing them in the last 2 layers (1 fully connected fusion + 1 softmax classification)},
archivePrefix = {arXiv},
arxivId = {1507.06821},
author = {Eitel, Andreas and Springenberg, Jost Tobias and Spinello, Luciano and Riedmiller, Martin and Burgard, Wolfram},
doi = {10.1109/TMM.2015.2476655},
eprint = {1507.06821},
file = {:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Eitel et al. - 2015 - Multimodal Deep Learning for Robust RGB-D Object Recognition.pdf:pdf;:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Eitel et al. - 2015 - Multimodal Deep Learning for Robust RGB-D Object Recognition.html:html},
isbn = {9781479969227},
issn = {1520-9210},
journal = {arXiv:1507.06821 [cs]},
keywords = {Computer Science - Computer Vision and Pattern Rec,Computer Science - Learning,Computer Science - Neural and Evolutionary Computi,Computer Science - Robotics,Folder - RGB and depth - Deep Learning},
mendeley-tags = {Computer Science - Computer Vision and Pattern Rec,Computer Science - Learning,Computer Science - Neural and Evolutionary Computi,Computer Science - Robotics,Folder - RGB and depth - Deep Learning},
month = {jul},
number = {11},
pages = {1887--1898},
title = {{Multimodal Deep Learning for Robust RGB-D Object Recognition}},
url = {http://arxiv.org/abs/1507.06821 http://www.arxiv.org/pdf/1507.06821.pdf},
volume = {17},
year = {2015}
}
@article{Erhan2009,
abstract = {Whereas theoretical work suggests that deep architectures might be more efﬁcient at representing highly-varying functions, training deep architectures was unsuccessful until the recent advent of algorithms based on unsupervised pretraining. Even though these new algorithms have enabled training deep models, many questions remain as to the nature of this difﬁcult learning problem. Answering these questions is important if learning in deep architectures is to be further improved. We attempt to shed some light on these questions through extensive simulations. The experiments conﬁrm and clarify the advantage of unsupervised pre-training. They demonstrate the robustness of the training procedure with respect to the random initialization, the positive effect of pre-training in terms of optimization and its role as a regularizer. We empirically show the inﬂuence of pre-training with respect to architecture depth, model capacity, and number of training examples.},
annote = {Pretraining is desirable over randomised initialisation of NNs, to avoid bad local optima.},
author = {Erhan, Dumitru and Manzagol, Pierre-Antoine and Bengio, Yoshua and Bengio, Samy and Vincent, Pascal},
isbn = {15324435},
issn = {15324435},
journal = {International Conference on Artificial Intelligence and Statistics},
pages = {153--160},
title = {{The difficulty of training deep architectures and the effect of unsupervised pre-training}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/AISTATS09{\_}ErhanMBBV.pdf},
volume = {5},
year = {2009}
}
@article{Ernst2002,
abstract = {When a person looks at an object while exploring it with their hand, vision and touch both provide information for estimating the properties of the object. Vision frequently dominates the integrated visual-haptic percept, for example when judging size, shape or position, but in some circumstances the percept is clearly affected by haptics. Here we propose that a general principle, which minimizes variance in the final estimate, determines the degree to which vision or haptics dominates. This principle is realized by using maximum-likelihood estimation to combine the inputs. To investigate cue combination quantitatively, we first measured the variances associated with visual and haptic estimation of height. We then used these measurements to construct a maximum-likelihood integrator. This model behaved very similarly to humans in a visual-haptic task. Thus, the nervous system seems to combine visual and haptic information in a fashion that is similar to a maximum-likelihood integrator. Visual dominance occurs when the variance associated with visual estimation is lower than that associated with haptic estimation.},
annote = {unread----------seems to suggest humans integrate visual and haptic data in a manner resembling MLE integrators.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Ernst, Marc O. and Banks, Martin S.},
doi = {10.1038/415429a},
eprint = {NIHMS150003},
isbn = {0028-0836 (Print)$\backslash$r0028-0836 (Linking)},
issn = {00280836},
journal = {Nature},
keywords = {DNA,Folder - Haptic-Visual integration,Folder - Haptic-Visual integration - bio motivatio,Nature,RNA,astronomy,astrophysics,biochemistry,bioinformatics,biology,biotechnology,cancer,cell cycle,cell signalling,climate change,computational biology,development,developmental biology,drug discovery,earth science,ecology,environmental science,evolution,evolutionary biology,functional genomics,genetics,genomics,geophysics,immunology,interdisciplinary science,life,marine biology,materials science,medical research,medicine,metabolomics,molecular biology,molecular interactions,nanotechnology,neurobiology,neuroscience,palaeobiology,pharmacology,physics,proteomics,quantum physics,science,science news,science policy,signal transduction,structural biology,systems biology,transcriptomics},
language = {en},
mendeley-tags = {DNA,Folder - Haptic-Visual integration,Folder - Haptic-Visual integration - bio motivatio,Nature,RNA,astronomy,astrophysics,biochemistry,bioinformatics,biology,biotechnology,cancer,cell cycle,cell signalling,climate change,computational biology,development,developmental biology,drug discovery,earth science,ecology,environmental science,evolution,evolutionary biology,functional genomics,genetics,genomics,geophysics,immunology,interdisciplinary science,life,marine biology,materials science,medical research,medicine,metabolomics,molecular biology,molecular interactions,nanotechnology,neurobiology,neuroscience,palaeobiology,pharmacology,physics,proteomics,quantum physics,science,science news,science policy,signal transduction,structural biology,systems biology,transcriptomics},
month = {jan},
number = {6870},
pages = {429--433},
pmid = {11807554},
title = {{Humans integrate visual and haptic information in a statistically optimal fashion.}},
url = {http://www.nature.com/nature/journal/v415/n6870/full/415429a.html http://www.nature.com/nature/journal/v415/n6870/pdf/415429a.pdf},
volume = {415},
year = {2002}
}
@article{Faldella1997,
abstract = {This paper describes a novel approach to robotic haptic recognition, which exploits an unsupervised Kohonen self-organizing feature map for performing a match-to-sample classification of three–dimensional (3-D) objects. The results obtained, even though currently referring to a simulated environment and to some working assumptions, have emphasized the validity of the approach and its applicability in a variety of dextrous robotic systems.},
annote = {Use computer simulations of grasps to demonstrate the potential of SOMs to recognise shapes from touch.},
author = {Faldella, E. and Fringuelli, B. and Passeri, D. and Rosi, L.},
doi = {10.1109/41.564167},
issn = {02780046},
journal = {IEEE Transactions on Industrial Electronics},
keywords = {3-D object recognition,Analytical models,Folder - Haptic-Visual integration,Information analysis,Iron,Neural networks,Probes,Robot sensing systems,Robot vision systems,Solid modeling,dextrous robotic systems,haptic interfaces,image classification,match-to-sample classification,neural approach,object recognition,robotic haptic recognition,robots,self-organising feature maps,simulated environment,unsupervised Kohonen self-organizing feature map,unsupervised learning},
mendeley-tags = {3-D object recognition,Analytical models,Folder - Haptic-Visual integration,Information analysis,Iron,Neural networks,Probes,Robot sensing systems,Robot vision systems,Solid modeling,dextrous robotic systems,haptic interfaces,image classification,match-to-sample classification,neural approach,object recognition,robotic haptic recognition,robots,self-organising feature maps,simulated environment,unsupervised Kohonen self-organizing feature map,unsupervised learning},
number = {2},
pages = {267--269},
title = {{A neural approach to robotic haptic recognition of 3-d objects based on a kohonen self-organizing feature map}},
url = {http://ieeexplore.ieee.org/ielx1/41/12253/00564167.pdf?tp={\&}arnumber=564167{\&}isnumber=12253 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=564167},
volume = {44},
year = {1997}
}
@article{Felzenszwalb2010,
abstract = {We describe an object detection system based on mixtures of multiscale deformable part models. Our system is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL datasets. Our system relies on new methods for discriminative training with partially labeled data. We combine a margin- sensitive approach for data-mining hard negative examples with a formalism we call latent SVM. A latent SVM is a reformulation of MI-SVM in terms of latent variables. A latent SVM is semi-convex and the training problem becomes convex once latent information is specified for the positive examples. This leads to an iterative training algorithm that alternates between fixing latent values for positive examples and optimizing the latent SVM objective function.},
annote = {Deformable part models (DPM), combine a coarse detection stage with a fine matching stage for locating and identifying objects based on the appearance and relative location of their parts.},
author = {Felzenszwalb, Pedro F. and Girshick, Ross B. and Mcallester, David and Ramanan, Deva},
doi = {10.1109/TPAMI.2009.167},
isbn = {0162-8828 VO - 32},
issn = {1939-3539},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {9},
pages = {1--20},
pmid = {20634557},
title = {{Object Detection with Discriminatively Trained Part Based Models}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5255236},
volume = {32},
year = {2009}
}
@inproceedings{Fergus2003,
abstract = {We present a method to learn and recognize object class models from unlabeled and unsegmented cluttered scenes in a scale invariant manner. Objects are modeled as flexible constellations of parts. A probabilistic representation is used for all aspects of the object: shape, appearance, occlusion and relative scale. An entropy-based feature detector is used to select regions and their scale within the image. In learning the parameters of the scale-invariant object model are estimated. This is done using expectation-maximization in a maximum-likelihood setting. In recognition, this model is used in a Bayesian manner to classify images. The flexible nature of the model is demonstrated by excellent results over a range of datasets including geometrically constrained classes (e.g. faces, cars) and flexible objects (such as animals).},
annote = {
notes 19/02
},
author = {Fergus, R. and Perona, P. and Zisserman, A.},
booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2003.1211479},
file = {:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fergus, Perona, Zisserman - 2003 - Object class recognition by unsupervised scale-invariant learning.pdf:pdf;:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fergus, Perona, Zisserman - 2003 - Object class recognition by unsupervised scale-invariant learning.html:html},
isbn = {0-7695-1900-8},
issn = {1063-6919},
keywords = {Animals,Bayes methods,Bayesian classification,Bayesian methods,Computer vision,Detectors,Feature extraction,Folder - constellation-star models,Layout,Maximum likelihood detection,Maximum likelihood estimation,Shape,Solid modeling,entropy-based feature detection,expectation-maximization,flexible model,flexible object,geometrically constrained class,image classification,image recognition,image region selection,image representation,learning (artificial intelligence),maximum entropy methods,maximum-likelihood setting,object appearance,object aspect,object class model,object class recognition,object modeling,object occlusion,object recognition,object shape,optimisation,parameter learning,probabilistic representation,relative scale,scale invariant manner,scale-invariant object model estimation,unlabeled cluttered scene,unsegmented cluttered scene,unsupervised scale-invariant learning},
mendeley-tags = {Animals,Bayes methods,Bayesian classification,Bayesian methods,Computer vision,Detectors,Feature extraction,Folder - constellation-star models,Layout,Maximum likelihood detection,Maximum likelihood estimation,Shape,Solid modeling,entropy-based feature detection,expectation-maximization,flexible model,flexible object,geometrically constrained class,image classification,image recognition,image region selection,image representation,learning (artificial intelligence),maximum entropy methods,maximum-likelihood setting,object appearance,object aspect,object class model,object class recognition,object modeling,object occlusion,object recognition,object shape,optimisation,parameter learning,probabilistic representation,relative scale,scale invariant manner,scale-invariant object model estimation,unlabeled cluttered scene,unsegmented cluttered scene,unsupervised scale-invariant learning},
month = {jun},
pages = {264--271},
title = {{Object class recognition by unsupervised scale-invariant learning}},
url = {http://ieeexplore.ieee.org/ielx5/8603/27266/01211479.pdf?tp={\&}arnumber=1211479{\&}isnumber=27266 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1211479{\&}tag=1},
volume = {2},
year = {2003}
}
@inproceedings{Fergus2005,
abstract = { We present a "parts and structure" model for object category recognition that can be learnt efficiently and in a semi-supervised manner: the model is learnt from example images containing category instances, without requiring segmentation from background clutter. The model is a sparse representation of the object, and consists of a star topology configuration of parts modeling the output of a variety of feature detectors. The optimal choice of feature types (whose repertoire includes interest points, curves and regions) is made automatically. In recognition, the model may be applied efficiently in an exhaustive manner, bypassing the need for feature detectors, to give the globally optimal match within a query image. The approach is demonstrated on a wide variety of categories, and delivers both successful classification and localization of the object within the image.},
annote = {STAR-models are a simplification over constellation models which requires a special part (called landmark) to be always present, and encodes the structure of the object as the relative position of the other parts with respect to the landmark.},
author = {Fergus, R. and Perona, P. and Zisserman, A.},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2005.47},
isbn = {0769523722},
issn = {10636919},
keywords = {Cats,Computer vision,Detectors,Feature extraction,Folder - constellation-star models,Image edge detection,Image segmentation,Optimal matching,Region 1,Topology,efficient learning,exhaustive recognition,feature detectors,image recognition,learning (artificial intelligence),object category recognition,object detection,object recognition,sparse object category model,star topology configuration},
mendeley-tags = {Cats,Computer vision,Detectors,Feature extraction,Folder - constellation-star models,Image edge detection,Image segmentation,Optimal matching,Region 1,Topology,efficient learning,exhaustive recognition,feature detectors,image recognition,learning (artificial intelligence),object category recognition,object detection,object recognition,sparse object category model,star topology configuration},
pages = {380--387},
title = {{A sparse object category model for efficient learning and exhaustive recognition}},
url = {http://ieeexplore.ieee.org/ielx5/9901/31472/01467293.pdf?tp={\&}arnumber=1467293{\&}isnumber=31472 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1467293{\&}tag=1},
volume = {1},
year = {2005}
}
@inproceedings{Fine2001,
abstract = {We propose a framework based on a parametric quadratic programming (QP) technique to solve the support vector machine (SVM) training problem. This framework, can be specialized to obtain two SVM optimization methods. The first solves the fixed bias problem, while the second starts with an optimal solution for a fixed bias problem and adjusts the bias until the optimal value is found. The later method can be applied in conjunction with any other existing technique which obtains a fixed bias solution. Moreover, the second method can also be used independently to solve the complete SVM training problem. A combination of these two methods is more flexible than each individual method and, among other things, produces an incremental algorithm which exactly solve the 1-Norm Soft Margin SVM optimization problem. Applying Selective Sampling techniques may further boost convergence.},
annote = {Similar to [Cauwenbergh 00] but can perform batch increments of any size and can solve fixed bias problems (Cauwenbergh 00 solves for b as part of the optimisation).},
author = {Fine, S and Scheinberg, K},
booktitle = {Advances in Neural Information Processing Systems 14, Vols 1 and 2},
file = {:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fine, Scheinberg - 2002 - Incremental learning and selective sampling via parametric optimization framework for SVM.pdf:pdf;:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fine, Scheinberg - 2002 - Incremental learning and selective sampling via parametric optimization framework for SVM.html:html},
isbn = {1049-5258},
issn = {10495258},
keywords = {Folder - Incremental,support},
mendeley-tags = {Folder - Incremental},
pages = {705--711$\backslash$r1594},
publisher = {MIT Press},
title = {{Incremental learning and selective sampling via parametric optimization framework for SVM}},
url = {{\textless}Go to ISI{\textgreater}://000180520100088},
volume = {14},
year = {2002}
}
@article{Fischler1981,
abstract = {A new paradigm, Random Sample Consensus (RANSAC), for fitting a model to experimental data is introduced. RANSAC is capable of interpreting/smoothing data containing a significant percentage of gross errors, and is thus ideally suited for applications in automated image analysis where interpretation is based on the data provided by error-prone feature detectors. A major portion of this paper describes the application of RANSAC to the Location Determination Problem (LDP): Given an image depicting a set of landmarks with known locations, determine that point in space from which the image was obtained. In response to a RANSAC requirement, new results are derived on the minimum number of landmarks needed to obtain a solution, and algorithms are presented for computing these minimum-landmark solutions in closed form. These results provide the basis for an automatic system that can solve the LDP under difficult viewing},
author = {Fischler, Martin a. and Bolles, Robert C.},
doi = {10.1145/358669.358692},
isbn = {0934613338},
issn = {00010782},
journal = {Communications of the ACM},
keywords = {automated cartography,camera calibration,image matching,location determination,model fitting,scene analysis},
mendeley-tags = {automated cartography,camera calibration,image matching,location determination,model fitting,scene analysis},
month = {jun},
number = {6},
pages = {381--395},
shorttitle = {Random Sample Consensus},
title = {{Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography}},
url = {http://portal.acm.org/citation.cfm?doid=358669.358692},
volume = {24},
year = {1981}
}
@inproceedings{Frieß1998,
abstract = {Support Vector Machines work by mapping training data for classi cation tasks into a high dimensional feature space. In the feature space they then nd a maximal margin hyperplane which separates the data. This hyperplane is usually found using a quadratic programming routine which is computationally intensive, and is non trivial to implement. In this paper we propose an adaptation of the Adatron algorithm for classi cation with kernels in high dimensional spaces. The algorithm is simple and can nd a solution very rapidly with an exponentially fast rate of convergence (in the number of iterations) towards the optimal solution. Experimental results with real and arti cial datasets are provided.},
address = {San Francisco, CA, USA},
author = {Frie{\ss}, Thilo-Thomas and Cristianini, Nello and Campbell, Colin},
booktitle = {Proceedings of the Fifteenth International Conference on Machine Learning},
isbn = {1-55860-556-8},
keywords = {Folder - SVM large dataset,adatron,large margin clas-,si er,statistical mechanics,support vector machine},
mendeley-tags = {Folder - SVM large dataset},
pages = {188--196},
publisher = {Morgan Kaufmann Publishers Inc.},
series = {ICML '98},
shorttitle = {The Kernel-Adatron Algorithm},
title = {{The Kernel-Adatron Algorithm: A Fast and Simple Learning Procedure for Support Vector Machines}},
url = {http://dl.acm.org/citation.cfm?id=645527.657470},
year = {1998}
}
@incollection{Frome2004,
abstract = {Recognition of three dimensional (3D) objects in noisy and cluttered scenes is a challenging problem in 3D computer vision. One approach that has been successful in past research is the regional shape descriptor. In this paper, we introduce two new regional shape descriptors: 3D shape contexts and harmonic shape contexts. We evaluate the performance of these descriptors on the task of recognizing vehicles in range scans of scenes using a database of 56 cars. We compare the two novel descriptors to an existing descriptor, the spin image, showing that the shape context based descriptors have a higher recognition rate on noisy scenes and that 3D shape contexts outperform the others on cluttered scenes.},
author = {Frome, Andrea and Huber, Daniel and Kolluri, Ravi and B{\"{u}}low, Thomas and Malik, Jitendra},
booktitle = {Eccv},
doi = {10.1007/978-3-540-24672-5_18},
editor = {Pajdla, Tom{\'{a}}{\v{s}} and Matas, Jiř{\'{i}}},
isbn = {978-3-540-21982-8},
issn = {{\textless}null{\textgreater}},
keywords = {Artificial Intelligence (incl. Robotics),Computer Graphics,Image Processing and Computer Vision,matching,pattern recognition},
language = {en},
mendeley-tags = {Artificial Intelligence (incl. Robotics),Computer Graphics,Image Processing and Computer Vision,pattern recognition},
month = {jan},
pages = {224--237},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Recognizing Objects in Range Data Using Regional Point Descriptors}},
url = {http://dx.doi.org/10.1007/978-3-540-24672-5{\_}18$\backslash$nhttp://www.springerlink.com/content/d4ukq18fpbfe9la6},
volume = {3023},
year = {2004}
}
@inproceedings{Golovinskiy2009,
abstract = {This paper investigates the design of a system for recognizing$\backslash$n$\backslash$nobjects in 3D point clouds of urban environments.$\backslash$n$\backslash$nThe system is decomposed into four steps: locating, segmenting,$\backslash$n$\backslash$ncharacterizing, and classifying clusters of 3D$\backslash$n$\backslash$npoints. Specifically, we first cluster nearby points to form$\backslash$n$\backslash$na set of potential object locations (with hierarchical clustering).$\backslash$n$\backslash$nThen, we segment points near those locations into$\backslash$n$\backslash$nforeground and background sets (with a graph-cut algorithm).$\backslash$n$\backslash$nNext, we build a feature vector for each point cluster$\backslash$n$\backslash$n(based on both its shape and its context). Finally, we label$\backslash$n$\backslash$nthe feature vectors using a classifier trained on a set of manually$\backslash$n$\backslash$nlabeled objects. The paper presents several alternative$\backslash$n$\backslash$nmethods for each step. We quantitatively evaluate the$\backslash$n$\backslash$nsystem and tradeoffs of different alternatives in a truthed$\backslash$n$\backslash$npart of a scan of Ottawa that contains approximately 100$\backslash$n$\backslash$nmillion points and 1000 objects of interest. Then, we use$\backslash$n$\backslash$nthis truth data as a training set to recognize objects amidst$\backslash$n$\backslash$napproximately 1 billion points of the remainder of the Ottawa$\backslash$n$\backslash$nscan.},
author = {Golovinskiy, A and Kim, V G and Funkhouser, T},
booktitle = {IEEE 12th International Conference on Computer Vision, 2009},
doi = {10.1109/ICCV.2009.5459471},
isbn = {978-1-4244-4420-5},
issn = {1550-5499},
keywords = {3D Point Cloud,3D point clouds,Clouds,Feature extraction,Image segmentation,feature vectors,graph cut algorithm,hierarchical clustering,object detection,objects recognition,pattern clustering,points segmentation,quantitative evaluation,shape based recognition,shape recognition,urban,urban environments},
mendeley-tags = {3D point clouds,Clouds,Feature extraction,Image segmentation,feature vectors,graph cut algorithm,hierarchical clustering,object detection,objects recognition,pattern clustering,points segmentation,quantitative evaluation,shape based recognition,shape recognition,urban environments},
month = {sep},
pages = {2154--2161},
title = {{Shape-based recognition of 3d point clouds in urban environments}},
url = {http://ieeexplore.ieee.org/xpl/login.jsp?tp={\&}arnumber=5459471{\&}url=http{\%}3A{\%}2F{\%}2Fieeexplore.ieee.org{\%}2Fxpls{\%}2Fabs{\_}all.jsp{\%}3Farnumber{\%}3D5459471},
year = {2009}
}
@incollection{Gorges2010a,
abstract = {This paper presents a new approach to generate a strategy for haptic object exploration. Each voxel of the exploration area is assigned to an attention value which depends on the surrounding structure, the distance to the hand, the distance to already visited points and the focus of the exploration. The voxel with the highest attention value is taken as the next point of interest. This exploration loop results in a point cloud which is classified using an Iterative-Closest-Point algorithm. The approach is evaluated in a simulation environment which includes in particular a simulation of tactile sensors.},
annote = {
Simulation of tactile object recognition and active exploration. Volumetric representation of objects is sought, whereby each voxel is assigned an "attention value" to decide which point to explore next. The resulting point cloud is used for classification. They demonstrate that a strategical exploration significantly outperforms random exploration.
},
author = {Gorges, Nicolas and Fritz, Peter and Woern, Heinz},
booktitle = {Ki 2010: Advances in Artificial Intelligence},
editor = {Dillmann, R{\"{u}}diger and Beyerer, J{\"{u}}rgen and Hanebeck, Uwe D. and Schultz, Tanja},
isbn = {978-3-642-16110-0},
keywords = {Artificial Intelligence (incl. Robotics),Data Mining and Knowledge Discovery,Information Systems Applications (incl.Internet),Multimedia Information Systems,Simulation and Modeling,User Interfaces and Human Computer Interaction,haptic exploration,object recognition,simulation},
mendeley-tags = {Artificial Intelligence (incl. Robotics),Data Mining and Knowledge Discovery,Information Systems Applications (incl.Internet),Multimedia Information Systems,Simulation and Modeling,User Interfaces and Human Computer Interaction,haptic exploration,object recognition,simulation},
month = {jan},
pages = {349--357},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Haptic Object Exploration Using Attention Cubes}},
url = {{\textless}Go to ISI{\textgreater}://WOS:000312450300040},
volume = {6359},
year = {2010}
}
@inproceedings{Gorges2010,
abstract = {This paper presents a novel approach for haptic object recognition with an anthropomorphic robot hand. Firstly, passive degrees of freedom are introduced to the tactile sensor system of the robot hand. This allows the planar tactile sensor patches to optimally adjust themselves to the object's surface and to acquire additional sensor information for shape reconstruction. Secondly, this paper presents an approach to classify an object directly from the haptic sensor data acquired by a palpation sequence with the robot hand - without building a 3d-model of the object. Therefore, a finite set of essential finger positions and tactile contact patterns are identified which can be used to describe a single palpation step. A palpation sequence can then be merged into a simple statistical description of the object and finally be classified. The proposed approach for haptic object recognition and the new tactile sensor system are evaluated with an anthropomorphic robot hand.},
annote = {
compares features extracted via PCA, binary and moments. Moments prevails, which may be due to the low resolution of the sensors (binary does as well as PCA)

----------

tactile and kinesthetic produces higher accuracy than either alone. Best when the decision is a weighted sum of the decision of each system, instead of combining features.

----------

obtain much better results with sensors that adapt to the object's shape

----------

Introduces "haptic key features" (combination of tactile and kinaesthetic)
},
author = {Gorges, Nicolas and Navarro, Stefan Escaida and G{\"{o}}ger, Dirk and W{\"{o}}rn, Heinz},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ROBOT.2010.5509553},
isbn = {9781424450381},
issn = {10504729},
keywords = {Anthropomorphism,Folder - TOR by BOF,Robot sensing systems,Robotics and automation,Sensor systems,Shape,Surface reconstruction,Transmission line matrix methods,anthropomorphic robot hand,dexterous manipulators,haptic interfaces,haptic key features,haptic object recognition,object recognition,palpation sequence,passive joints,shape reconstruction,tactile sensor system,tactile sensors},
mendeley-tags = {Anthropomorphism,Folder - TOR by BOF,Robot sensing systems,Robotics and automation,Sensor systems,Shape,Surface reconstruction,Transmission line matrix methods,anthropomorphic robot hand,dexterous manipulators,haptic interfaces,haptic key features,haptic object recognition,object recognition,palpation sequence,passive joints,shape reconstruction,tactile sensor system,tactile sensors},
pages = {2349--2355},
title = {{Haptic object recognition using passive joints and haptic key features}},
url = {http://ieeexplore.ieee.org/ielx5/5501116/5509124/05509553.pdf?tp={\&}arnumber=5509553{\&}isnumber=5509124 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5509553},
year = {2010}
}
@inproceedings{Gorges2011,
abstract = {This work presents a point cloud approach for haptic object recognition with an anthropomorphic robot hand. It introduces several statistical point cloud features to provide robust descriptions of objects. It addresses the domain specific problems of sparsely populated and distorted point clouds that result from the direct interaction with the object. Also the contact normals registered during exploration {\&}x2014; a natural byproduct {\&}x2014; are taken into account for computing some of these features.},
annote = {
Creates a point cloud volumetric representation of an object after a number of grasps and attempts to classify it by using kNN.
},
author = {Gorges, Nicolas and Navarro, Stefan Escaida and Worn, Heinz},
booktitle = {2011 15th International Conference on Advanced Robotics (ICAR)},
doi = {10.1109/ICAR.2011.6088637},
isbn = {978-1-4577-1159-6},
keywords = {Dexterous Manipulation,Histograms,Humanoid Robots,Shape,Vectors,anthropomorphic robot hand,computational geometry,contact normals,distorted point clouds,haptic interfaces,haptic object recognition,manipulators,natural byproduct,object recognition,point cloud approach,robust object descriptions,sparsely populated point clouds,statistical analysis,statistical point cloud features,tactile sensors},
mendeley-tags = {Histograms,Shape,Vectors,anthropomorphic robot hand,computational geometry,contact normals,distorted point clouds,haptic interfaces,haptic object recognition,manipulators,natural byproduct,object recognition,point cloud approach,robust object descriptions,sparsely populated point clouds,statistical analysis,statistical point cloud features,tactile sensors},
pages = {15--20},
title = {{Haptic object recognition using statistical point cloud features}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6088637},
year = {2011}
}
@inproceedings{Gorges2014,
abstract = {This work analyses tactile imprints as observed on multi-fingered robot hands - in particular on an anthropomorphic robot hand and on an industrial 3-finger gripper. Various alternatives of representing the contact shapes are shown and evaluated in the scope of a recognition task. Another focus is on the question of what kind of contacts actually occurr and how sensor patches are involved according to their placement in the robot hand. The evaluation of the approach considers two different tactile sensors, one representing a larger surface as the palm and one smaller sensor integrated into the fingers of the robot hand. {\textcopyright} 2014 IEEE.},
annote = {
Analyses different ways of encoding tactile information gathered by three systems (palm, hand, gripper) and two different tactile sensors (palm and fingertips). Uses SOMs to identify the most common contact categories. Does not attempt recognition, but suggests contact patterns analysis using moments is promising.
},
author = {Gorges, Nicolas and Navarro, Stefan Escaida and Worn, Heinz},
booktitle = {2014 2nd RSI/ISM International Conference on Robotics and Mechatronics, ICRoM 2014},
doi = {10.1109/ICRoM.2014.6990998},
file = {:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gorges, Navarro, Worn - 2014 - Analysis of tactile imprints for multi-fingered robot hands.html:html},
isbn = {9781479967438},
keywords = {Grippers,Histograms,Service robots,Shape,Vectors,anthropomorphic robot hand,contact shape,dexterous manipulators,industrial 3-finger gripper,multifingered robot hand,sensor patches,tactile imprint,tactile sensor,tactile sensors},
mendeley-tags = {Grippers,Histograms,Service robots,Shape,Vectors,anthropomorphic robot hand,contact shape,dexterous manipulators,industrial 3-finger gripper,multifingered robot hand,sensor patches,tactile imprint,tactile sensor,tactile sensors},
month = {oct},
pages = {779--784},
title = {{Analysis of tactile imprints for multi-fingered robot hands}},
url = {http://ieeexplore.ieee.org/xpls/icp.jsp?arnumber=6990998{\&}tag=1},
year = {2014}
}
@inproceedings{Guler2014,
abstract = {Robots operating in household environments need to interact with food containers of different types. Whether a container is filled with milk, juice, yogurt or coffee may affect the way robots grasp and manipulate the container. In this paper, we concentrate on the problem of identifying what kind of content is in a container based on tactile and/or visual feedback in combination with grasping. In particular, we investigate the benefits of using unimodal (visual or tactile) or bimodal (visual-tactile) sensory data for this purpose. We direct our study toward cardboard containers with liquid or solid content or being empty. The motivation for using grasping rather than shaking is that we want to investigate the content prior to applying manipulation actions to a container. Our results show that we achieve comparable classification rates with unimodal data and that the visual and tactile data are complimentary.},
annote = {KUKA+Schunk hand+Kinect. Grasp containers and identify if they are empty, or filled (and to discern the content from amongs rice, flour, yogurt, water, empty), by squeezing them and observing the results (deformation and pressure).


They conclude that the task is achievable with either modality alone, but the combination can provide complementary information.

Limitation: Tracking assumes objects are rigid.},
author = {Guler, Puren and Bekiroglu, Yasemin and Gratal, Xavi and Pauwels, Karl and Kragic, Danica},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2014.6943119},
file = {:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Guler et al. - 2014 - What's in the container Classifying object contents from vision and touch.html:html},
isbn = {9781479969340},
issn = {21530866},
keywords = {Containers,Folder - Haptic-Visual integration,Grasping,Robot sensing systems,Visualization,bimodal sensory data,cardboard containers,image classification,manipulators,object content classification,robot grasping,robot vision,tactile feedback,unimodal sensory data,visual feedback},
mendeley-tags = {Containers,Folder - Haptic-Visual integration,Grasping,Robot sensing systems,Visualization,bimodal sensory data,cardboard containers,image classification,manipulators,object content classification,robot grasping,robot vision,tactile feedback,unimodal sensory data,visual feedback},
month = {sep},
pages = {3961--3968},
shorttitle = {What's in the container?},
title = {{What's in the container? Classifying object contents from vision and touch}},
url = {http://ieeexplore.ieee.org/xpls/icp.jsp?arnumber=6943119},
year = {2014}
}
@inproceedings{Hadfield2011a,
abstract = {The motion field of a scene can be used for object segmentation and to provide features for classification tasks like action recognition. Scene flow is the full 3D motion field of the scene, and is more difficult to estimate than it's 2D counterpart, optical flow. Current approaches use a smoothness cost for regularisation, which tends to over-smooth at object boundaries. This paper presents a novel formulation for scene flow estimation, a collection of moving points in 3D space, modelled using a particle filter that supports multiple hypotheses and does not oversmooth the motion field. In addition, this paper is the first to address scene flow estimation, while making use of modern depth sensors and monocular appearance images, rather than traditional multi-viewpoint rigs. The algorithm is applied to an existing scene flow dataset, where it achieves comparable results to approaches utilising multiple views, while taking a fraction of the time.},
annote = {
Uses 6D scene particles (more than one particle can share a position hypothesis, but have different velocity hypotheses).


 
----------

Particles spawned with gaussian noise (whose sigma reduces with time). Weight = number of children

----------

{\textless}10 seconds per update
},
author = {Hadfield, Simon and Bowden, Richard},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2011.6126509},
isbn = {9781457711015},
issn = {1550-5499},
keywords = {3D motion field,Cameras,Estimation,Feature extraction,Folder - scene flow - dense scene flow,Image segmentation,Optical imaging,Optical sensors,Three dimensional displays,Vectors,action recognition,depth sensor,image classification,image motion analysis,image sensors,image sequences,monocular appearance images,multiview point rigs,object segmentation,optical flow,particle based scene flow,particle filter,particle filtering (numerical methods),scene flow estimation},
mendeley-tags = {3D motion field,Cameras,Estimation,Feature extraction,Folder - scene flow - dense scene flow,Image segmentation,Optical imaging,Optical sensors,Three dimensional displays,Vectors,action recognition,depth sensor,image classification,image motion analysis,image sensors,image sequences,monocular appearance images,multiview point rigs,object segmentation,optical flow,particle based scene flow,particle filter,particle filtering (numerical methods),scene flow estimation},
month = {nov},
pages = {2290--2295},
shorttitle = {Kinecting the dots},
title = {{Kinecting the dots: Particle based scene flow from depth sensors}},
url = {http://ieeexplore.ieee.org/ielx5/6118259/6126217/06126509.pdf?tp={\&}arnumber=6126509{\&}isnumber=6126217 http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6126509{\&}navigation=1},
year = {2011}
}
@article{Hadfield2014,
abstract = {In this paper, an algorithm is presented for estimating scene flow, which is a richer, 3D analog of optical flow. The approach operates orders of magnitude faster than alternative techniques and is well suited to further performance gains through parallelized implementation. The algorithm employs multiple hypotheses to deal with motion ambiguities, rather than the traditional smoothness constraints, removing oversmoothing errors and providing significant performance improvements on benchmark data, over the previous state of the art. The approach is flexible and capable of operating with any combination of appearance and/or depth sensors, in any setup, simultaneously estimating the structure and motion if necessary. Additionally, the algorithm propagates information over time to resolve ambiguities, rather than performing an isolated estimation at each frame, as in contemporary approaches. Approaches to smoothing the motion field without sacrificing the benefits of multiple hypotheses are explored, and a probabilistic approach to occlusion estimation is demonstrated, leading to 10 and 15 percent improved performance, respectively. Finally, a data-driven tracking approach is described, and used to estimate the 3D trajectories of hands during sign language, without the need to model complex appearance variations at each viewpoint.},
annote = {
Extends (Hadfield 2011) to have an equal number of particles per ray. It also handles occlusion explicitly by only labelling points as occluded if their predicted position is "behind" another point with high probability on the ray.

----------

Exemplified by tracking unknown objects with simple clustering.
},
author = {Hadfield, Simon and Bowden, Richard},
doi = {10.1109/TPAMI.2013.162},
isbn = {0162-8828 VO  - 36},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {3D,3D hand trajectory estimation,3D motion,3D optical flow analog,3D tracking,Equations,Estimation,Folder - scene flow - dense scene flow,Motion estimation,Motion segmentation,Optical sensors,Scene flow,Smoothing methods,Sociology,Statistics,Tracking,bilateral filter,data-driven tracking approach,hand tracking,image sequences,motion ambiguities,motion estimation,motion field smoothing,motion segmentation,object tracking,occlusion,occlusion estimation,optical flow,parallel processing,parallelized implementation,particle,particle filter,probabilistic approach,probabilistic occlusion,probability,scene particles,sign language,sign language recognition,smoothness constraints,structure estimation,tracking,unregularized particle-based scene flow estimation},
mendeley-tags = {3D,3D hand trajectory estimation,3D motion,3D optical flow analog,3D tracking,Equations,Estimation,Folder - scene flow - dense scene flow,Motion estimation,Motion segmentation,Optical sensors,Scene flow,Smoothing methods,Sociology,Statistics,Tracking,bilateral filter,data-driven tracking approach,hand tracking,image sequences,motion ambiguities,motion field smoothing,object tracking,occlusion,occlusion estimation,optical flow,parallel processing,parallelized implementation,particle,particle filter,probabilistic approach,probabilistic occlusion,probability,scene particles,sign language,sign language recognition,smoothness constraints,structure estimation,unregularized particle-based scene flow estimation},
month = {mar},
number = {3},
pages = {564--576},
pmid = {24457511},
shorttitle = {Scene Particles},
title = {{Scene particles: Unregularized particle-based scene flow estimation}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6587031{\&}navigation=1},
volume = {36},
year = {2014}
}
@inproceedings{Hadfield2011,
abstract = {The motion field of a scene can be used for object segmentation and to provide features for classification tasks like action recognition. Scene flow is the full 3D motion field of the scene, and is more difficult to estimate than it's 2D counterpart, optical flow. Current approaches use a smoothness cost for regularisation, which tends to over-smooth at object boundaries. This paper presents a novel formulation for scene flow estimation, a collection of moving points in 3D space, modelled using a particle filter that supports multiple hypotheses and does not oversmooth the motion field. In addition, this paper is the first to address scene flow estimation, while making use of modern depth sensors and monocular appearance images, rather than traditional multi-viewpoint rigs. The algorithm is applied to an existing scene flow dataset, where it achieves comparable results to approaches utilising multiple views, while taking a fraction of the time.},
author = {Hadfield, Simon and Bowden, Richard},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2011.6126509},
isbn = {9781457711015},
issn = {1550-5499},
keywords = {3D motion field,Cameras,Estimation,Feature extraction,Image segmentation,Optical imaging,Optical sensors,Three dimensional displays,Vectors,action recognition,depth sensor,image classification,image motion analysis,image sensors,image sequences,monocular appearance images,multiview point rigs,object segmentation,optical flow,particle based scene flow,particle filter,particle filtering (numerical methods),scene flow estimation},
mendeley-tags = {3D motion field,Cameras,Estimation,Feature extraction,Image segmentation,Optical imaging,Optical sensors,Three dimensional displays,Vectors,action recognition,depth sensor,image classification,image motion analysis,image sensors,image sequences,monocular appearance images,multiview point rigs,object segmentation,optical flow,particle based scene flow,particle filter,particle filtering (numerical methods),scene flow estimation},
month = {nov},
pages = {2290--2295},
shorttitle = {Kinecting the dots},
title = {{Kinecting the dots: Particle based scene flow from depth sensors}},
url = {http://ieeexplore.ieee.org/xpls/icp.jsp?arnumber=6126509},
year = {2011}
}
@article{Wu2013,
annote = {
State of the art results in cross depictive style and mixted depictive style. But database is small.

----------

Uses Arbelaez11Contour to retrieve a graph of child-parent segmented regions. This is then filterede to grate a graph of parts for a picture. Class model is computed as the median graph of all graphs of exemplars of a given class. Class prediction is the one that maximises a matching energy between a tested image and the graph of the class. The comparison matches node (part) descreiption and edge (relative location).
},
author = {Hall, Peter},
doi = {10.5244/C.27.23},
isbn = {1-901725-49-9},
journal = {Bmvc2013},
pages = {1--12},
title = {{Modelling Visual Objects Invariant to Depictive Style}},
url = {http://www.bmva.org/bmvc/2013/Papers/paper0023/paper0023.pdf},
year = {2013}
}
@book{Hartigan1975,
address = {New York, NY, USA},
author = {Hartigan, John A.$\backslash$},
edition = {99th},
isbn = {0-471-35645-X},
publisher = {John Wiley {\&} Sons, Inc.},
title = {{Clustering Algorithms}},
year = {1937}
}
@inproceedings{He2015,
abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra com-putational cost and little overfitting risk. Second, we de-rive a robust initialization method that particularly consid-ers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94{\%} top-5 test error on the ImageNet 2012 classifica-tion dataset. This is a 26{\%} relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66{\%} [29]). To our knowledge, our result is the first to surpass human-level per-formance (5.1{\%}, [22]) on this visual recognition challenge.},
archivePrefix = {arXiv},
arxivId = {1502.01852},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2015.123},
eprint = {1502.01852},
file = {:C$\backslash$:/Users/tadeo/Google Drive/phd/other{\_}pdf/He{\_}Delving{\_}Deep{\_}into{\_}ICCV{\_}2015{\_}paper.pdf:pdf},
isbn = {978-1-4673-8391-2},
month = {dec},
pages = {1026--1034},
publisher = {IEEE},
title = {{Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7410480},
year = {2015}
}
@inproceedings{Hebert2011,
abstract = {This paper develops a method to fuse stereo vision, force-torque sensor, and joint angle encoder measurements to estimate and track the location of a grasped object within the hand. We pose the problem as a hybrid systems estimation problem, where the continuous states are the object 6D pose, finger contact location, wrist-to-camera transform and the discrete states are the finger contact modes with the object. This paper develops the key measurement equations that govern the fusion process. Experiments with a Barrett Hand, Bumblebee 2 stereo camera, and an ATI omega force-torque sensor validate and demonstrate the method.},
annote = {Recovers the pose of a grasped object whose geometry is known. It combines stereo-vision, propioception and hand sensors via concatenation and uses a Static Multiple Model Estimator (SMM) approach.},
author = {Hebert, Paul and Hudson, Nicolas and Ma, Jeremy and Burdick, Joel},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2011.5980185},
isbn = {9781612843865},
issn = {10504729},
keywords = {ATI omega,Barrett Hand,Bumblebee 2 stereo camera,Cameras,Estimation,Folder - Haptic-Visual integration,Joints,Mathematical model,Sensors,Stereo vision,Wrist,dexterous manipulators,discrete state,estimation theory,finger contact location,finger contact mode,force sensors,force-torque sensor,in-hand object location estimation,joint angle encoder measurement,object 6D pose,object detection,robots,stereo image processing,wrist-to-camera transform},
mendeley-tags = {ATI omega,Barrett Hand,Bumblebee 2 stereo camera,Cameras,Estimation,Folder - Haptic-Visual integration,Joints,Mathematical model,Sensors,Stereo vision,Wrist,dexterous manipulators,discrete state,estimation theory,finger contact location,finger contact mode,force sensors,force-torque sensor,in-hand object location estimation,joint angle encoder measurement,object 6D pose,object detection,robots,stereo image processing,wrist-to-camera transform},
pages = {5935--5941},
title = {{Fusion of stereo vision, force-torque, and joint sensors for estimation of in-hand object location}},
url = {http://ieeexplore.ieee.org/ielx5/5967842/5979525/05980185.pdf?tp={\&}arnumber=5980185{\&}isnumber=5979525 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5980185},
year = {2011}
}
@inproceedings{Heidemann2004,
abstract = {We propose a neural architecture for the recognition of objects by haptics. We demonstrate its performance for a set of household objects and toys using a low cost 2D pressure sensor of coarse resolution, which is moved by a robot arm guided by contact points. The approach transfers the well known view-based method from computer vision to the domain of tactile sensing. However, in contrast to computer vision, not static frames but entire time series of 2D pressure profiles are evaluated.},
annote = {
Local VQ and PCA to extract features which are fed to a NN to classify objects. Does not learn new objects after training. Remarkable 81{\%} using a low density sensor and no hard-coded feature extraction algorithms. Assumes pose is largely invariant.
},
author = {Heidemann, G. and Schopfer, M.},
booktitle = {IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA '04. 2004},
doi = {10.1109/ROBOT.2004.1307249},
isbn = {0-7803-8232-3},
issn = {1050-4729},
keywords = {Costs,Data acquisition,Hardware,Humans,Object recognition,Principal component analysis,Robot sensing systems,Sensor arrays,Shape,Solid modeling,data acquisition,dynamic tactile sensing,haptic interfaces,haptics,low cost 2D pressure sensor,manipulators,object identification,object recognition,pressure sensors,robot arm,tactile sensors},
mendeley-tags = {Costs,Hardware,Humans,Principal component analysis,Robot sensing systems,Sensor arrays,Shape,Solid modeling,data acquisition,dynamic tactile sensing,haptic interfaces,haptics,low cost 2D pressure sensor,manipulators,object identification,object recognition,pressure sensors,robot arm,tactile sensors},
month = {apr},
pages = {813--818 Vol.1},
title = {{Dynamic tactile sensing for object identification}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1307249},
volume = {1},
year = {2004}
}
@inproceedings{Heidemann2001,
abstract = {We propose a new algorithm for vector quantization, the Activity Equalization Vector quantization (AEV). It is based on the winner takes all rule with an additional supervision of the average node activities over a training interval and a subsequent re-positioning of those nodes with low average activities. The re-positioning is aimed to both an exploration of the data space and a better approximation of already discovered data clusters by an equalization of the node activities. We introduce a learning scheme for AEV which requires as previous knowledge about the data only their bounding box. Using an example of Martinetz et al. [1], AEV is compared with the Neural Gas, Frequency Sensitive Competitive Learning (FSCL) and other standard algorithms. It turns out to converge much faster and requires less computational effort.},
annote = {
Introduces AEV (activity equalization VQ) a variant on VQ which deals with problems such as poorly used nodes (idle or dead).
},
author = {Heidemann, Gunther and Ritter, Helge},
booktitle = {Neural Processing Letters},
doi = {10.1023/A:1009678928250},
issn = {13704621},
keywords = {Clustering,Codebook generation,Competitive learning,Neural gas,Unsupervised learning,Vector quantization,Winner takes all},
number = {1},
pages = {17--30},
title = {{Efficient vector quantization using the WTA-rule with activity equalization}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=A7DE541A0A40F300474C50C37DDCF1A1?doi=10.1.1.92.7027{\&}rep=rep1{\&}type=pdf http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.92.7027},
volume = {13},
year = {2001}
}
@inproceedings{Henry2010,
abstract = {RGB-D cameras are novel sensing systems that capture RGB images along with per-pixel depth information. RGB-D cameras rely on either structured light patterns combined with stereo sensing [6,10] or time-of-flight laser sensing [1] to generate depth estimates that can be associated with RGB pixels. Very soon, small, high-quality RGB-D cameras developed for computer gaming and home entertainment applications will become available at cost below {\$}100. In this paper we investigate how such cameras can be used in the context of robotics, specifically for building dense 3D maps of indoor environments. Such maps have applications in robot navigation, manipulation, semantic mapping, and telepresence. The robotics and computer vision communities have developed a variety of techniques for 3D mapping based on laser range scans [8, 11], stereo cameras [7], monocular cameras [3], and unsorted collections of photos [4]. While RGB-D cameras provide the opportunity to build 3D maps of unprecedented richness, they have drawbacks that make their application to 3D mapping difficult: They provide depth only up to a limited distance (typically less than 5m), depth values are much noisier than those provided by laser scanners, and their field of view ( ∼ 60 ◦ ) is far more constrained than that of specialized cameras or laser scanners typically used for 3D mapping ( ∼ 180 ◦). In our work, we use a camera developed by PrimeSense [10]. The key insights of this investigation are: first, that existing frame matching techniques are not sufficient to provide robust visual odometry with these cameras; second, that a tight integration of depth and color information can yield robust frame matching and loop closure detection; third, that building on best practice techniques in SLAM and computer graphics makes it possible to build and visualize accurate and extremely rich 3D maps with such cameras; and, fourth, that it will be feasible to build complete robot navigation and interaction systems solely based on cheap depth cameras.},
annote = {
3D registered SIFT matched with RANSAC and ICP on pointclouds to produce dense 3D mapping.
},
author = {Henry, P and Krainin, M and Herbst, E and Ren, X and Fox, D},
booktitle = {12th International Symposium on Experimental Robotics (ISER)},
shorttitle = {RGB-D Mapping},
title = {{RGB-D mapping: Using depth cameras for dense 3D modeling of indoor environments}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.226.91{\&}rep=rep1{\&}type=pdf http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.226.91},
year = {2010}
}
@inproceedings{Heo2013,
abstract = {Haptic is a tactile feedback technology which has the advantage of the sense of touch by applying forces, vibrations or motions to the user. To use the haptic device for remote control, it is need to sense the object. These sensors are called as haptic sensor, and it senses the magnitude, direction, and distribution of a force. The sensed force data are used to reproduce the boundary limit of remote user's movement or the reactions from objects. We develop an elastic material based tactile sensor for sensing the reaction force and the shape of contact surface. Sensor surface is made of Poly- dimethylsiloxane(PDMS) and it has small patterns with chrome. The deformation of these patterns is calculated by computer vision. By this process we could know the amount force that gave from some object. We performed the experiment with force sensor to find the relationship between the force and deformation of surface. {\textcopyright} 2013 IEEE.},
annote = {
CCD camera captures the deformation of regular patterns on a PDMS membrane.
},
author = {Heo, Geun Sub and Choi, Jin and Kim, Gyu Man and Lee, Choon Young and {Geun Sub Heo} and {Jin ho Choi} and {Gyu Man Kim} and {Choon Young Lee}},
booktitle = {Ieee Isr 2013},
doi = {10.1109/ISR.2013.6695693},
isbn = {978-1-4799-1173-8},
keywords = {4 keywords or phrases (in alphabetical order and s,Basic structure,Boundary limits,Computer vision,Contact surface,Deformation,Dimethylsiloxane,Elastic materials,End effectors,Force sensor,Force sensors,Haptic device,Haptic interfaces,Human-robot interaction,Image recognition,Microchannels,Micromechanical devices,PDMS membrane,Remote control,Robot,Robot sensing systems,Separated by commas,Tactile sensors,Telerobotics,Visualization,basic structure design,contact surface,elastic material,elasticity,end effector,end effectors,force sensor,force sensors,haptic device,haptic interfaces,haptic sensor,human-robot interaction,image recognition,poly-dimethylsiloxane,reaction force sensing,remote control,robot,robot sensing systems,robotics,sensor surface,sensors,tactile feedback,tactile feedback technology,tactile sensor,tactile sensors,telerobotics},
mendeley-tags = {4 keywords or phrases (in alphabetical order and s,Computer vision,Micromechanical devices,PDMS membrane,Robot sensing systems,Visualization,basic structure design,contact surface,elastic material,elasticity,end effector,end effectors,force sensor,force sensors,haptic device,haptic interfaces,haptic sensor,human-robot interaction,image recognition,poly-dimethylsiloxane,reaction force sensing,remote control,robot,sensor surface,tactile feedback technology,tactile sensor,tactile sensors,telerobotics},
month = {oct},
pages = {1--2},
shorttitle = {PDMS membrane based force sensor},
title = {{PDMS membrane based force sensor: Basic structure design and assessment}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6695693},
year = {2013}
}
@inproceedings{Herbst2011,
abstract = {The performance of indoor robots that stay in a single environment can be enhanced by gathering detailed knowledge of objects that frequently occur in that environment. We use an inexpensive sensor providing dense color and depth, and fuse information from multiple sensing modalities to detect changes between two 3-D maps. We adapt a recent SLAM technique to align maps. A probabilistic model of sensor readings lets us reason about movement of surfaces. Our method handles arbitrary shapes and motions, and is robust to lack of texture. We demonstrate the ability to find whole objects in complex scenes by regularizing over surface patches.},
annote = {
Builds on RGB-D mapping (P Henry 2010)

----------

Probabilistical matching of pointcloud region features to estimate the probability of a patch (surfel, Pfister 2000) having moved between two scenes.

----------

Not online (30 min per scene pair)
},
author = {Herbst, Evan and Henry, Peter and Ren, Xiaofeng and Fox, Dieter},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2011.5980542},
isbn = {9781612843865},
issn = {10504729},
keywords = {3D map,3D scene comparison,Cameras,Color,Data mining,Image color analysis,Measurement by laser beam,Robot sensing systems,SLAM (robots),SLAM technique,Sensors,Solid modeling,Surface reconstruction,image texture,indoor robots,mobile robots,multiple sensing modality,object detection,object discovery,object modeling,probabilistic model,robot vision,sensor reading,solid modelling,surface patch},
mendeley-tags = {3D map,3D scene comparison,Cameras,Color,Data mining,Image color analysis,Measurement by laser beam,Robot sensing systems,SLAM (robots),SLAM technique,Sensors,Solid modeling,Surface reconstruction,image texture,indoor robots,mobile robots,multiple sensing modality,object detection,object discovery,object modeling,probabilistic model,robot vision,sensor reading,solid modelling,surface patch},
month = {may},
pages = {2623--2629},
title = {{Toward object discovery and modeling via 3-D scene comparison}},
url = {http://ieeexplore.ieee.org/ielx5/5967842/5979525/05980542.pdf?tp={\&}arnumber=5980542{\&}isnumber=5979525 http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5980542{\&}navigation=1},
year = {2011}
}
@inproceedings{Herbst2013,
abstract = {combine depth with color},
annote = {
Variational method to determine the 3D flow of points between a pair of RGB-D images. Including color and depth consistency and regularization
},
author = {Herbst, Evan and Ren, Xiaofeng and Fox, Dieter},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2013.6630885},
isbn = {9781467356411},
issn = {10504729},
keywords = {Cameras,Computer vision,Folder - scene flow - dense scene flow,Image color analysis,Image segmentation,Motion estimation,Motion segmentation,Optical imaging,Optical sensors,RGB-D camera motion,RGB-D flow,Robustness,Smoothing methods,color information,dense 3D flow,dense 3D motion estimation,depth information,depth noise,image colour analysis,image sequences,natural scenes,object rigidity,rigid motion segmentation,robotics,scene complexity,scene flow formulation,two-frame variational 2D flow algorithm,two-frame variational 3D flow algorithm},
mendeley-tags = {Cameras,Computer vision,Folder - scene flow - dense scene flow,Image color analysis,Image segmentation,Motion estimation,Motion segmentation,Optical imaging,Optical sensors,RGB-D camera motion,RGB-D flow,Robustness,Smoothing methods,color information,dense 3D flow,dense 3D motion estimation,depth information,depth noise,image colour analysis,image sequences,natural scenes,object rigidity,rigid motion segmentation,robotics,scene complexity,scene flow formulation,two-frame variational 2D flow algorithm,two-frame variational 3D flow algorithm},
month = {may},
pages = {2276--2282},
shorttitle = {RGB-D flow},
title = {{RGB-D flow: Dense 3-D motion estimation using color and depth}},
url = {http://ieeexplore.ieee.org/ielx7/6615630/6630547/06630885.pdf?tp={\&}arnumber=6630885{\&}isnumber=6630547 http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6630885{\&}contentType=Conference+Publications},
year = {2013}
}
@incollection{Herrera2011,
abstract = {At present, autonomous, robotic dexterous manipulation in unknown environments still eludes us. Children only a few years old lift and manipulate unfamiliar objects more dexterously than today's robots. Thus, robotics researchers increasingly agree that ideas from biology can strongly benefit the design of autonomous robots.},
author = {Herrera, Rosana Matuk},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-23232-9_55},
editor = {Gro{\ss}, Roderich and Alboul, Lyuba and Melhuish, Chris and Witkowski, Mark and Prescott, Tony J. and Penders, Jacques},
isbn = {9783642232312},
issn = {03029743},
keywords = {Artificial Intelligence (incl. Robotics),Image Processing and Computer Vision,User Interfaces and Human Computer Interaction},
mendeley-tags = {Artificial Intelligence (incl. Robotics),Image Processing and Computer Vision,User Interfaces and Human Computer Interaction},
month = {jan},
pages = {416--417},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Tactile afferent simulation from pressure arrays}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-23232-9{\_}55 http://link.springer.com/content/pdf/10.1007{\%}2F978-3-642-23232-9{\_}55.pdf},
volume = {6856 LNAI},
year = {2011}
}
@inproceedings{Hetzel2001,
abstract = {The paper explores a view-based approach to recognize free-form objects in range images. We are using a set of local features that are easy to calculate and robust to partial occlusions. By combining those features in a multidimensional histogram, we can obtain highly discriminant classifiers without the need for segmentation. Recognition is performed using either histogram matching or a probabilistic recognition algorithm. We compare the performance of both methods in the presence of occlusions and test the system on a database of almost 2000 full-sphere views of 30 free-form objects. The system achieves a recognition accuracy above 93{\%} on ideal images, and of 89{\%} with 20{\%} occlusion.},
annote = {
Various forms of histogram comparison
},
author = {Hetzel, Gunter and Leibe, Bastian and Levi, Paul and Schiele, Bernt},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2001.990988},
isbn = {0-7695-1272-0},
keywords = {3D object recognition,Computer vision,Histograms,Image databases,Image segmentation,Robustness,Shape,Spatial databases,Testing,free-form object recognition,full-sphere views,hidden feature removal,highly discriminant classifiers,histogram matching,image matching,image recognition,local feature histograms,local features,multidimensional histogram,object recognition,occlusions,partial occlusions,probabilistic recognition algorithm,range images,recognition accuracy,view-based approach,visual databases},
mendeley-tags = {3D object recognition,Computer vision,Histograms,Image databases,Image segmentation,Robustness,Shape,Spatial databases,Testing,free-form object recognition,full-sphere views,hidden feature removal,highly discriminant classifiers,histogram matching,image matching,image recognition,local feature histograms,local features,multidimensional histogram,object recognition,occlusions,partial occlusions,probabilistic recognition algorithm,range images,recognition accuracy,view-based approach,visual databases},
pages = {II--394},
title = {{3D object recognition from range images using local feature histograms}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=990988},
volume = {2},
year = {2001}
}
@incollection{Hinterstoisser2013,
abstract = {We propose a framework for automatic modeling, detection, and tracking of 3D objects with a Kinect. The detection part is mainly based on the recent template-based LINEMOD approach [1] for object detection. We show how to build the templates automatically from 3D models, and how to estimate the 6 degrees-of-freedom pose accurately and in real-time. The pose estimation and the color information allow us to check the detection hypotheses and improves the correct detec-tion rate by 13{\%} with respect to the original LINEMOD. These many improvements make our framework suitable for object manipulation in Robotics applications. Moreover we propose a new dataset made of 15 registered, 1100+ frame video sequences of 15 various objects for the evaluation of future competing methods. Fig. 1. 15 different texture-less 3D objects are simultaneously detected with our ap-proach under different poses on heavy cluttered background with partial occlusion. Each detected object is augmented with its 3D model. We also show the corresponding coordinate systems.},
annote = {Given the 3D model of various objects, create a series of multi-view templates of RGB data and normals to achieve fast and very accurate ({\textgreater}96{\%}) object identification. Objects are textureless, so the focus for gradients is the objects silouhette. Time {\~{}}0.1s. Robust to cluttered backgrounds.},
author = {Hinterstoisser, Stefan and Lepetit, Vincent and Ilic, Slobodan and Holzer, Stefan and Konolige, Kurt and Navab, Nassir},
editor = {Lee, Kyoung Mu and Matsushita, Yasuyuki and Rehg, James M. and Hu, Zhanyi},
file = {:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hinterstoisser et al. - 2013 - Model Based Training , Detection and Pose Estimation of Texture-Less 3D Objects in Heavily Cluttered Scen.pdf:pdf;:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hinterstoisser et al. - 2013 - Model Based Training , Detection and Pose Estimation of Texture-Less 3D Objects in Heavily Cluttered Sce.html:html},
isbn = {978-3-642-37330-5 978-3-642-37331-2},
keywords = {Artificial Intelligence (incl. Robotics),Health Informatics,Image Processing and Computer Vision,pattern recognition},
mendeley-tags = {Artificial Intelligence (incl. Robotics),Health Informatics,Image Processing and Computer Vision,pattern recognition},
month = {jan},
pages = {1--14},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Model Based Training , Detection and Pose Estimation of Texture-Less 3D Objects in Heavily Cluttered Scenes}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-37331-2{\_}42 http://link.springer.com/content/pdf/10.1007{\%}2F978-3-642-37331-2{\_}42.pdf},
year = {2013}
}
@article{Hinton2006,
abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “autoencoder” networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
author = {Hinton, G E and Salakhutdinov, R R},
doi = {10.1126/science.1127647},
isbn = {3135786504},
issn = {0036-8075, 1095-9203},
journal = {Science},
language = {en},
month = {jul},
number = {5786},
pages = {504--507},
pmid = {16873662},
title = {{Reducing the Dimensionality of Data with Neural Networks}},
url = {http://www.sciencemag.org/content/313/5786/504$\backslash$nfiles/264/Science-Hinton-Salakhutdinov-2006-Reducing{\_}the{\_}Dimensionality{\_}of{\_}Data{\_}with{\_}Neural{\_}Networks.pdf},
volume = {313},
year = {2006}
}
@article{Hinton2007,
abstract = {To achieve its impressive performance in tasks such as speech perception or object recognition, the brain extracts multiple levels of representation from the sensory input. Backpropagation was the first computationally efficient model of how neural networks could learn multiple layers of representation, but it required labeled training data and it did not work well in deep networks. The limitations of backpropagation learning can now be overcome by using multilayer neural networks that contain top-down connections and training them to generate sensory data rather than to classify it. Learning multilayer generative models might seem difficult, but a recent discovery makes it easy to learn nonlinear distributed representations one layer at a time. ?? 2007 Elsevier Ltd. All rights reserved.},
annote = {
deep ANNs can be pretrained one layer at a time using RBMs
},
author = {Hinton, Geoffrey E.},
doi = {10.1016/j.tics.2007.09.004},
isbn = {1364-6613 (Print)$\backslash$r1364-6613 (Linking)},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
language = {English},
month = {jan},
number = {10},
pages = {428--434},
pmid = {17921042},
title = {{Learning multiple layers of representation}},
url = {http://www.cell.com/article/S1364661307002173/abstract http://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(07)00217-3 http://www.ncbi.nlm.nih.gov/pubmed/17921042},
volume = {11},
year = {2007}
}
@book{Horn1989,
address = {Cambridge, MA, USA},
author = {Horn, Berthold K P and Brooks, Michael J},
editor = {Horn, Berthold K. P. and Brooks, Michael J.},
isbn = {0-262-08183-0},
pages = {577},
publisher = {MIT Press},
title = {{Shape from Shading}},
year = {1989}
}
@article{Hornik1989,
abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators. ?? 1989.},
annote = {
Neural networks can approximate vrtiually any function.
},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
doi = {10.1016/0893-6080(89)90020-8},
eprint = {arXiv:1011.1669v3},
isbn = {08936080 (ISSN)},
issn = {08936080},
journal = {Neural Networks},
keywords = {Back-propagation networks,Feedforward networks,Mapping networks,Network representation capability,Sigma-Pi networks,Squashing functions,Stone-Weierstrass Theorem,Universal approximation},
mendeley-tags = {Back-propagation networks,Feedforward networks,Mapping networks,Network representation capability,Sigma-Pi networks,Squashing functions,Stone-Weierstrass Theorem,Universal approximation},
number = {5},
pages = {359--366},
pmid = {74},
title = {{Multilayer feedforward networks are universal approximators}},
url = {http://www.sciencedirect.com/science/article/pii/0893608089900208},
volume = {2},
year = {1989}
}
@article{Hu1962,
abstract = {In this paper a theory of two-dimensional moment invariants for planar geometric figures is presented. A fundamental theorem is established to relate such moment invariants to the well-known algebraic invariants. Complete systems of moment invariants under translation, similitude and orthogonal transformations are derived. Some moment invariants under general two-dimensional linear transformations are also included. Both theoretical formulation and practical models of visual pattern recognition based upon these moment invariants are discussed. A simple simulation program together with its performance are also presented. It is shown that recognition of geometrical patterns and alphabetical characters independently of position, size and orientation can be accomplished. It is also indicated that generalization is possible to include invariance with parallel projection.},
annote = {
Introduces Hu moments
},
author = {Hu, Ming Kuei},
doi = {10.1109/TIT.1962.1057692},
isbn = {0096-1000},
issn = {21682712},
journal = {IRE Transactions on Information Theory},
keywords = {Artificial intelligence,Bibliographies,Character recognition,Distribution functions,Image analysis,Information processing,Information theory,Senior members,Shape,decision theory,pattern recognition},
mendeley-tags = {Artificial intelligence,Bibliographies,Character recognition,Distribution functions,Image analysis,Information processing,Information theory,Senior members,Shape,decision theory,pattern recognition},
month = {feb},
number = {2},
pages = {179--187},
title = {{Visual Pattern Recognition by Moment Invariants}},
url = {http://ieeexplore.ieee.org/ielx5/4547527/22787/01057692.pdf?tp={\&}arnumber=1057692{\&}isnumber=22787 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1057692},
volume = {8},
year = {1962}
}
@inproceedings{Hu2011,
abstract = {This paper presents a system for retrieving photographs using free-hand sketched queries. Regions are extracted from each image by gathering nodes of a hierarchical image segmentation into a bag-of-regions (BoR) representation. The BoR represents object shape at multiple scales, encoding shape even in the presence of adjacent clutter. We extract a shape representation from each region, using the Gradient Field HoG (GF-HOG) descriptor which enables direct comparison with the sketched query. The retrieval pipeline yields significant performance improvements over the previous GF-HOG results reliant on single-scale Canny edge maps, and over leading descriptors (SIFT, SSIM) for visual search. In addition, our system enables localization of the sketched object within matching images.},
annote = {
Uses Arbelaez10Contour to create bags of segmented regions to compare edges (for sketches) and contours (for images) using a range of features. Average precision reaches 63{\%} (using GF-HOG).
},
author = {Hu, Rui and Wang, Tinghuai and Collomosse, John},
booktitle = {Proceedings - International Conference on Image Processing, ICIP},
doi = {10.1109/ICIP.2011.6116513},
isbn = {9781457713033},
issn = {15224880},
keywords = {Bag-of-Regions,Bag-of-visual-words,BoR,Clutter,Folder - BoW,GF-HOG,Gradient Field HoG,Histograms,Image edge detection,Image segmentation,Shape,Sketch based Image Retrieval (SBIR),Visualization,adjacent clutter,bag-of-regions approach,gradient methods,image matching,image retrieval,object shape,photographs retrieval,shape encoding,shape recognition,shape representation,sketch based image retrieval},
mendeley-tags = {Bag-of-Regions,Bag-of-visual-words,BoR,Clutter,Folder - BoW,GF-HOG,Gradient Field HoG,Histograms,Image edge detection,Image segmentation,Shape,Sketch based Image Retrieval (SBIR),Visualization,adjacent clutter,bag-of-regions approach,gradient methods,image matching,image retrieval,object shape,photographs retrieval,shape encoding,shape recognition,shape representation,sketch based image retrieval},
month = {sep},
pages = {3661--3664},
title = {{A bag-of-regions approach to sketch-based image retrieval}},
url = {http://ieeexplore.ieee.org/ielx5/6094293/6115588/06116513.pdf?tp={\&}arnumber=6116513{\&}isnumber=6115588 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6116513{\&}tag=1},
year = {2011}
}
@inproceedings{Huang2006,
abstract = { The detection and recognition of generic object categories with invariance to viewpoint, illumination, and clutter requires the combination of a feature extractor and a classifier. We show that architectures such as convolutional networks are good at learning invariant features, but not always optimal for classification, while Support Vector Machines are good at producing decision surfaces from wellbehaved feature vectors, but cannot learn complicated invariances. We present a hybrid system where a convolutional network is trained to detect and recognize generic objects, and a Gaussian-kernel SVM is trained from the features learned by the convolutional network. Results are given on a large generic object recognition task with six categories (human figures, four-legged animals, airplanes, trucks, cars, and "none of the above"), with multiple instances of each object category under various poses, illuminations, and backgrounds. On the test set, which contains different object instances than the training set, an SVM alone yields a 43.3{\%} error rate, a convolutional net alone yields 7.2{\%} and an SVM on top of features produced by the convolutional net yields 5.9{\%}.},
annote = {
NORB 5.9{\%} error

----------

Combine CNN and Gaussian Kernel SVMs to detect and recognise objects. Combination is better than either method alone.
},
author = {Huang, Fu Jie and LeCun, Yann},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2006.164},
isbn = {0769525970},
issn = {10636919},
keywords = {Feature extraction,Folder - convolutional,Gaussian processes,Humans,Large-scale systems,Lighting,Support vector machine classification,machine learning,object detection,object recognition,support vector machines},
mendeley-tags = {Feature extraction,Folder - convolutional,Gaussian processes,Humans,Large-scale systems,Lighting,Support vector machine classification,machine learning,object detection,object recognition,support vector machines},
month = {jun},
pages = {284--291},
pmid = {1000102573},
title = {{Large-scale learning with SVM and convolutional nets for generic object categorization}},
url = {http://ieeexplore.ieee.org/ielx5/10924/34373/01640771.pdf?tp={\&}arnumber=1640771{\&}isnumber=34373 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1640771},
volume = {1},
year = {2006}
}
@inproceedings{Huang2005b,
abstract = {In this paper, we propose a novel online algorithm for simultaneous localization and mapping (SLAM) in dynamic environments. We first formulate the problem with two interdependent parts: SLAM and multiple target tracking (MTT). To pursue online performance, we propose a hierarchical hybrid method to solve SLAM: locally by maximum likelihood (ML) with occupancy grid map, and globally by extended Kalman filter (EKF) with feature-based map. Meanwhile we apply a straightforward nearest neighborhood (NN) algorithm based on Euclidean metric to address MTT. In order to track multiple moving objects reliably, we propose an enhanced fuzzy clustering (EFC) method to segment 2D range images and reliably group objects. Experiments validated on Pioneer 2DX mobile robot with SICK LMS200 demonstrate the capability and robustness of the proposed algorithm},
annote = {
Local ML with voxels. Global EKF with features

----------

Bayesian approach, describing the relationship between SLAM and MTT as a DBN (bayesian network)

----------

SICK
},
author = {Huang, G. Q. and Rad, A. B. and Wong, Y. K.},
booktitle = {2005 International Conference on Advanced Robotics, ICAR '05, Proceedings},
doi = {10.1109/ICAR.2005.1507422},
isbn = {0780391772},
keywords = {Clustering algorithms,Euclidean metrics,Extended Kalman Filter (EKF),Fuzzy Clustering (FC),Humans,Image segmentation,Information filtering,Information filters,Kalman filters,Maximum Likelihood (ML),Maximum likelihood estimation,Multiple Target Tracking (MTT),Nearest Neighborhood (NN),Neural networks,Simultaneous Localization and Mapping (SLAM),dynamic environments,extended Kalman filter,feature-based map,fuzzy clustering,fuzzy set theory,maximum likelihood method,mobile robots,multiple target tracking,nearest neighborhood algorithm,object detection,occupancy grid map,online SLAM,robot vision,simultaneous localization,simultaneous localization and mapping,simultaneous mapping,target tracking},
mendeley-tags = {Clustering algorithms,Euclidean metrics,Humans,Image segmentation,Information filtering,Information filters,Kalman filters,Maximum likelihood estimation,Neural networks,dynamic environments,extended Kalman filter,feature-based map,fuzzy clustering,fuzzy set theory,maximum likelihood method,mobile robots,multiple target tracking,nearest neighborhood algorithm,object detection,occupancy grid map,online SLAM,robot vision,simultaneous localization,simultaneous localization and mapping,simultaneous mapping,target tracking},
month = {jul},
pages = {262--267},
title = {{Online SLAM in dynamic environments}},
url = {http://ieeexplore.ieee.org/ielx5/10070/32295/01507422.pdf?tp={\&}arnumber=1507422{\&}isnumber=32295 http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=1507422{\&}navigation=1},
volume = {2005},
year = {2005}
}
@inproceedings{Huang2005,
abstract = {In this paper, we propose a novel online algorithm for simultaneous localization and mapping (SLAM) in dynamic environments. We first formulate the problem with two interdependent parts: SLAM and multiple target tracking (MTT). To pursue online performance, we propose a hierarchical hybrid method to solve SLAM: locally by maximum likelihood (ML) with occupancy grid map, and globally by extended Kalman filter (EKF) with feature-based map. Meanwhile we apply a straightforward nearest neighborhood (NN) algorithm based on Euclidean metric to address MTT. In order to track multiple moving objects reliably, we propose an enhanced fuzzy clustering (EFC) method to segment 2D range images and reliably group objects. Experiments validated on Pioneer 2DX mobile robot with SICK LMS200 demonstrate the capability and robustness of the proposed algorithm},
author = {Huang, G. Q. and Rad, A. B. and Wong, Y. K.},
booktitle = {2005 International Conference on Advanced Robotics, ICAR '05, Proceedings},
doi = {10.1109/ICAR.2005.1507422},
isbn = {0780391772},
keywords = {Clustering algorithms,Euclidean metrics,Extended Kalman Filter (EKF),Fuzzy Clustering (FC),Humans,Image segmentation,Information filtering,Information filters,Kalman filters,Maximum Likelihood (ML),Maximum likelihood estimation,Multiple Target Tracking (MTT),Nearest Neighborhood (NN),Neural networks,Simultaneous Localization and Mapping (SLAM),dynamic environments,extended Kalman filter,feature-based map,fuzzy clustering,fuzzy set theory,maximum likelihood method,mobile robots,multiple target tracking,nearest neighborhood algorithm,object detection,occupancy grid map,online SLAM,robot vision,simultaneous localization,simultaneous localization and mapping,simultaneous mapping,target tracking},
mendeley-tags = {Clustering algorithms,Euclidean metrics,Humans,Image segmentation,Information filtering,Information filters,Kalman filters,Maximum likelihood estimation,Neural networks,dynamic environments,extended Kalman filter,feature-based map,fuzzy clustering,fuzzy set theory,maximum likelihood method,mobile robots,multiple target tracking,nearest neighborhood algorithm,object detection,occupancy grid map,online SLAM,robot vision,simultaneous localization,simultaneous localization and mapping,simultaneous mapping,target tracking},
month = {jul},
pages = {262--267},
title = {{Online SLAM in dynamic environments}},
url = {http://ieeexplore.ieee.org/xpls/icp.jsp?arnumber=1507422},
volume = {2005},
year = {2005}
}
@inproceedings{Huang2005a,
abstract = {In this paper, we propose a novel online algorithm for simultaneous localization and mapping (SLAM) in dynamic environments. We first formulate the problem with two interdependent parts: SLAM and multiple target tracking (MTT). To pursue online performance, we propose a hierarchical hybrid method to solve SLAM: locally by maximum likelihood (ML) with occupancy grid map, and globally by extended Kalman filter (EKF) with feature-based map. Meanwhile we apply a straightforward nearest neighborhood (NN) algorithm based on Euclidean metric to address MTT. In order to track multiple moving objects reliably, we propose an enhanced fuzzy clustering (EFC) method to segment 2D range images and reliably group objects. Experiments validated on Pioneer 2DX mobile robot with SICK LMS200 demonstrate the capability and robustness of the proposed algorithm},
author = {Huang, G. Q. and Rad, A. B. and Wong, Y. K.},
booktitle = {2005 International Conference on Advanced Robotics, ICAR '05, Proceedings},
doi = {10.1109/ICAR.2005.1507422},
isbn = {0780391772},
keywords = {Clustering algorithms,Euclidean metrics,Extended Kalman Filter (EKF),Fuzzy Clustering (FC),Humans,Image segmentation,Information filtering,Information filters,Kalman filters,Maximum Likelihood (ML),Maximum likelihood estimation,Multiple Target Tracking (MTT),Nearest Neighborhood (NN),Neural networks,Simultaneous Localization and Mapping (SLAM),dynamic environments,extended Kalman filter,feature-based map,fuzzy clustering,fuzzy set theory,maximum likelihood method,mobile robots,multiple target tracking,nearest neighborhood algorithm,object detection,occupancy grid map,online SLAM,robot vision,simultaneous localization,simultaneous localization and mapping,simultaneous mapping,target tracking},
mendeley-tags = {Clustering algorithms,Euclidean metrics,Humans,Image segmentation,Information filtering,Information filters,Kalman filters,Maximum likelihood estimation,Neural networks,dynamic environments,extended Kalman filter,feature-based map,fuzzy clustering,fuzzy set theory,maximum likelihood method,mobile robots,multiple target tracking,nearest neighborhood algorithm,object detection,occupancy grid map,online SLAM,robot vision,simultaneous localization,simultaneous localization and mapping,simultaneous mapping,target tracking},
month = {jul},
pages = {262--267},
title = {{Online SLAM in dynamic environments}},
url = {http://ieeexplore.ieee.org/xpls/icp.jsp?arnumber=1507422},
volume = {2005},
year = {2005}
}
@article{Ilonen2013,
abstract = {In this work, we propose to reconstruct a complete three-dimensional (3-D) model of an unknown object by fusion of visual and tactile information while the object is grasped. Assuming the object is symmetric, a first hypothesis of its complete 3-D shape is generated. A grasp is executed on the object with a robotic manipulator equipped with tactile sensors. Given the detected contacts between the fingers and the object, the initial full object model including the symmetry parameters can be refined. This refined model will then allow the planning of more complex manipulation tasks. The main contribution of this work is an optimal estimation approach for the fusion of visual and tactile data applying the constraint of object symmetry. The fusion is formulated as a state estimation problem and solved with an iterated extended Kalman filter. The approach is validated experimentally using both artificial and real data from two different robotic platforms. {\textcopyright} The Author(s) 2013.},
annote = {Fuse vision RGB-D with touch using IEKF to refine a 3d model of an observed object},
author = {Ilonen, J. and Bohg, J. and Kyrki, V.},
doi = {10.1177/0278364913497816},
isbn = {978-1-4673-5643-5},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
keywords = {3-D visual perception,Folder - Haptic-Visual integration,Object shape reconstruction,Sensor fusion,Tactile sensing,Three dimensional,Visual perception,grasping,object shape reconstruction,sensor fusion,sensors,tactile sensing},
language = {en},
mendeley-tags = {Folder - Haptic-Visual integration},
month = {oct},
number = {2},
pages = {321--341},
title = {{Three-dimensional object reconstruction of symmetric objects by fusing visual and tactile sensing}},
url = {http://ijr.sagepub.com/cgi/doi/10.1177/0278364913497816},
volume = {33},
year = {2014}
}
@inproceedings{Iravani2011,
abstract = {The challenge addressed in this paper is the classification of visual objects by robots. Visual classification is an active field within Computer Vision, with excellent results achieved recently. However, not all of the advances transfer into the study of robots in free environments; two differences stand out. One is that Computer Vision algorithms often rely on batch learning over a large but fixed data set, whereas free robots cannot predict the objects they will encounter, making batch learning inappropriate. The second difference is that Computer Vision algorithms often assume a passive relationship with their input to the world, but robots can actively affect the world around them. The main contributions of the paper are to demonstrate: (i) that an on-line version of a successful batch classifier can be adapted so that objects are treated as topic mixtures rather than single topics; and (ii) that robots can self-supervise their learning of such models by interacting with the environment.},
author = {Iravani, Pejman and Hall, Peter and Beale, Daniel and Charron, Cyril and Hicks, Yulia},
booktitle = {2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)},
doi = {10.1109/ICCVW.2011.6130372},
isbn = {978-1-4673-0063-6},
keywords = {Accuracy,Computer vision,Databases,Dictionaries,Training,Visualization,batch classifier,batch learning,image classification,learning (artificial intelligence),robot vision,robots,self-supervised learning,topic mixture,visual object classification},
mendeley-tags = {Accuracy,Computer vision,Databases,Dictionaries,Training,Visualization,batch classifier,batch learning,image classification,learning (artificial intelligence),robot vision,robots,self-supervised learning,topic mixture,visual object classification},
month = {nov},
pages = {1092--1099},
title = {{Visual object classification by robots, using on-line, self-supervised learning}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6130372},
year = {2011}
}
@article{Irie2013,
abstract = {Despite significant progress, most existing visual dictionary learning methods rely on image descriptors alone or together with class labels. However, Web images are often associated with text data which may carry substantial information regarding image semantics, and may be exploited for visual dictionary learning. This paper explores this idea by leveraging relational information between image descriptors and textual words via co-clustering, in addition to information of image descriptors. Existing co-clustering methods are not optimal for this problem because they ignore the structure of image descriptors in the continuous space, which is crucial for capturing visual characteristics of images. We propose a novel Bayesian co-clustering model to jointly estimate the underlying distributions of the continuous image descriptors as well as the relationship between such distributions and the textual words through a unified Bayesian inference. Extensive experiments on image categorization and retrieval have validated the substantial value of the proposed joint modeling in improving visual dictionary learning, where our model shows superior performance over several recent methods.},
author = {Irie, Go and Liu, Dong and Li, Zhenguo and Chang, Shih Fu},
doi = {10.1109/CVPR.2013.49},
file = {:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Irie et al. - 2013 - A bayesian approach to multimodal visual dictionary learning.pdf:pdf},
isbn = {978-0-7695-4989-7},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer  Vision and Pattern Recognition},
keywords = {co-clustering,multimodal,visual dictionary},
pages = {329--336},
title = {{A bayesian approach to multimodal visual dictionary learning}},
volume = {25},
year = {2013}
}
@article{Jamali2011,
abstract = {In this paper, we present an application of machine learning to distinguish between different materials based on their surface texture. Such a system can be used for the estimation of surface friction during manipulation tasks; quality assurance in the textile, cosmetics, and harvesting industries; and other appli- cations requiring tactile sensing. Several machine learning algo- rithms, such as naive Bayes, decision trees, and naive Bayes trees, have been trained to distinguish textures sensed by a biologically inspired artificial finger.The finger has randomlydistributed strain gauges and polyvinylidene fluoride (PVDF) films embedded in sil- icone. Different textures induce different intensities of vibrations in the silicone. Consequently, textures can be distinguished by the presence of different frequencies in the signal. The data from the finger are preprocessed, and the Fourier coefficients of the sen- sor outputs are used to train classifiers. We show that the classi- fiers generalize well for unseen datasets with performance exceed- ing previously reported algorithms. Our classifiers can distinguish between different materials, such as carpet, flooring vinyls, tiles, sponge, wood, and polyvinyl-chloride (PVC) woven mesh with an accuracy of 95{\%} ±4{\%} on unseen test data.},
annote = {
Analyses the Fourier coefficients of the vibrations resulting from sliding a sensor finger along a surface. Feeds the feature vector onto a novel voting mechanism using various algorithms, the best performing of which was a Boosting NB Tree algorithm, which produced a performance of 95{\%}.
},
author = {Jamali, Nawid and Sammut, Claude},
doi = {10.1109/TRO.2011.2127110},
isbn = {1552-3098},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Fourier coefficient,Frequency-domain analysis,Friction,Grippers,Humans,Materials,Strain,Surface texture,biologically inspired artificial finger,dexterous manipulators,learning (artificial intelligence),machine learning,machine learning algorithm,manipulation task,material classification,pattern classification,polyvinylidene fluoride film,quality assurance,randomly distributed strain gauge,silicone,silicones,strain gauges,surface friction estimation,surface texture classification,tactile sensing,tactile sensors,texture classification,train classifier,vibration intensity,vibrations},
mendeley-tags = {Fourier coefficient,Frequency-domain analysis,Friction,Grippers,Humans,Materials,Strain,Surface texture,biologically inspired artificial finger,dexterous manipulators,learning (artificial intelligence),machine learning,machine learning algorithm,manipulation task,material classification,pattern classification,polyvinylidene fluoride film,quality assurance,randomly distributed strain gauge,silicone,silicones,strain gauges,surface friction estimation,surface texture classification,tactile sensing,tactile sensors,texture classification,train classifier,vibration intensity,vibrations},
number = {3},
pages = {508--521},
shorttitle = {Majority Voting},
title = {{Majority voting: Material classification by tactile sensing using surface texture}},
url = {http://ieeexplore.ieee.org/ielx5/8860/5784192/05756488.pdf?tp={\&}arnumber=5756488{\&}isnumber=5784192 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5756488},
volume = {27},
year = {2011}
}
@inproceedings{Jang2012,
abstract = {This paper represents 3D object recognition, which is an extension of the common feature point-based object recognition, based on novel descriptors utilizing local angles (for shape), gradient orientations (for texture of corners), and color information. First, the proposed algorithm extracts complementary feature points by randomly sampling the positions of the object edges. Then, it generates the proposed descriptors combining local angle patterns, gradient orientations, and color information. After making the descriptors, the method learns a codebook to enable the proposed algorithm to integrate the extracted feature points into a histogram through this codebook. Finally, the method classifies the query histogram based on a classifier. We expect that the proposed algorithm is robust to less textured and similar-shaped objects. The proposed method could be used as a core technology of the initial step of the information retrieval.},
annote = {
Proposes combining a range of complementary features to deal with textureless objects but do not provide empirical testing of their ideas.
},
author = {Jang, Youngkyoon and Woo, Woontack},
booktitle = {Proceedings - 2012 International Symposium on Ubiquitous Virtual Reality, ISUVR 2012},
doi = {10.1109/ISUVR.2012.20},
file = {:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jang, Woo - 2012 - Local feature descriptors for 3D object recognition in ubiquitous virtual reality.pdf:pdf;:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jang, Woo - 2012 - Local feature descriptors for 3D object recognition in ubiquitous virtual reality.html:html},
isbn = {9780769547664},
keywords = {3D object recognition,Bag-of-Words (BoWs),Feature extraction,Histograms,Image color analysis,Local Angle Pattern (LAP),Robustness,Sensors,Shape,classifier-based query histogram,codebook,color information,edge detection,feature point-based object recognition,feature points extraction,gradient methods,gradient orientations,image classification,image colour analysis,image matching,image retrieval,image sampling,information retrieval,local angle patterns,local feature descriptors,object edges position,object recognition,query processing,random sampling,similar-shaped objects,solid modelling,ubiquitous computing,ubiquitous virtual reality,virtual reality},
mendeley-tags = {3D object recognition,Bag-of-Words (BoWs),Feature extraction,Histograms,Image color analysis,Local Angle Pattern (LAP),Robustness,Sensors,Shape,classifier-based query histogram,codebook,color information,edge detection,feature point-based object recognition,feature points extraction,gradient methods,gradient orientations,image classification,image colour analysis,image matching,image retrieval,image sampling,information retrieval,local angle patterns,local feature descriptors,object edges position,object recognition,query processing,random sampling,similar-shaped objects,solid modelling,ubiquitous computing,ubiquitous virtual reality,virtual reality},
pages = {42--45},
title = {{Local feature descriptors for 3D object recognition in ubiquitous virtual reality}},
url = {http://ieeexplore.ieee.org/ielx5/6296787/6296795/06296806.pdf?tp={\&}arnumber=6296806{\&}isnumber=6296795 http://ieeexplore.ieee.org/xpl/abstractCitations.jsp?arnumber=6296806},
year = {2012}
}
@inproceedings{Jentoft2013a,
abstract = {Many applications for tactile sensors require a flexible, stretchable array to allow installation on curved surfaces or to measure forces on deformable objects. This paper presents a sensor array created with barometers and flexible printed circuit boards that delivers high sensitivity on a flexible, stretchable package using commercial off-the-shelf (COTS) components: MEMS barometers and commercially-compatible flexible printed circuit boards. The array is demonstrated on the surface of a jamming gripper, where it provides the ability to sense grasping events and detect object shape.},
author = {Jentoft, Leif P. and Tenzer, Yaroslav and Vogt, Daniel and Wood, Robert J. and Howe, Robert D.},
booktitle = {2013 16th International Conference on Advanced Robotics (ICAR)},
doi = {10.1109/ICAR.2013.6766525},
isbn = {978-1-4799-2722-7},
keywords = {COTS components,Grippers,Jamming,MEMS barometers,Robot sensing systems,Rubber,Sensitivity,Sensor arrays,Wires,barometers,commercial off-the-shelf,commercially compatible flexible printed circuit b,deformable objects,deformation,electronics packaging,flexible array,flexible electronics,flexible package,force measurement,grippers,jamming gripper,microsensors,object detection,object shape detection,printed circuits,sense grasping,sensor arrays,stretchable package,stretchable tactile array,tactile sensor array,tactile sensors},
month = {nov},
pages = {1--6},
publisher = {IEEE},
title = {{Flexible, stretchable tactile arrays from MEMS barometers}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6766525},
year = {2013}
}
@inproceedings{Jentoft2013,
abstract = {Many applications for tactile sensors require a flexible, stretchable array to allow installation on curved surfaces or to measure forces on deformable objects. This paper presents a sensor array created with barometers and flexible printed circuit boards that delivers high sensitivity on a flexible, stretchable package using commercial off-the-shelf (COTS) components: MEMS barometers and commercially-compatible flexible printed circuit boards. The array is demonstrated on the surface of a jamming gripper, where it provides the ability to sense grasping events and detect object shape.},
annote = {
Attempts ave been made to make open source tactile sensors of high reliability and durability and low sensor shift, in order to reducing the cost and increase customisability. These units, named Takktile, are based on arrays of MEMS barometers covered by a flexible compressible rubber membrane, the deformation of which results in the pressure changes measured.
},
author = {Jentoft, Leif P. and Tenzer, Yaroslav and Vogt, Daniel and Wood, Robert J. and Howe, Robert D.},
booktitle = {2013 16th International Conference on Advanced Robotics (ICAR)},
doi = {10.1109/ICAR.2013.6766525},
isbn = {978-1-4799-2722-7},
keywords = {COTS components,Grippers,Jamming,MEMS barometers,Robot sensing systems,Rubber,Sensitivity,Sensor arrays,Wires,barometers,commercial off-the-shelf,commercially compatible flexible printed circuit b,deformable objects,deformation,electronics packaging,flexible array,flexible electronics,flexible package,force measurement,grippers,jamming gripper,microsensors,object detection,object shape detection,printed circuits,sense grasping,sensor arrays,stretchable package,stretchable tactile array,tactile sensor array,tactile sensors},
mendeley-tags = {COTS components,Grippers,Jamming,MEMS barometers,Robot sensing systems,Rubber,Sensitivity,Sensor arrays,Wires,barometers,commercial off-the-shelf,commercially compatible flexible printed circuit b,deformable objects,deformation,electronics packaging,flexible array,flexible electronics,flexible package,force measurement,jamming gripper,microsensors,object detection,object shape detection,printed circuits,sense grasping,stretchable package,stretchable tactile array,tactile sensor array,tactile sensors},
month = {nov},
pages = {1--6},
title = {{Flexible, stretchable tactile arrays from MEMS barometers}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6766525},
year = {2013}
}
@inproceedings{Jin2013,
abstract = {In this paper a novel approach is proposed for tactile shape recognition, which uses tactile point location and normal information. Superquadric functions are applied to construct several shape primitives and k-means unsupervised clustering method is used to partition the objects as several patches. By extracting geometrical features from each patch and rearranging features, object feature vectors are constructed for Gaussian process (GP) classifier to identify object shapes. Simulations results prove that our approach can achieve a high recognition rate in object shape classification task from sparse and noisy tactile point clouds.},
annote = {{\textless}div class="standard"{\textgreater}Attempting to address the sparsity and noise problems in point clouds obtained by tactile exploration, use clustering to subdivide the point cloud into regions which are then encoded as features. These features are then classified using a Gaussian Process (with a squared exponential kernel) for object classification. This is therefore a bag-of-tactile-features approach (see section sec{\_}bag{\_}of{\_}tactile{\_}features). Simulations of 8 shape primitives (e.g. pyramid, cone, etc.) give a high accuracy for recognition.},
author = {Jin, Minghe and Gu, Haiwei and Fan, Shaowei and Zhang, Yuanfei and Liu, Hong},
booktitle = {2013 IEEE International Conference on Robotics and Biomimetics (ROBIO)},
doi = {10.1109/ROBIO.2013.6739518},
isbn = {978-1-4799-2744-9},
keywords = {Accuracy,Computer Graphics,Computer graphics,Feature extraction,GP classifier,Gaussian Processes,Gaussian process classifier,Gaussian processes,Geometry,Haptic interfaces,Image classification,Object shape recognition,Shape,Shape recognition,Tactile exploration,Tactile sensors,Vectors,accuracy,geometrical feature extraction,haptic interfaces,image classification,k-means unsupervised clustering method,noisy tactile point clouds,object feature vectors,object partition,object recognition,object shape classification task,object shape identification,object shape recognition,pattern clustering,recognition rate,shape primitives,shape recognition,sparse tactile point clouds,superquadric functions,tactile exploration,tactile point location,tactile sensors,tactile shape recognition},
mendeley-tags = {Accuracy,Computer Graphics,Feature extraction,GP classifier,Gaussian process classifier,Gaussian processes,Geometry,Shape,Vectors,geometrical feature extraction,haptic interfaces,image classification,k-means unsupervised clustering method,noisy tactile point clouds,object feature vectors,object partition,object recognition,object shape classification task,object shape identification,object shape recognition,pattern clustering,recognition rate,shape primitives,shape recognition,sparse tactile point clouds,superquadric functions,tactile exploration,tactile point location,tactile sensors,tactile shape recognition},
month = {dec},
number = {December},
pages = {558--562},
title = {{Object shape recognition approach for sparse point clouds from tactile exploration}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6739518},
year = {2013}
}
@article{Joachims2009,
abstract = {Discriminative training approaches like structural SVMs have shown much promise for building highly complex and accurate models in areas like natural language processing, protein structure prediction, and information retrieval. However, current train- ing algorithms are computationally expensive or intractable on large datasets. To overcome this bottleneck, this paper explores how cutting-plane methods can provide fast training not only for classification SVMs, but also for structural SVMs.We show that for an equivalent “1-slack” reformulation of the linear SVM training problem, our cutting-plane method has time complexity linear in the number of training examples. In particular, the number of it- erations does not depend on the number of training examples, and it is linear in the desired precision and the regularization parameter. Furthermore, we present an extensive empirical evaluation of the method applied to binary classification, multi-class classification, HMM sequence tagging, and CFG parsing. The experiments show that the cutting-plane algorithm is broadly applicable and fast in practice. On large datasets, it is typically several orders of magnitude faster than conventional training methods derived from decomposition methods like SVM-light, or conventional cutting-plane methods. Implementations of our methods are available at www.joachims.org.},
author = {Joachims, Thorsten and Finley, Thomas and Yu, Chun Nam John},
doi = {10.1007/s10994-009-5108-8},
file = {:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Joachims, Finley, Yu - 2009 - Cutting-plane training of structural SVMs.pdf:pdf;:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Joachims, Finley, Yu - 2009 - Cutting-plane training of structural SVMs.html:html},
isbn = {0885-6125},
issn = {08856125},
journal = {Machine Learning},
keywords = {Artificial Intelligence (incl. Robotics),Computing Methodologies,Control - Robotics- Mechatronics,Folder - SSVMs,Language Translation and Linguistics,Simulation and Modeling,Structural SVMs,Structured output prediction,Support vector machines,Training algorithms,support vector machines},
language = {en},
mendeley-tags = {Artificial Intelligence (incl. Robotics),Computing Methodologies,Control - Robotics- Mechatronics,Folder - SSVMs,Language Translation and Linguistics,Simulation and Modeling,Structural SVMs,Structured output prediction,Training algorithms,support vector machines},
month = {oct},
number = {1},
pages = {27--59},
title = {{Cutting-plane training of structural SVMs}},
url = {http://link.springer.com/article/10.1007/s10994-009-5108-8 http://link.springer.com/content/pdf/10.1007{\%}2Fs10994-009-5108-8.pdf},
volume = {77},
year = {2009}
}
@inproceedings{Johnsson2007,
abstract = {We have experimented with proprioception in a bio-inspired self-organizing haptic system. To this end a 12 d.o.f. anthropomorphic robot hand with proprioceptive sensors was developed. The system uses a self-organizing map for the mapping of the explored objects. In our experiments the system was trained and tested with 10 different objects of different sizes from two different shape categories. To estimate the generalization ability the system was also tested with 6 new objects. The system showed good performance with the objects from both the training set as well as in the generalization experiment. In both cases the system was able to discriminate the shape, the size and to some extent the individual objects.},
annote = {
it is possible to classify objects (according to size, and to some extent into distinct classes) using only propioceptive information.
},
author = {{Johnsson Magnus} and {Balkenius Christian}},
booktitle = {Towards Autonomous Robotic Systems},
pages = {239--245},
title = {{Experiments with Proprioception in a Self-Organizing  System for Haptic Perception}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=EAF725EBF52697A9C8AEDFAAC94AA98B?doi=10.1.1.142.2391{\&}rep=rep1{\&}type=pdf http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.142.2391},
year = {2007}
}
@article{Johnsson2011,
abstract = {We review a number of self-organizing-robot systems that are able to extract features from haptic sensory information. They are all based on self-organizing maps (SOMs). First, we describe a number of systems based on the three-fingered-robot hand, i.e., the Lund University Cognitive Science (LUCS) Haptic-Hand II, that successfully extracts the shapes of objects. These systems explore each object with a sequence of grasps while superimposing the information from individual grasps after cross-coding proprioceptive information for different parts of the hand and the registrations of tactile sensors. The cross-coding is done by employing either the tensor-product operation or a novel self-organizing neural network called the tensor multiple peak SOM (T-MPSOM). Second, we present a system based on proprioception that uses an anthropomorphic robot hand, i.e., the LUCS haptic-hand III. This system is able to distinguish objects both according to shape and size. Third, we present systems that are able to extract and combine the texture and hardness properties from explored materials.},
annote = {
Combine proprioceptive and tactile information using SOMs and T-MPSOMs to combine information from many grasps, to classify objects by shape and size

----------

LUCS haptic-hand II and III.
},
author = {Johnsson, Magnus and Balkenius, Christian},
doi = {10.1109/TRO.2011.2130090},
isbn = {1552-3098},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Cognitive robotics,Joints,Lund University cognitive science haptic hand II,Neurons,Robot sensing systems,Shape,Tensile stress,anthropomorphic robot hand,cognitive systems,control engineering computing,cross coding proprioceptive information,grasps sequence,haptic interfaces,haptic sensory information,manipulators,self-organising feature maps,self-organizing feature maps,self-organizing maps,tactile sensor registrations,tactile sensors,tensor multiple peak SOM,three flngered robot hand,unsupervised learning},
mendeley-tags = {Cognitive robotics,Joints,Lund University cognitive science haptic hand II,Neurons,Robot sensing systems,Shape,Tensile stress,anthropomorphic robot hand,cognitive systems,control engineering computing,cross coding proprioceptive information,grasps sequence,haptic interfaces,haptic sensory information,manipulators,self-organising feature maps,self-organizing feature maps,self-organizing maps,tactile sensor registrations,tactile sensors,tensor multiple peak SOM,three flngered robot hand,unsupervised learning},
number = {3},
pages = {498--507},
title = {{Sense of touch in robots with self-organizing maps}},
url = {http://ieeexplore.ieee.org/ielx5/8860/5784192/05739536.pdf?tp={\&}arnumber=5739536{\&}isnumber=5784192 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5739536{\&}tag=1},
volume = {27},
year = {2011}
}
@article{Karasuyama2010,
abstract = {We propose a multiple incremental decremental algorithm of support vector machines (SVM). In online learning, we need to update the trained model when some new observations arrive and/or some observations become obsolete. If we want to add or remove single data point, conventional single incremental decremental algorithm can be used to update the model efficiently. However, to add and/or remove multiple data points, the computational cost of current update algorithm becomes inhibitive because we need to repeatedly apply it for each data point. In this paper, we develop an extension of incremental decremental algorithm which efficiently works for simultaneous update of multiple data points. Some analyses and experimental results show that the proposed algorithm can substantially reduce the computational cost. Our approach is especially useful for online SVM learning in which we need to remove old data points and add new data points in a short amount of time.},
annote = {
Extends Cauwenberghs00 to allow for multiple datapoints to be added or removed simultaneously instead of one at a time.
},
author = {Karasuyama, Masayuki and Takeuchi, Ichiro},
doi = {10.1109/TNN.2010.2048039},
file = {:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Karasuyama, Takeuchi - 2010 - Multiple incremental decremental learning of support vector machines.pdf:pdf;:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Karasuyama, Takeuchi - 2010 - Multiple incremental decremental learning of support vector machines.html:html},
isbn = {1045-9227},
issn = {10459227},
journal = {IEEE Transactions on Neural Networks},
keywords = {Algorithms,Artificial intelligence,Computer Simulation,Folder - Incremental,Humans,Incremental decremental algorithm,Information Storage and Retrieval,Online Systems,Pattern Recognition- Automated,Support vector machine,incremental decremental algorithm,learning,learning (artificial intelligence),multiple incremental decremental learning,online SVM learning,path-following,support vector machines,support vector machines (SVM)},
mendeley-tags = {Algorithms,Artificial intelligence,Computer Simulation,Folder - Incremental,Humans,Information Storage and Retrieval,Online Systems,Pattern Recognition- Automated,Support vector machine,incremental decremental algorithm,learning,learning (artificial intelligence),multiple incremental decremental learning,online SVM learning,path-following,support vector machines,support vector machines (SVM)},
month = {jul},
number = {7},
pages = {1048--1059},
pmid = {20550990},
title = {{Multiple incremental decremental learning of support vector machines}},
url = {http://ieeexplore.ieee.org/ielx5/72/5504504/05484614.pdf?tp={\&}arnumber=5484614{\&}isnumber=5504504 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5484614},
volume = {21},
year = {2010}
}
@inproceedings{Katz2013,
abstract = {We present an interactive perceptual skill for segmenting, tracking, and modeling the kinematic structure of 3D articulated objects. This skill is a prerequisite for general manipulation in unstructured environments. Robot-environment interactions are used to move an unknown object, creating a perceptual signal that reveals the kinematic properties of the object. The resulting perceptual information can then inform and facilitate further manipulation. The algorithm is computationally efficient, handles partial occlusions, and depends on little object motion; it only requires sufficient texture for visual feature tracking. We conducted experiments with everyday objects on a robotic manipulation platform equipped with an RGB-D sensor. The results demonstrate the robustness of the proposed method to lighting conditions, object appearance, size, structure, and configuration.},
address = {Karlsruhe, Germany},
annote = {Online speed (under 50ms) and very accurate.----------Interaction is prescribed.----------Tracks KLT and SIFT features to classify kinematic relationships between rigid objects as prismatic (part of same rigid part), revolute (connected by rotation) or disconnected.},
author = {Katz, Dov and Kazemi, Moslem and {Andrew Bagnell}, J. and Stentz, Anthony},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2013.6631292},
isbn = {9781467356411},
issn = {10504729},
number = {March},
pages = {5003--5010},
title = {{Interactive segmentation, tracking, and kinematic modeling of unknown 3D articulated objects}},
url = {file:///home/tadeo/x-drive/conferences/ICRA2013/media/files/papers{\_}videos/1616.pdf http://ieeexplore.ieee.org/xpl/abstractCitations.jsp?tp={\&}arnumber=6631292{\&}queryText{\%}3DInteractive+Segmentation{\%}2C+Tracking{\%}2C+and+Kinematic},
year = {2013}
}
@inproceedings{Keller2013,
abstract = {Real-time or online 3D reconstruction has wide applicability and receives further interest due to availability of consumer depth cameras. Typical approaches use a moving sensor to accumulate depth measurements into a single model which is continuously refined. Designing such systems is an intricate balance between reconstruction quality, speed, spatial scale, and scene assumptions. Existing online methods either trade scale to achieve higher quality reconstructions of small objects/scenes. Or handle larger scenes by trading real-time performance and/or quality, or by limiting the bounds of the active reconstruction. Additionally, many systems assume a static scene, and cannot robustly handle scene motion or reconstructions that evolve to reflect scene changes. We address these limitations with a new system for real-time dense reconstruction with equivalent quality to existing online methods, but with support for additional spatial scale and robustness in dynamic scenes. Our system is designed around a simple and flat point-Based representation, which directly works with the input acquired from range/depth sensors, without the overhead of converting between representations. The use of points enables speed and memory efficiency, directly leveraging the standard graphics pipeline for all central operations, i.e., camera pose estimation, data association, outlier removal, fusion of depth maps into a single denoised model, and detection and update of dynamic objects. We conclude with qualitative and quantitative results that highlight robust tracking and high quality reconstructions of a diverse set of scenes at varying scales.},
annote = {
Reconstructs moving objects but does not reason about their movement. They are simply excluded from camera estimation

----------

Kinect

----------

Maintains a point representation of the world, with stable and unstable points. Reinforced points over multiple observations become stable. Noisy or moving points become unstable.


 },
author = {Keller, Maik and Lefloch, Damien and Lambers, Martin and Izadi, Shahram and Weyrich, Tim and Kolb, Andreas},
booktitle = {3Dv},
doi = {10.1109/3DV.2013.9},
isbn = {978-0-7695-5067-1},
keywords = {3D cameras and sensors,3D shape reconstruction,Cameras,Data models,Estimation,GPU,Kinect Fusion,Real-time,Real-time systems,Robustness,Surface reconstruction,active reconstruction,camera pose estimation,consumer depth camera availability,data association,denoised model,depth map fusion,depth measurements,depth sensors,distance measurement,dynamic object detection,dynamic object update,dynamic scene reconstruction,image fusion,image motion analysis,image reconstruction,iterative closest point algorithm,memory efficiency,moving sensor,natural scenes,object detection,object tracking,online 3D reconstruction,online method,outlier removal,point-based fusion,point-based representation,pose estimation,range sensors,real-time 3D reconstruction,real-time dense reconstruction,real-time performance,reconstruction quality,robust tracking,scene motion handling,standard graphics pipeline,static scene,three-dimensional displays},
mendeley-tags = {3D cameras and sensors,3D shape reconstruction,Cameras,Data models,Estimation,GPU,Kinect Fusion,Real-time,Real-time systems,Robustness,Surface reconstruction,active reconstruction,camera pose estimation,consumer depth camera availability,data association,denoised model,depth map fusion,depth measurements,depth sensors,distance measurement,dynamic object detection,dynamic object update,dynamic scene reconstruction,image fusion,image motion analysis,image reconstruction,iterative closest point algorithm,memory efficiency,moving sensor,natural scenes,object detection,object tracking,online 3D reconstruction,online method,outlier removal,point-based fusion,point-based representation,pose estimation,range sensors,real-time 3D reconstruction,real-time dense reconstruction,real-time performance,reconstruction quality,robust tracking,scene motion handling,standard graphics pipeline,static scene,three-dimensional displays},
month = {jun},
pages = {1--8},
title = {{Real-time 3D Reconstruction in Dynamic Scenes using Point-based Fusion}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6599048},
year = {2013}
}
@article{Khotanzad1990,
abstract = {The problem of rotation-, scale-, and translation-invariant recognition of images is discussed. A set of rotation-invariant features are introduced. They are the magnitudes of a set of orthogonal complex moments of the image known as Zernike moments. Scale and translation invariance are obtained by first normalizing the image with respect to these parameters using its regular geometrical moments. A systematic reconstruction-based method for deciding the highest-order Zernike moments required in a classification problem is developed. The quality of the reconstructed image is examined through its comparison to the original one. The orthogonality property of the Zernike moments, which simplifies the process of image reconstruction, make the suggest feature selection approach practical. Features of each order can also be weighted according to their contribution to the reconstruction process. The superiority of Zernike moment features over regular moments and moment invariants was experimentally verified.},
annote = {
Decompose shape of alphabet letters and reconstruct them using Zernike moments.
},
author = {Khotanzad, Alireza and Hong, Yaw Hua},
doi = {10.1109/34.55109},
isbn = {9781424464043},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)},
keywords = {Image analysis,Image processing,Instruments,Laboratories,Lakes,Testing,Zernike moments,feature selection,geometrical moments,image recognition,image reconstruction,image representation,invariant image recognition,orthogonality,pattern recognition,picture processing,rotation-invariant features,scale invariance,translation invariance},
mendeley-tags = {Image analysis,Image processing,Instruments,Laboratories,Lakes,Testing,Zernike moments,feature selection,geometrical moments,image recognition,image reconstruction,image representation,invariant image recognition,orthogonality,pattern recognition,picture processing,rotation-invariant features,scale invariance,translation invariance},
month = {may},
number = {5},
pages = {489--497},
title = {{Invariant Image Recognition by {\{}Zernike{\}} Moments}},
url = {http://ieeexplore.ieee.org/xpl/login.jsp?tp={\&}arnumber=55109{\&}url=http{\%}3A{\%}2F{\%}2Fieeexplore.ieee.org{\%}2Fxpls{\%}2Fabs{\_}all.jsp{\%}3Farnumber{\%}3D55109},
volume = {12},
year = {1990}
}
@inproceedings{Kim2004,
abstract = {Human being recognizes the physical world by integrating a great variety of sensory inputs, the information acquired by their own action, and their knowledge of the world using hierarchically parallel-distributed mechanism. In this paper, authors propose the sensor fusion system that can recognize multiple 3D objects from 2D projection images and tactile information. The proposed system focuses on improving object recognition rate. Unlike the conventional object recognition system that uses image sensor alone, the proposed method uses tactual sensors in addition to visual sensor. Tactual signals are obtained from the reaction force by the pressure sensors at the fingertips when unknown objects are grasped by four-fingered robot hand. The experiment evaluates the recognition rate and the number of learning iterations of various objects. The experimental results show that the proposed system can improve recognition rate and reduce learning time. These results verify the effectiveness of the proposed sensor fusion system as 3D object recognition scheme},
annote = {Uses images of objects and tactile pressure responses from rotation, feeding it together onto a neural network for training and demonstrating that the fusion of senses increases accuracy and decreases learning time,


Not interactive.},
author = {Kim, Ji Kyoung and Wee, Jae Woo and Lee, Chong Ho},
booktitle = {IEEE Conference on Cybernetics and Intelligent Systems, 2004.},
doi = {10.1109/ICCIS.2004.1460763},
isbn = {0-7803-8643-4},
keywords = {1-3 december,2004,3D object recognition,Folder - Haptic-Visual integration,Grippers,Humans,Image edge detection,Impedance matching,Information technology,Machine vision,Robot sensing systems,ceedings of the 2004,conference on cybernetics and,haptic interfaces,haptic recognition,ieee,image recognition,image sensor,image sensors,improving,intelligent system,intelligent systems,iterative methods,learning (artificial intelligence),learning iteration,neural nets,neural network,object recognition,parallel-distributed mechanism,pressure sensors,sensor fusion,sensor fusion system,sensor fusion system for,sensory motor,singapore,tactile sensors,tactual sensors,visual sensor},
mendeley-tags = {3D object recognition,Folder - Haptic-Visual integration,Grippers,Humans,Image edge detection,Impedance matching,Information technology,Machine vision,Robot sensing systems,haptic interfaces,haptic recognition,image recognition,image sensor,image sensors,intelligent system,iterative methods,learning (artificial intelligence),learning iteration,neural nets,neural network,object recognition,parallel-distributed mechanism,pressure sensors,sensor fusion,sensor fusion system,sensory motor,tactile sensors,tactual sensors,visual sensor},
pages = {1207--1212},
title = {{Sensor fusion system for improving the recognition of 3D object}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1460763},
volume = {2},
year = {2004}
}
@article{Kampmann2014,
abstract = {With the increasing complexity of robotic missions and the development towards long-term autonomous systems, the need for multi-modal sensing of the environment increases. Until now, the use of tactile sensor systems has been mostly based on sensing one modality of forces in the robotic end-effector. The use of a multi-modal tactile sensory system is motivated, which combines static and dynamic force sensor arrays together with an absolute force measurement system. This publication is focused on the development of a compact sensor interface for a fiber-optic sensor array, as optic measurement principles tend to have a bulky interface. Mechanical, electrical and software approaches are combined to realize an integrated structure that provides decentralized data pre-processing of the tactile measurements. Local behaviors are implemented using this setup to show the effectiveness of this approach.},
author = {Kirchner, Frank},
doi = {10.3390/s140406854},
issn = {14248220},
journal = {Sensors (Basel, Switzerland)},
keywords = {local pre-processing,multi-modal tactile sensing,tactile sensor processing},
language = {en},
mendeley-tags = {local pre-processing,multi-modal tactile sensing,tactile sensor processing},
month = {apr},
number = {4},
pages = {6854--6876},
pmid = {24743158},
title = {{Integration of fiber-optic sensor arrays into a multi-modal tactile sensor processing system for robotic end-effectors}},
url = {http://www.mdpi.com/1424-8220/14/4/6854 http://www.mdpi.com/1424-8220/14/4/6854/pdf},
volume = {14},
year = {2014}
}
@incollection{Klatzky1990,
abstract = {Humans are remarkably successful at identifying and learning about objects haptically. During the course of haptic object identification, purposive “exploratory procedures” are executed. A variety of factors control the course of haptic object exploration by constraining and influencing the selection of the next exploratory procedure to use. These factors include stored information about objects and their perceptual attributes, associations between exploratory procedures and attributes, and constraints arising from the nature of exploratory procedures themselves. A conceptual model describes such factors and their interactions in object processing.},
annote = {
Introduces the idea of exploratory procedures, to retrieve information (softness, weight, temperature, shape, function, contour, motion).
},
author = {Klatzky, Roberta L. and Lederman, Susan J.},
booktitle = {Dextrous hands for robots},
editor = {Venkataraman, Subramanian T. and Iberall, Thea},
isbn = {978-1-4613-8976-7},
keywords = {Artificial Intelligence (incl. Robotics),Control- Robotics- Mechatronics,Electrical Engineering,Engineering Economics- Organization- Logistics- Ma,Mechanics},
language = {en},
mendeley-tags = {Artificial Intelligence (incl. Robotics),Control- Robotics- Mechatronics,Electrical Engineering,Engineering Economics- Organization- Logistics- Ma,Mechanics},
month = {jan},
pages = {66-- 81},
publisher = {Springer New York},
title = {{Intelligent exploration by the human hand}},
url = {http://link.springer.com/chapter/10.1007/978-1-4613-8974-3{\_}4 http://link.springer.com/chapter/10.1007{\%}2F978-1-4613-8974-3{\_}4},
year = {1990}
}
@article{Kohonen1982,
abstract = {This work contains a theoretical study and computer simulations of a new self-organizing process. The principal discovery is that in a simple network of adaptive physical elements which receives signals from a primary event space, the signal representations are automatically mapped onto a set of output responses in such a way that the responses acquire the same topological order as that of the primary events. In other words, a principle has been discovered which facilitates the automatic formation of topologically correct maps of features of observable events. The basic self-organizing system is a one- or two-dimensional array of processing units resembling a network of threshold-logic units, and characterized by short-range lateral feedback between neighbouring units. Several types of computer simulations are used to demonstrate the ordering process as well as the conditions under which it fails.},
author = {Kohonen, Teuvo},
doi = {10.1007/BF00337288},
isbn = {0262010976},
issn = {0340-1200},
journal = {Biological Cybernetics},
keywords = {Neurosciences,Zoology},
language = {en},
mendeley-tags = {Neurosciences,Zoology},
month = {jan},
number = {1},
pages = {59--69},
title = {{Self-organized formation of topologically correct feature maps}},
url = {http://link.springer.com/article/10.1007/BF00337288 http://link.springer.com/article/10.1007{\%}2FBF00337288},
volume = {43},
year = {1982}
}
@inproceedings{Konolige2009,
abstract = {The typical SLAM mapping system assumes a static environment and constructs a map that is then used without regard for ongoing changes. Most SLAM systems, such as FastSLAM, also require a single connected run to create a map. In this paper we present a system of visual mapping, using only input from a stereo camera, that continually updates an optimized metric map in large indoor spaces with movable objects: people, furniture, partitions, etc. The system can be stopped and restarted at arbitrary disconnected points, is robust to occlusion and localization failures, and efficiently maintains alternative views of a dynamic environment. It operates completely online at a 30 Hz frame rate.},
author = {Konolige, Kurt and Bowman, James},
booktitle = {2009 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2009},
doi = {10.1109/IROS.2009.5354121},
isbn = {9781424438044},
keywords = {Cameras,Extraterrestrial measurements,FastSLAM,Filters,Intelligent robots,Layout,Robot sensing systems,Robustness,SLAM (robots),SLAM mapping system,USA Councils,image sensors,localization failures,mobile robots,occlusion,robot vision,simultaneous localization and mapping,visual maps},
mendeley-tags = {Cameras,Extraterrestrial measurements,FastSLAM,Filters,Intelligent robots,Layout,Robot sensing systems,Robustness,SLAM (robots),SLAM mapping system,USA Councils,image sensors,localization failures,mobile robots,occlusion,robot vision,simultaneous localization and mapping,visual maps},
month = {oct},
pages = {1156--1163},
pmid = {5354121},
title = {{Towards lifelong visual maps}},
url = {http://ieeexplore.ieee.org/ielx5/5342790/5353884/05354121.pdf?tp={\&}arnumber=5354121{\&}isnumber=5353884 http://ieeexplore.ieee.org/xpl/abstractCitations.jsp?tp={\&}arnumber=5354121{\&}contentType=Conference+Publications{\#}abstractCitations},
year = {2009}
}
@inproceedings{Krainin2012,
abstract = {While Iterative Closest Point (ICP) algorithms have been successful at aligning 3D point clouds, they do not take into account constraints arising from sensor viewpoints. More recent beam-based models take into account sensor noise and viewpoint, but problems still remain. In particular, good optimization strategies are still lacking for the beam-based model. In situations of occlusion and clutter, both beam-based and ICP approaches can fail to find good solutions. In this paper, we present both an optimization method for beambased models and a novel framework for modeling observation dependencies in beam-based models using over-segmentations. This technique enables reasoning about object extents and works well in heavy clutter. We also make available a ground-truth 3D dataset for testing algorithms in this area.},
author = {Krainin, Michael and Konolige, Kurt and Fox, Dieter},
booktitle = {2012 IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2012.6224714},
isbn = {978-1-4673-1405-3},
issn = {1050-4729},
keywords = {3D point clouds,Clutter,Computational modeling,Data models,Estimation,ICP approach,Image segmentation,Optimization,Robot sensing systems,beam-based models,ground-truth 3D dataset,image matching,inference mechanisms,iterative closest point algorithm,iterative closest point algorithms,iterative methods,object extent reasoning,optimisation,optimization method,over-segmentations,robust 3D object matching,segmentation exploitation,sensor noise,sensor viewpoints,solid modelling},
mendeley-tags = {3D point clouds,Clutter,Computational modeling,Data models,Estimation,ICP approach,Image segmentation,Optimization,Robot sensing systems,beam-based models,ground-truth 3D dataset,image matching,inference mechanisms,iterative closest point algorithm,iterative closest point algorithms,iterative methods,object extent reasoning,optimisation,optimization method,over-segmentations,robust 3D object matching,segmentation exploitation,sensor noise,sensor viewpoints,solid modelling},
month = {may},
pages = {4399--4405},
title = {{Exploiting segmentation for robust 3D object matching}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6224714},
year = {2012}
}
@incollection{Krizhevsky2012,
author = {Krizhevsky, Alex and Sulskever, IIya and Hinton, Geoffret E},
booktitle = {Advances in Neural Information and Processing Systems (NIPS)},
editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
keywords = {Folder - convolutional},
mendeley-tags = {Folder - convolutional},
pages = {1--9},
publisher = {Curran Associates, Inc.},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks},
year = {2012}
}
@article{Kroemer2011,
abstract = {Dynamic tactile sensing is a fundamental ability to recognize materials and objects. However, while humans are born with partially developed dynamic tactile sensing and quickly master this skill, today{\&}{\#}x2019;s robots remain in their infancy. The development of such a sense requires not only better sensors but the right algorithms to deal with these sensors{\&}{\#}x2019; data as well. For example, when classifying a material based on touch, the data are noisy, high-dimensional, and contain irrelevant signals as well as essential ones. Few classification methods from machine learning can deal with such problems. In this paper, we propose an efficient approach to infer suitable lower dimensional representations of the tactile data. In order to classify materials based on only the sense of touch, these representations are autonomously discovered using visual information of the surfaces during training. However, accurately pairing vision and tactile samples in real-robot applications is a difficult problem. The proposed approach, therefore, works with weak pairings between the modalities. Experiments show that the resulting approach is very robust and yields significantly higher classification performance based on only dynamic tactile sensing.},
author = {Kroemer, Oliver and Lampert, Christoph H. and Peters, Jan},
doi = {10.1109/TRO.2011.2121130},
file = {:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kroemer, Lampert, Peters - 2011 - Learning dynamic tactile sensing with robust vision-based training.pdf:pdf},
isbn = {1552-3098},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Intelligent robots,robot sensing systems,tactile sensing},
month = {jun},
number = {3},
pages = {545--557},
pmid = {5752870},
title = {{Learning dynamic tactile sensing with robust vision-based training}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5752870},
volume = {27},
year = {2011}
}
@article{Kullback1951,
abstract = {Volume 22, Number 1 (1951), 1-164},
author = {Kullback, S. and Leibler, R. A.},
doi = {10.1214/aoms/1177729694},
isbn = {00034851},
issn = {0003-4851},
journal = {The Annals of Mathematical Statistics},
language = {EN},
month = {mar},
number = {1},
pages = {79--86},
pmid = {10896709},
title = {{On Information and Sufficiency}},
url = {https://projecteuclid.org/euclid.aoms/1177729694},
volume = {22},
year = {1951}
}
@article{Lacey2007,
abstract = {The relationship between visually and haptically derived representations of objects is an important question in multisensory processing and, increasingly, in mental representation. We review evidence for the format and properties of these representations, and address possible theoretical models. We explore the relevance of visual imagery processes and highlight areas for further research, including the neglected question of asymmetric performance in the visuo-haptic cross-modal memory paradigm. We conclude that the weight of evidence suggests the existence of a multisensory representation, spatial in format, and flexibly accessible by both bottom-up and top-down inputs, although efficient comparison between modality-specific representations cannot entirely be ruled out.},
annote = {suggests humans have object models which integrate multiple sensory information

----------

unread},
author = {Lacey, Simon and Campbell, Christine and Sathian, K.},
doi = {10.1068/p5850},
isbn = {0301-0066},
issn = {03010066},
journal = {Perception},
keywords = {Folder - Haptic-Visual integration,Folder - Haptic-Visual integration - bio motivatio,Form Perception,Humans,Recognition (Psychology),Touch,Visual Perception},
language = {eng},
mendeley-tags = {Folder - Haptic-Visual integration,Folder - Haptic-Visual integration - bio motivatio,Form Perception,Humans,Recognition (Psychology),Touch,Visual Perception},
number = {10},
pages = {1513--1521},
pmid = {18265834},
shorttitle = {Vision and touch},
title = {{Vision and touch: Multiple or multisensory representations of objects?}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18265834},
volume = {36},
year = {2007}
}
@article{Lacey2014,
abstract = {Visual and haptic unisensory object processing show many similarities in terms of categorization, recognition, and representation. In this review, we discuss how these similarities contribute to multisensory object processing. In particular, we show that similar unisensory visual and haptic representations lead to a shared multisensory representation underlying both cross-modal object recognition and view-independence. This shared representation suggests a common neural substrate and we review several candidate brain regions, previously thought to be specialized for aspects of visual processing, that are now known also to be involved in analogous haptic tasks. Finally, we lay out the evidence for a model of multisensory object recognition in which top-down and bottom-up pathways to the object-selective lateral occipital complex are modulated by object familiarity and individual differences in object and spatial imagery.},
annote = {Visual and haptic object representations are intrinsically linked. Cross modal object recognition achieves view independance easier than either modality alone.},
author = {Lacey, Simon and Sathian, K.},
doi = {10.3389/fpsyg.2014.00730},
issn = {16641078},
journal = {Frontiers in Psychology},
keywords = {Cross-modal,Effective connectivity,Face processing,Folder - Haptic-Visual integration - bio motivatio,Viewpoint dependence,Visual imagery,cross-modal,effective connectivity,fMRI,face processing,viewpoint dependence,visual imagery},
mendeley-tags = {Folder - Haptic-Visual integration - bio motivatio,cross-modal,effective connectivity,fMRI,face processing,viewpoint dependence,visual imagery},
number = {JUL},
pages = {730},
pmid = {25101014},
title = {{Visuo-haptic multisensory object recognition, categorization, and representation}},
url = {http://journal.frontiersin.org/Journal/10.3389/fpsyg.2014.00730/abstract http://journal.frontiersin.org/Journal/10.3389/fpsyg.2014.00730/pdf},
volume = {5},
year = {2014}
}
@inproceedings{Lai2012,
abstract = {We propose a view-based approach for labeling objects in 3D scenes reconstructed from RGB-D (color+depth) videos. We utilize sliding window detectors trained from object views to assign class probabilities to pixels in every RGB-D frame. These probabilities are projected into the reconstructed 3D scene and integrated using a voxel representation. We perform efficient inference on a Markov Random Field over the voxels, combining cues from view-based detection and 3D shape, to label the scene. Our detection-based approach produces accurate scene labeling on the RGB-D Scenes Dataset and improves the robustness of object detection.},
annote = {
Detect objects using a volumetric representation (voxels) and performing MRF inference. The result is 88{\%} recall accuracy and 91{\%} precision.
},
author = {Lai, Kevin and Bo, Liefeng and Ren, Xiaofeng and Fox, Dieter},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2012.6225316},
file = {:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lai et al. - 2012 - Detection-based object labeling in 3D scenes.pdf:pdf;:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lai et al. - 2012 - Detection-based object labeling in 3D scenes.html:html},
isbn = {9781467314039},
issn = {10504729},
keywords = {3D scene reconstruction,3D shape,Detectors,Feature extraction,Labeling,Markov processes,Markov random field,RGB-D frame,RGB-D scenes dataset,RGB-D videos,Shape,Training,Videos,class probabilities,color+depth videos,detection-based object labeling,image colour analysis,image reconstruction,object detection,probability,random processes,sliding window detectors,view-based detection,voxel representation},
mendeley-tags = {3D scene reconstruction,3D shape,Detectors,Feature extraction,Labeling,Markov processes,Markov random field,RGB-D frame,RGB-D scenes dataset,RGB-D videos,Shape,Training,Videos,class probabilities,color+depth videos,detection-based object labeling,image colour analysis,image reconstruction,object detection,probability,random processes,sliding window detectors,view-based detection,voxel representation},
pages = {1330--1337},
title = {{Detection-based object labeling in 3D scenes}},
url = {http://ieeexplore.ieee.org/ielx5/6215071/6224548/06225316.pdf?tp={\&}arnumber=6225316{\&}isnumber=6224548 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6225316},
year = {2012}
}
@inproceedings{Lee2009,
abstract = {There has been much interest in unsupervised learning of hierarchical generative models such as deep belief networks. Scaling such models to full-sized, high-dimensional images remains a difficult problem. To address this problem, we present the convolutional deep belief network, a hierarchical generative model which scales to realistic image sizes. This model is translation-invariant and supports efficient bottom-up and top-down probabilistic inference. Key to our approach is probabilistic max-pooling, a novel technique which shrinks the representations of higher layers in a probabilistically sound way. Our experiments show that the algorithm learns useful high-level visual features, such as object parts, from unlabeled images of objects and natural scenes. We demonstrate excellent performance on several visual recognition tasks and show that our model can perform hierarchical (bottom-up and top-down) inference over full-sized images.},
address = {New York, NY, USA},
annote = {
Using the principle of deep belief networks (DBNs), i.e. layer-by-layer pretraining using a generative model such as stacked RBMs, it has been shown that a convolutional DBN using novel probabilistic pooling can learn hierarchical part-based representations of objects
},
author = {Lee, Honglak and Grosse, Roger and Ranganath, Rajesh and Ng, Andrew Y},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning ICML 09},
doi = {10.1145/1553374.1553453},
isbn = {9781605585161},
issn = {02643294},
keywords = {Folder - convolutional},
mendeley-tags = {Folder - convolutional},
pages = {1--8},
pmid = {20957573},
publisher = {ACM},
series = {ICML '09},
title = {{Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations}},
url = {http://portal.acm.org/citation.cfm?doid=1553374.1553453},
volume = {2008},
year = {2009}
}
@article{Lenz2015a,
abstract = {We consider the problem of detecting robotic grasps in an RGB-D view of a scene containing objects. In this work, we apply a deep learning approach to solve this problem, which avoids time-consuming hand-design of features. This presents two main challenges. First, we need to evaluate a huge number of candidate grasps. In order to make detection fast and robust, we present a two-step cascaded system with two deep networks, where the top detections from the first are re-evaluated by the second. The first network has fewer features, is faster to run, and can effectively prune out unlikely candidate grasps. The second, with more features, is slower but has to run only on the top few detections. Second, we need to handle multimodal inputs effectively, for which we present a method that applies structured regularization on the weights based on multimodal group regularization. We show that our method improves performance on an RGBD robotic grasping dataset, and can be used to successfully execute grasps on two different robotic platforms.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3592v5},
author = {Lenz, I. and Lee, H. and Saxena, A.},
doi = {10.1177/0278364914549607},
eprint = {arXiv:1301.3592v5},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
keywords = {3d feature learning,baxter,deep learning,pr2,rgb-d multi-modal data,robotic grasping},
number = {4-5},
pages = {705--724},
title = {{Deep learning for detecting robotic grasps}},
url = {http://ijr.sagepub.com/content/34/4-5/705.short},
volume = {34},
year = {2015}
}
@article{Lenz2015,
abstract = {We consider the problem of detecting robotic grasps in an RGB-D view of a scene containing objects. In this work, we apply a deep learning approach to solve this problem, which avoids time-consuming hand-design of features. This presents two main challenges. First, we need to evaluate a huge number of candidate grasps. In order to make detection fast and robust, we present a two-step cascaded system with two deep networks, where the top detections from the first are re-evaluated by the second. The first network has fewer features, is faster to run, and can effectively prune out unlikely candidate grasps. The second, with more features, is slower but has to run only on the top few detections. Second, we need to handle multimodal inputs effectively, for which we present a method that applies structured regularization on the weights based on multimodal group regularization. We show that our method improves performance on an RGBD robotic grasping dataset, and can be used to successfully execute grasps on two different robotic platforms.},
annote = {Create a multimodal deep net to detect valid grasps, using RGB as one modality and Depth as the other. They argue that for multimodal learning where the modalities are expected to share feature statistics (such as their case) it is benefitial to fuse the modalities early (feature concatenation or regularised feature concatenation). In other problems, such as video and audio [Multimodal deep learning, Ngiam], they recognise that late model fusion (joining of the networks at a higher layer) is to be preferred, to avoid learning weak correlations and thus overfitting and to allow for learning of associations that may exists between modalities at a more abstract level.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3592v5},
author = {Lenz, I. and Lee, H. and Saxena, A.},
doi = {10.1177/0278364914549607},
eprint = {arXiv:1301.3592v5},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
keywords = {3d feature learning,baxter,deep learning,pr2,rgb-d multi-modal data,robotic grasping},
number = {4-5},
pages = {705--724},
title = {{Deep learning for detecting robotic grasps}},
url = {http://ijr.sagepub.com/content/34/4-5/705.short},
volume = {34},
year = {2015}
}
@inproceedings{Liu2012,
abstract = {Object surface properties are among the most important information which a robot requires in order to effectively interact with an unknown environment. This paper presents a novel haptic exploration strategy for recognizing the physical properties of unknown object surfaces using an intelligent finger. This developed intelligent finger is capable of identifying the contact location, normal and tangential force, and the vibrations generated from the contact in real time. In the proposed strategy, this finger gently slides along the surface with a short stroke while increasing and decreasing the sliding velocity. By applying a dynamic friction model to describe this contact, rich and accurate surface physical properties can be identified within this stroke. This allows different surface materials to be easily distinguished even if when they have very similar texture. Several supervised learning algorithms have been applied and compared for surface recognition based on the obtained surface properties. It has been found that the naïve Bayes classifier is superior to radial basis function network and k-NN method, achieving an overall classification accuracy of 88.5{\%} for distinguishing twelve different surface materials.},
annote = {
Slide a specifically designed fingertip along a number of surface and manage to classify them using Naive Bayes Classifier, according to texture with an accuracy of 88.5{\%}, even considering the textures are relatively similar.
},
author = {Liu, Hongbin and Song, Xiaojing and Bimbo, Joao and Seneviratne, Lakmal and Althoefer, Kaspar},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2012.6385815},
isbn = {9781467317375},
issn = {21530858},
keywords = {Bayes methods,Fingers,Force,Friction,Rough surfaces,Sensors,Surface roughness,Surface texture,contact location,dynamic friction model,haptic exploration,haptic interfaces,intelligent contact sensing finger,intelligent sensors,k-NN method,learning (artificial intelligence),na{\"{i}}ve Bayes classifier,normal force,object surface property,radial basis function network,robots,sliding velocity,supervised learning,surface material recognition,surface recognition,tangential force,vibration},
mendeley-tags = {Bayes methods,Fingers,Force,Friction,Rough surfaces,Sensors,Surface roughness,Surface texture,contact location,dynamic friction model,haptic exploration,haptic interfaces,intelligent contact sensing finger,intelligent sensors,k-NN method,learning (artificial intelligence),na{\"{i}}ve Bayes classifier,normal force,object surface property,radial basis function network,robots,sliding velocity,supervised learning,surface material recognition,surface recognition,tangential force,vibration},
pages = {52--57},
title = {{Surface material recognition through haptic exploration using an intelligent contact sensing finger}},
url = {http://ieeexplore.ieee.org/ielx5/6363628/6385431/06385815.pdf?tp={\&}arnumber=6385815{\&}isnumber=6385431 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6385815},
year = {2012}
}
@inproceedings{Liu2012a,
abstract = {Multi sensor fusion has been widely used in recognition problems. Most existing works highly depend on the calibration between different sensors, but less on modeling and reasoning of the co-incidence of multiple hints. In this paper, we propose a generic framework for recognition and clustering problem using a non-parametric Dirichlet hierarchical model, named DP-Fusion. It enables online labeling, clustering and recognition of sequential data simultaneously, while considering multiple types of sensor readings. The algorithm is data-driven, which does not depend on priorknowledge of the data structure. The results show the feasibility and reliability against noise data.},
annote = {simulated testing

----------

propose a Dirichlet Process Mixture Model for the fusion of multisensory data},
author = {Liu, Ming and Wang, Lujia and Siegwart, Roland},
booktitle = {IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems},
doi = {10.1109/MFI.2012.6343031},
isbn = {9781467325110},
keywords = {Approximation algorithms,Approximation methods,Clustering algorithms,DP-fusion,Data models,Folder - Haptic-Visual integration,Inference algorithms,Robot sensing systems,clustering problem,data clustering,data structure,multisensor fusion,noise data,nonparametric Dirichlet hierarchical model,nonparametric statistics,online labeling,online multisensor recognition,pattern clustering,recognition problem,sensor calibration,sensor fusion,sensor reading,sequential data recognition},
mendeley-tags = {Approximation algorithms,Approximation methods,Clustering algorithms,DP-fusion,Data models,Folder - Haptic-Visual integration,Inference algorithms,Robot sensing systems,clustering problem,data clustering,data structure,multisensor fusion,noise data,nonparametric Dirichlet hierarchical model,nonparametric statistics,online labeling,online multisensor recognition,pattern clustering,recognition problem,sensor calibration,sensor fusion,sensor reading,sequential data recognition},
pages = {7--12},
shorttitle = {DP-Fusion},
title = {{DP-Fusion: A generic framework for online multi sensor recognition}},
url = {http://ieeexplore.ieee.org/ielx5/6335102/6342994/06343031.pdf?tp={\&}arnumber=6343031{\&}isnumber=6342994 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6343031},
year = {2012}
}
@inproceedings{Lowe1999,
abstract = {An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low residual least squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially occluded images with a computation time of under 2 seconds},
annote = {
Introduces SIFT
},
archivePrefix = {arXiv},
arxivId = {cs/0112017},
author = {Lowe, David G.},
booktitle = {Proceedings of the Seventh IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.1999.790410},
eprint = {0112017},
isbn = {0-7695-0164-8},
issn = {0-7695-0164-8},
keywords = {3D projection,Computer science,Electrical capacitance tomography,Feature extraction,Filters,Layout,Lighting,Neurons,Programmable logic arrays,Reactive power,blurred image gradients,candidate object matches,cluttered partially occluded images,computation time,computational geometry,image matching,image recognition,inferior temporal cortex,least squares approximations,local geometric deformations,local image features,local scale-invariant features,low residual least squares solution,multiple orientation planes,nearest neighbor indexing method,object recognition,primate vision,robust object recognition,staged filtering approach,unknown model parameters},
mendeley-tags = {3D projection,Computer science,Electrical capacitance tomography,Feature extraction,Filters,Layout,Lighting,Neurons,Programmable logic arrays,Reactive power,blurred image gradients,candidate object matches,cluttered partially occluded images,computation time,computational geometry,image matching,image recognition,inferior temporal cortex,least squares approximations,local geometric deformations,local image features,local scale-invariant features,low residual least squares solution,multiple orientation planes,nearest neighbor indexing method,object recognition,primate vision,robust object recognition,staged filtering approach,unknown model parameters},
number = {[8},
pages = {1150--1157},
pmid = {15806121},
primaryClass = {cs},
title = {{Object recognition from local scale-invariant features}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=790410},
volume = {2},
year = {1999}
}
@article{Lunghi2013,
abstract = {Multisensory integration is a common feature of the mammalian brain that allows it to deal more efficiently with the ambiguity of sensory input by combining complementary signals from several sensory sources. Growing evidence suggests that multisensory interactions can occur as early as primary sensory cortices. Here we present incompatible visual signals (orthogonal gratings) to each eye to create visual competition between monocular inputs in primary visual cortex where binocular combination would normally take place. The incompatibility prevents binocular fusion and triggers an ambiguous perceptual response in which the two images are perceived one at a time in an irregular alternation. One key function of multisensory integration is to minimize perceptual ambiguity by exploiting cross-sensory congruence. We show that a haptic signal matching one of the visual alternatives helps disambiguate visual perception during binocular rivalry by both prolonging the dominance period of the congruent visual stimulus and by shortening its suppression period. Importantly, this interaction is strictly tuned for orientation, with a mismatch as small as 7.5° between visual and haptic orientations sufficient to annul the interaction. These results indicate important conclusions: first, that vision and touch interact at early levels of visual processing where interocular conflicts are first detected and orientation tunings are narrow, and second, that haptic input can influence visual signals outside of visual awareness, bringing a stimulus made invisible by binocular rivalry suppression back to awareness sooner than would occur without congruent haptic input.},
annote = {suggest that integration of visual and tactile sensation in humans occur at early stages of perception.},
author = {Lunghi, Claudia and Alais, David},
doi = {10.1371/journal.pone.0058754},
file = {:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lunghi, Alais - 2013 - Touch Interacts with Vision during Binocular Rivalry with a Tight Orientation Tuning.pdf:pdf},
isbn = {1932-6203 (Electronic) 1932-6203 (Linking)},
issn = {19326203},
journal = {PLoS ONE},
keywords = {Folder - Haptic-Visual integration - bio motivatio},
mendeley-tags = {Folder - Haptic-Visual integration - bio motivatio},
month = {mar},
number = {3},
pages = {e58754},
pmid = {23472219},
title = {{Touch Interacts with Vision during Binocular Rivalry with a Tight Orientation Tuning}},
url = {http://dx.doi.org/10.1371/journal.pone.0058754 http://www.plosone.org/article/fetchObject.action?uri=info{\%}3Adoi{\%}2F10.1371{\%}2Fjournal.pone.0058754{\&}representation=PDF},
volume = {8},
year = {2013}
}
@inproceedings{Luo2014,
abstract = {In this paper a novel approach is proposed to recognise different objects invariant to their translation and rotation by utilising a tactile sensor attached to a robotic arm. As the sensor is small compared to the tested objects, the robot needs to access those objects multiple times at different positions and is prone to move or rotate them. This inevitably increases difficulty in object recognition during manipulations. To solve this problem, it is proposed to extract tactile translation and rotation invariant local features to represent objects; a dictionary of k words is therefore learned by $\kappa$-means unsupervised learning and a histogram codebook is then used to identify objects. The proposed system has been validated by classifying real objects with data from an off-the-shelf tactile sensor. The average overall accuracy of 91.2{\%} has been achieved with only 10 touches and a dictionary size of 50 clusters. {\textcopyright} 2014 IEEE.},
annote = {
{\textless}span style="font-size: 13px; font-family: arial,sans,sans-serif;"{\textgreater}{\textless}span style="font-size: 13px; font-family: arial,sans,sans-serif;"{\textgreater}Weiss sensor, SIFT and k-means, BoW to achieve 91.2{\%} accuracy over 10 objects after 10 touches.
},
author = {Luo, Shan and Mou, Wenxuan and Li, Min and Althoefer, Kaspar and Liu, Hongbin},
booktitle = {IEEE Sensors Conference},
doi = {10.1109/ICSENS.2014.6985179},
file = {:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Luo et al. - 2014 - Rotation and Translation Invariant Object Recognition with a Tactile Sensor.pdf:pdf;:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Luo et al. - 2014 - Rotation and Translation Invariant Object Recognition with a Tactile Sensor.html:html},
isbn = {9781479901623},
keywords = {Accuracy,Codebooks,Dictionaries,Feature extraction,Folder - TOR by BOF,Histograms,Overall accuracies,Real objects,Robots,Rotation invariant,Tactile sensors,Tested objects,Translation invariants,accuracy,histogram codebook,k-means unsupervised learning,learning (artificial intelligence),object recognition,robotic arm,robots,rotation invariant object recognition,tactile sensor,tactile sensors,translation invariant object recognition},
mendeley-tags = {Accuracy,Dictionaries,Feature extraction,Folder - TOR by BOF,Histograms,histogram codebook,k-means unsupervised learning,learning (artificial intelligence),object recognition,robotic arm,robots,rotation invariant object recognition,tactile sensor,tactile sensors,translation invariant object recognition},
month = {nov},
pages = {1030--1033},
title = {{Rotation and Translation Invariant Object Recognition with a Tactile Sensor}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84931028408{\&}partnerID=40{\&}md5=152305338ae585be1b715d0d3a5c3c07},
volume = {2014-Decem},
year = {2014}
}
@inproceedings{Madry2014,
abstract = {Tactile sensing plays an important role in robot grasping and object recognition. In this work, we propose a new descriptor named Spatio-Temporal Hierarchical Matching Pursuit (ST-HMP) that captures properties of a time series of tactile sensor measurements. It is based on the concept of unsupervised hierarchical feature learning realized using sparse coding. The ST-HMP extracts rich spatio-temporal structures from raw tactile data without the need to predefine discriminative data characteristics. We apply it to two different applications: (1) grasp stability assessment and (2) object instance recognition, presenting its universal properties. An extensive evaluation on several synthetic and real datasets collected using the Schunk Dexterous, Schunk Parallel and iCub hands shows that our approach outperforms previously published results by a large margin.},
annote = {Extends HMP to include temporal information, and uses it to learn tactile features in an unsupervised manner.},
author = {Madry, Marianna and Bo, Liefeng and Kragic, Danica and Fox, Dieter},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2014.6907172},
isbn = {9781479936847},
issn = {10504729},
pages = {2262--2269},
shorttitle = {ST-HMP},
title = {{ST-HMP: Unsupervised Spatio-Temporal feature learning for tactile data}},
year = {2014}
}
@article{Martin2002,
annote = {
Extends [Cauwenbergh 00] to use in SVM regression, i.e. function approximation.
},
author = {Martin, Mario},
file = {:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Martin - 2002 - On-line support vector machines for function approximation.pdf:pdf},
journal = {Techn. report, Universitat Polit{\`{e}}cnica de Catalunya, Departament de Llengatges i Sistemes Inform{\`{a}}tics},
keywords = {Folder - Incremental},
mendeley-tags = {Folder - Incremental},
pages = {1--11},
title = {{On-line support vector machines for function approximation}},
url = {http://www.lsi.upc.edu/{~}mmartin/repsvmr.pdf},
year = {2002}
}
@article{Martinovic2012,
abstract = {Vision identifies objects rapidly and efficiently. In contrast, object recognition by touch is much slower. Furthermore, haptics usually serially accumulates information from different parts of objects, whereas vision typically processes object information in parallel. Is haptic object identification slower simply due to sequential information acquisition and the resulting memory load or due to more fundamental processing differences between the senses? To compare the time course of visual and haptic object recognition, we slowed visual processing using a novel, restricted viewing technique. In an electroencephalographic (EEG) experiment, participants discriminated familiar, nameable from unfamiliar, unnamable objects both visually and haptically. Analyses focused on the evoked and total fronto-central theta-band (5-7 Hz; a marker of working memory) and the occipital upper alpha-band (10-12 Hz; a marker of perceptual processing) locked to the onset of classification. Decreases in total upper alpha-band activity for haptic identification of objects indicate a likely processing role of multisensory extrastriate areas. Long-latency modulations of alpha-band activity differentiated between familiar and unfamiliar objects in haptics but not in vision. In contrast, theta-band activity showed a general increase over time for the slowed-down visual recognition task only. We conclude that haptic object recognition relies on common representations with vision but also that there are fundamental differences between the senses that do not merely arise from differences in their speed of processing.},
annote = {In humans, haptic object recognition depends on joint visual-tactile representations.},
author = {Martinovic, Jasna and Lawson, Rebecca and Craddock, Matt},
doi = {10.3389/fnhum.2012.00049},
file = {:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Martinovic, Lawson, Craddock - 2012 - Time Course of Information ProcessingMartinovic, J., Lawson, R., {\&} Craddock, M. (2012). Time Cours.pdf:pdf},
issn = {1662-5161},
journal = {Frontiers in Human Neuroscience},
keywords = {EEG,Folder - Haptic-Visual integration - bio motivatio,Vision,alpha,alpha-band activity,eeg,haptics,object classification,theta-band activity,vision,vision, haptics, object classification, EEG, alpha},
mendeley-tags = {EEG,Folder - Haptic-Visual integration - bio motivatio,Vision,alpha-band activity,haptics,object classification,theta-band activity},
number = {March},
pages = {1--11},
pmid = {22470327},
title = {{Time Course of Information ProcessingMartinovic, J., Lawson, R., {\&} Craddock, M. (2012). Time Course of Information Processing in Visual and Haptic Object Classification. Frontiers in Human Neuroscience, 6(March), 1–11. http://doi.org/10.3389/fnhum.2012.00}},
url = {http://journal.frontiersin.org/article/10.3389/fnhum.2012.00049/abstract http://journal.frontiersin.org/article/10.3389/fnhum.2012.00049/pdf},
volume = {6},
year = {2012}
}
@incollection{Masci2011,
abstract = {We present a novel convolutional auto-encoder (CAE) for unsupervised feature learning. A stack of CAEs forms a convolutional neural network (CNN). Each CAE is trained using conventional on-line gradient descent without additional regularization terms. A max-pooling layer is essential to learn biologically plausible features consistent with those found by previous approaches. Initializing a CNN with filters of a trained CAE stack yields superior performance on a digit (MNIST) and an object recognition (CIFAR10) benchmark.},
annote = {
Convolutional versions of these (named convolutional auto-encoder stacks, or CAESs) have been developed and used for pre-training convolutional networks in order to extract a hierarchy of features, obtaining excellent results in CIFAR10 object recognition benchmarks
},
author = {Masci, Jonathan and Meier, Ueli and Ciresan, Dan and Schmidhuber, Juergen},
booktitle = {Artificial Neural Networks and Machine Learning - Icann 2011, Pt I},
editor = {Honkela, Timo and Duch, W{\l}odzis{\l}aw and Girolami, Mark and Kaski, Samuel},
isbn = {978-3-642-21734-0},
keywords = {Algorithm Analysis and Problem Complexity,Artificial Intelligence (incl. Robotics),Computation by Abstract Devices,Folder - convolutional,Image Processing and Computer Vision,Information Systems Applications (incl.Internet),auto-encoder,classification,convolutional neural network,pattern recognition,unsupervised learning},
language = {en},
mendeley-tags = {Algorithm Analysis and Problem Complexity,Artificial Intelligence (incl. Robotics),Computation by Abstract Devices,Folder - convolutional,Image Processing and Computer Vision,Information Systems Applications (incl.Internet),auto-encoder,classification,convolutional neural network,pattern recognition,unsupervised learning},
month = {jan},
pages = {52--59},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Stacked Convolutional Auto-Encoders for Hierarchical Feature Extraction}},
url = {{\textless}Go to ISI{\textgreater}://WOS:000296364500007},
volume = {6791},
year = {2011}
}
@article{McGregor2010,
abstract = {Participants learned through feedback to haptically classify the identity$\backslash$nof upright versus inverted versus scrambled faces depicted in simple$\backslash$n2D raised-line displays. We investigated whether identity classification$\backslash$nwould make use of a configural face representation, as is evidenced$\backslash$nfor vision and 3D haptic facial displays. Upright and scrambled faces$\backslash$nproduced equivalent accuracy, and both were identified more accurately$\backslash$nthan inverted faces. The mean magnitude of the haptic inversion effect$\backslash$nfor 2D facial identity was a sizable 26 percent, indicating that$\backslash$nthe upright orientation was {\#}x0201C;privileged {\#}x0201D; in the haptic$\backslash$nrepresentations of facial identity in these 2D displays, as with$\backslash$nother facial modalities. However, given the effect of scrambling,$\backslash$nwe conclude that configural processing was not employed; rather,$\backslash$nonly local information about the features was used, the features$\backslash$nbeing treated as oriented objects within a body-centered frame of$\backslash$nreference. The results indicate a fundamental difference between$\backslash$nhaptic identification of 2D facial depictions and 3D faces, paralleling$\backslash$na corresponding difference in recognition of nonface objects.},
annote = {
suggests human haptic recognition is strongly based on local features, and largely ignores their geometric configuration, supporting a 'bag of features approach'
},
author = {McGregor, Tara A. and Klatzky, Roberta L. and Hamilton, Cheryl and Lederman, Susan J.},
doi = {10.1109/TOH.2009.49},
issn = {19391412},
journal = {IEEE Transactions on Haptics},
keywords = {2D displays,3D haptic facial displays,Biology computing,Cognition,Education.,Face detection,Feature extraction,Feedback,Folder - TOR by BOF,Humans,Perception and psychophysics,Psychology,Social communication,Telecommunication computing,Three dimensional displays,Two dimensional displays,Visual Perception,configural processing,education.,face recognition,facial identity,feature-based processing,haptic classification,haptic interfaces,identity classification,perception and psychophysics,social communication,three-dimensional displays},
mendeley-tags = {2D displays,3D haptic facial displays,Biology computing,Cognition,Face detection,Feature extraction,Feedback,Folder - TOR by BOF,Humans,Psychology,Telecommunication computing,Three dimensional displays,Two dimensional displays,Visual Perception,configural processing,education.,face recognition,facial identity,feature-based processing,haptic classification,haptic interfaces,identity classification,perception and psychophysics,social communication,three-dimensional displays},
number = {1},
pages = {48--55},
shorttitle = {Haptic Classification of Facial Identity in 2D Dis},
title = {{Haptic classification of facial identity in 2D displays: Configural versus feature-based processing}},
url = {http://ieeexplore.ieee.org/ielx5/4543165/5438559/05339123.pdf?tp={\&}arnumber=5339123{\&}isnumber=5438559 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5339123},
volume = {3},
year = {2010}
}
@article{Meier2011,
abstract = {In this paper, we present a probabilistic spatial approach to build compact 3-D representations of unknown objects probed by tactile sensors. Our approach exploits the high frame rates provided by modern tactile sensors and utilizes Kalman filters to build a probabilistic model of the contact point cloud that is efficiently stored in a kd-tree. The quality of generated shape representations is compared with a naive averaging approach, and we show that our method provides superior accuracy. We also evaluate the feasibility of object classification combining the generated object representations, together with the iterative closest point algorithm.},
annote = {
One way to solve the problem of having too much data (especially potentially redundant data), is to merge points that are closeby into a probability point modelled by a kalman filter, which was proven to be achievable in real time and with no significant error with respect to a direct ICP
},
author = {Meier, Martin and Sch{\"{o}}pfer, Matthias and Haschke, Robert and Ritter, Helge},
doi = {10.1109/TRO.2011.2120830},
isbn = {1552-3098},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {3-D reconstruction,Grasping,Kalman filters,Manganese,Shape,compact 3D object representation,contact point cloud,iterative closest point algorithm,iterative methods,kd-tree,naive averaging approach,object classification,pattern classification,probabilistic spatial approach,probability,recognition,solid modelling,tactile sensing,tactile sensors,tactile shape reconstruction,tree data structures},
mendeley-tags = {3-D reconstruction,Grasping,Kalman filters,Manganese,Shape,compact 3D object representation,contact point cloud,iterative closest point algorithm,iterative methods,kd-tree,naive averaging approach,object classification,pattern classification,probabilistic spatial approach,probability,recognition,solid modelling,tactile sensing,tactile sensors,tactile shape reconstruction,tree data structures},
month = {jun},
number = {3},
pages = {630--635},
title = {{A probabilistic approach to tactile shape reconstruction}},
url = {http://ieeexplore.ieee.org/ielx5/8860/5784192/05740987.pdf?tp={\&}arnumber=5740987{\&}isnumber=5784192 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5740987},
volume = {27},
year = {2011}
}
@article{Mercimek2005,
abstract = {Moments and functions of moments have been extensively employed as invariant global features of images in pattern recognition. In this study, a flex- ible recognition system that can compute the good features for high classification of 3-D real objects is investigated. For object recognition, regardless of orientation, size and position, feature vectors are computed with the help of nonlinear moment invariant functions. Representations of objects using two-dimensional images that are taken from different angles of view are the main features leading us to our objective. After efficient feature extraction, the main focus of this study, the recog- nition performance of classifiers in conjunction with momentbased feature sets, is introduced.},
author = {Mercimek, Muharrem and Gulez, Kayhan and Mumcu, Tarik Veli},
doi = {10.1007/BF02716709},
isbn = {0256-2499},
issn = {0256-2499},
journal = {Sadhana},
keywords = {3-D object recognition,3-d object recognition,Engineering- general,Image processing,Neural networks,Regular moment functions,fuzzy K-NN,fuzzy k-nn,image process-,ing,neural networks,regular moment functions},
language = {en},
mendeley-tags = {3-D object recognition,Engineering- general,Image processing,Neural networks,Regular moment functions,fuzzy K-NN},
month = {dec},
number = {6},
pages = {765--775},
title = {{Real object recognition using moment invariants}},
url = {http://link.springer.com/article/10.1007/BF02716709 http://link.springer.com/article/10.1007{\%}2FBF02716709 http://link.springer.com/content/pdf/10.1007{\%}2FBF02716709.pdf},
volume = {30},
year = {2005}
}
@article{Meyer2009,
abstract = {How does the brain represent external reality so that it can be perceived in the form of mental images? How are the representations stored in memory so that an approximation of their original content can be re-experienced during recall? A framework introduced in the late 1980s proposed that mental images arise from neural activity in early sensory cortices both during perception and recall. Neurons in the association cortices, by contrast, would not code explicit mental content; rather, they would hold the records needed to reconstruct an approximation of the original perceptual maps in early cortices. Several neurophysiological and neuroimaging studies now lend growing support to this proposal. ?? 2009 Elsevier Ltd. All rights reserved.},
annote = {Information is coded in associative manner, not fully, but enough to reconstruct an approximated multisensory image.},
author = {Meyer, Kaspar and Damasio, Antonio},
doi = {10.1016/j.tins.2009.04.002},
isbn = {0166-2236},
issn = {01662236},
journal = {Trends in Neurosciences},
keywords = {Animals,Brain,Folder - Haptic-Visual integration - bio motivatio,Humans,Memory,Nerve Net,Neurons,Recognition (Psychology)},
language = {eng},
mendeley-tags = {Animals,Brain,Folder - Haptic-Visual integration - bio motivatio,Humans,Memory,Nerve Net,Neurons,Recognition (Psychology)},
month = {jul},
number = {7},
pages = {376--382},
pmid = {19520438},
title = {{Convergence and divergence in a neural architecture for recognition and memory}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19520438},
volume = {32},
year = {2009}
}
@book{Mobahi,
abstract = {This work proposes a learning method for deep architectures that takes advantage of sequential data, in particular from the tem- poral coherence that naturally exists in un- labeled video recordings. That is, two suc- cessive frames are likely to contain the same object or objects. This coherence is used as a supervisory signal over the unlabeled data, and is used to improve the performance on a supervised task of interest. We demonstrate the effectiveness of this method on some pose invariant object and face recognition tasks.},
annote = {
Using videos of rotating objects, COIL100 comparisons show state of the art results. Crucially they show that even if the unlabelled video data (used for regularization by enforcing coherence of consecutive frames) is of similar or even different (animal) objects, there is still an improvement in the supervised learning task.

----------

Uses deep CNNs together with an enforcement of coherence between high layer representation of pairs of images that come from consecutive frames in a video.
},
author = {Mobahi, Hossein and Collobert, Ronan and Weston, Jason},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
doi = {10.1145/1553374.1553469},
isbn = {9781605585161},
keywords = {Folder - convolutional},
mendeley-tags = {Folder - convolutional},
pages = {737--744},
title = {{Deep Learning from Temporal Coherence in Video}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=3F77492CF644A31D26C0720E07AB5BE6?doi=10.1.1.148.5771{\&}rep=rep1{\&}type=pdf http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.148.5771},
year = {2009}
}
@inproceedings{Morioka2011,
abstract = {Spatial relationships between local features are thought to play a vital role in representing object categories. How- ever, learning a compact set of higher-order spatial features based on visual words, e.g., doublets and triplets, remains a challenging problem as possible combinations of visual words grow exponentially. While the local pairwise code- book achieves a compact codebook of pairs of spatially close local features without feature selection, its formula- tion is not scale invariant and is only suitable for densely sampled local features. In contrast, the proximity distribu- tion kernel is a scale-invariant and robust representation capturing rich spatial proximity information between lo- cal features, but its representation grows quadratically in the number of visual words. Inspired by the two above- mentioned techniques, this paper presents the compact cor- relation coding that combines the strengths of the two. Our method achieves a compact representation that is scale- invariant and robust against object deformation. In addi- tion, we adopt sparse coding instead of k-means clustering during the codebook construction to increase the discrimi- native power of our method. We systematically evaluate our method against both the local pairwise codebook and prox- imity distribution kernel on several challenging object cat- egorization datasets to show performance improvements.},
author = {Morioka, Nobuyuki and Satoh, Shin'ichi},
booktitle = {2011 International Conference on Computer Vision},
doi = {10.1109/ICCV.2011.6126425},
file = {:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Morioka, Satoh - 2011 - Compact correlation coding for visual object categorization.html:html},
isbn = {978-1-4577-1102-2},
issn = {1550-5499},
keywords = {CCC,Compact Correlation Coding,Computer vision,Correlation,Encoding,Folder - constellation-star models - geometry over,Joints,Kernel,Robustness,Sparse Coding,Visualization,codebook construction,compact codebook,compact correlation coding,correlation methods,image coding,image representation,k-means clustering,local pairwise codebook,object category representation,object deformation,pattern clustering,proximity distribution kernel,scale-invariant representation,spatial proximity information,visual object categorization,visual words},
mendeley-tags = {Computer vision,Correlation,Encoding,Folder - constellation-star models - geometry over,Joints,Kernel,Robustness,Sparse Coding,Visualization,codebook construction,compact codebook,compact correlation coding,correlation methods,image coding,image representation,k-means clustering,local pairwise codebook,object category representation,object deformation,pattern clustering,proximity distribution kernel,scale-invariant representation,spatial proximity information,visual object categorization,visual words},
month = {nov},
pages = {1639--1646},
title = {{Compact correlation coding for visual object categorization}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6126425},
volume = {1},
year = {2011}
}
@inproceedings{Myers2014,
annote = {dataset available: http://www.umiacs.umd.edu/{\~{}}amyers/part-affordance-dataset/----------affordances:  grasp, wrap-
grasp, cut, contain, support, scoop, and pound----------Segments depthmap of objects into parts and tries to identify the affordance of the object from the parts' geometries, curvature, colour, etc. using SVMs. Best indicator is geometry (3D points)----------107 tools, 17 categories, 7 affordances},
author = {Myers, Austin and Kanazawa, Angjoo and Fermuller, Cornelia and Aloimonos, Yiannis},
booktitle = {CVPR Workshop},
pages = {5--6},
publisher = {IROS},
title = {{Affordance of Object Parts from Geometric Features}},
url = {http://www.cs.cornell.edu/{~}hema/rgbd-workshop-2014/papers/Myers.pdf},
year = {2014}
}
@inproceedings{Navarro2012,
abstract = {In this paper, we present an approach for haptic object recognition and its evaluation on multi-fingered robot hands. The recognition approach is based on extracting key features of tactile and kinesthetic data from multiple palpations using a clustering algorithm. A multi-sensory object representation is built by fusion of tactile and kinesthetic features. We evaluated our approach on three robot hands and compared the recognition performance using object sets consisting of daily household objects. Experimental results using the five-fingered hand of the humanoid robot ARMAR, the three-fingered Schunk Dexterous Hand 2 and a parallel Gripper are performed. The results show that the proposed approach generalizes to different robot hands.},
annote = {
weiss sensor

----------

Better results combining modalities

----------

robustly portable to other hands

----------

Uses PCA + SOM to extract tactile features and SOM to extract kinesthetic features. Concatenates both to create a "haptic feature". Uses ANN to classfiy objects according to these.

----------

concatenates tactile and kinesthetic descriptors
},
author = {Navarro, Stefan Escaida and Gorges, Nicolas and W{\"{o}}rn, Heinz and Schill, Julian and Asfour, Tamim and Dillmann, R{\"{u}}diger},
booktitle = {Haptics Symposium 2012, HAPTICS 2012 - Proceedings},
doi = {10.1109/HAPTIC.2012.6183837},
isbn = {9781467308090},
keywords = {ARMAR,Feature extraction,Folder - TOR by BOF,Grippers,Vectors,clustering algorithm,daily household objects,dexterous manipulators,five-fingered hand,haptic interfaces,haptic object recognition,humanoid robot,humanoid robots,key feature extraction,kinesthetic data,kinesthetic features,multifingered robot hands,multiple palpations,multisensory object representation,object recognition,object sets,parallel gripper,pattern clustering,recognition approach,recognition performance,tactile data,tactile features,tactile sensors,three-fingered Schunk dexterous hand 2},
mendeley-tags = {ARMAR,Feature extraction,Folder - TOR by BOF,Grippers,Vectors,clustering algorithm,daily household objects,dexterous manipulators,five-fingered hand,haptic interfaces,haptic object recognition,humanoid robot,humanoid robots,key feature extraction,kinesthetic data,kinesthetic features,multifingered robot hands,multiple palpations,multisensory object representation,object recognition,object sets,parallel gripper,pattern clustering,recognition approach,recognition performance,tactile data,tactile features,tactile sensors,three-fingered Schunk dexterous hand 2},
pages = {497--502},
title = {{Haptic object recognition for multi-fingered robot hands}},
url = {http://ieeexplore.ieee.org/ielx5/6179079/6183751/06183837.pdf?tp={\&}arnumber=6183837{\&}isnumber=6183751 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6183837},
year = {2012}
}
@inproceedings{Newcombe2010,
abstract = {We present a method which enables rapid and dense reconstruction of scenes browsed by a single live camera. We take point-based real-time structure from motion (SFM) as our starting point, generating accurate 3D camera pose estimates and a sparse point cloud. Our main novel contribution is to use an approximate but smooth base mesh generated from the SFM to predict the view at a bundle of poses around automatically selected reference frames spanning the scene, and then warp the base mesh into highly accurate depth maps based on view-predictive optical flow and a constrained scene flow update. The quality of the resulting depth maps means that a convincing global scene model can be obtained simply by placing them side by side and removing overlapping regions. We show that a cluttered indoor environment can be reconstructed from a live hand-held camera in a few seconds, with all processing performed by current desktop hardware. Real-time monocular dense reconstruction opens up many application areas, and we demonstrate both real-time novel view synthesis and advanced augmented reality where augmentations interact physically with the 3D scene and are correctly clipped by occlusions.},
author = {Newcombe, Richard A. and Davison, Andrew J.},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2010.5539794},
isbn = {9781424469840},
issn = {10636919},
keywords = {3D camera pose estimates,3D scene,Augmented reality,Cameras,Clouds,Computer Graphics,Layout,Machine vision,Robot vision systems,Surface reconstruction,accurate depth maps,advanced augmented reality,constrained scene flow update,desktop hardware,global scene model,image motion analysis,image reconstruction,live dense reconstruction,live hand-held camera,occlusion,point-based real-time structure from motion,real-time monocular dense reconstruction,scenes,simultaneous localization and mapping,single moving camera,sparse point cloud,view-predictive optical flow},
mendeley-tags = {3D camera pose estimates,3D scene,Augmented reality,Cameras,Clouds,Computer Graphics,Layout,Machine vision,Robot vision systems,Surface reconstruction,accurate depth maps,advanced augmented reality,constrained scene flow update,desktop hardware,global scene model,image motion analysis,image reconstruction,live dense reconstruction,live hand-held camera,occlusion,point-based real-time structure from motion,real-time monocular dense reconstruction,scenes,simultaneous localization and mapping,single moving camera,sparse point cloud,view-predictive optical flow},
month = {jun},
pages = {1498--1505},
pmid = {5539794},
title = {{Live dense reconstruction with a single moving camera}},
url = {http://ieeexplore.ieee.org/xpls/icp.jsp?arnumber=5539794},
year = {2010}
}
@inproceedings{Newcombe2011,
abstract = {DTAM is a system for real-time camera tracking and reconstruction which relies not on feature extraction but dense, every pixel methods. As a single hand-held RGB camera flies over a static scene, we estimate detailed textured depth maps at selected keyframes to produce a surface patchwork with millions of vertices. We use the hundreds of images available in a video stream to improve the quality of a simple photometric data term, and minimise a global spatially regularised energy functional in a novel non-convex optimisation framework. Interleaved, we track the camera's 6DOF motion precisely by frame-rate whole image alignment against the entire dense model. Our algorithms are highly parallelisable throughout and DTAM achieves real-time performance using current commodity GPU hardware. We demonstrate that a dense model permits superior tracking performance under rapid motion compared to a state of the art method using features; and also show the additional usefulness of the dense model for real-time scene interaction in a physics-enhanced augmented reality application.},
annote = {
Minimises an energy function which includes a depthmap regularization term (considering smooth 2d patches should be smooth in 3d) and a photographic cost (how well does the projection of the 3d match the 2d seen image).

----------

First monocular dense SLAM

----------

Ignores bad matches (moving, disappearing or noisy points)
},
author = {Newcombe, Richard A. and Lovegrove, Steven J. and Davison, Andrew J.},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2011.6126513},
isbn = {9781457711015},
issn = {1550-5499},
keywords = {Augmented reality,Cameras,GPU hardware,Optimization,Real time systems,Robustness,Tracking,Vectors,concave programming,dense model,dense tracking and mapping,energy functional,graphics processing units,hand-held RGB camera,image alignment,image motion analysis,image reconstruction,image texture,nonconvex optimisation,object tracking,photometric data term,physics-enhanced augmented reality,real-time camera reconstruction,real-time camera tracking,real-time scene interaction,textured depth map,video stream},
mendeley-tags = {Augmented reality,Cameras,GPU hardware,Optimization,Real time systems,Robustness,Tracking,Vectors,concave programming,dense model,dense tracking and mapping,energy functional,graphics processing units,hand-held RGB camera,image alignment,image motion analysis,image reconstruction,image texture,nonconvex optimisation,object tracking,photometric data term,physics-enhanced augmented reality,real-time camera reconstruction,real-time camera tracking,real-time scene interaction,textured depth map,video stream},
month = {nov},
pages = {2320--2327},
pmid = {6126513},
shorttitle = {DTAM},
title = {{DTAM: Dense tracking and mapping in real-time}},
url = {http://ieeexplore.ieee.org/ielx5/6118259/6126217/06126513.pdf?tp={\&}arnumber=6126513{\&}isnumber=6126217 http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp={\&}arnumber=6126513{\&}queryText{\%}3Ddtam},
year = {2011}
}
@article{Newell2005,
abstract = {Real-world scene perception can often involve more than one sensory modality. Here we investigated the visual, haptic and crossmodal recognition of scenes of familiar objects. In three experiments participants first learned a scene of objects arranged in random positions on a platform. After learning, the experimenter swapped the position of two objects in the scene and the task for the participant was to identify the two swapped objects. In experiment 1, we found a cost in scene recognition performance when there was a change in sensory modality and scene orientation between learning and test. The cost in crossmodal performance was not due to the participants verbally encoding the objects (experiment 2) or by differences between serial and parallel encoding of the objects during haptic and visual learning, respectively (experiment 3). Instead, our findings suggest that differences between visual and haptic representations of space may affect the recognition of scenes of objects across these modalities.},
annote = {It has been noted however, that the representation of scene layouts in haptics and vision are likely not the same, but some form of abstraction is possibly required to make them compatible},
author = {Newell, Fiona N. and Woods, Andrew T. and Mernagh, Marion and B??lthoff, Heinrich H.},
doi = {10.1007/s00221-004-2067-y},
isbn = {3531671200},
issn = {00144819},
journal = {Experimental Brain Research},
keywords = {Crossmodal recognition,Folder - Haptic-Visual integration - bio motivatio,Haptics,Orientation dependency,Scene perception,Vision,haptics},
language = {en},
mendeley-tags = {Crossmodal recognition,Folder - Haptic-Visual integration - bio motivatio,Orientation dependency,Scene perception,Vision,haptics},
month = {feb},
number = {2},
pages = {233--242},
pmid = {15490135},
title = {{Visual, haptic and crossmodal recognition of scenes}},
url = {http://link.springer.com/article/10.1007/s00221-004-2067-y http://link.springer.com/content/pdf/10.1007{\%}2Fs00221-004-2067-y.pdf},
volume = {161},
year = {2005}
}
@article{Newell2001,
abstract = {Abstract—On the whole, people recognize objects best when they see the objects from a familiar view and worse when they see the objects from views that were previously occluded from sight. Unexpectedly, we found haptic object recognition to be viewpoint-specific as well, even though hand movements were unrestricted. This viewpoint dependence was due to the hands preferring the back “view ” of the objects. Furthermore, when the sensory modalities (visual vs. haptic) differed between learning an object and recognizing it, recognition performance was best when the objects were rotated back-to-front between learning and recognition. Our data indicate that the visual system recognizes the front view of objects best, whereas the hand recognizes objects best from the back. People explore and navigate through their environment mainly using sight and touch. In order to guide actions and interactions with objects, information acquired from the visual and the haptic systems},
annote = {suggest that haptic models are view-point specific and that human visual and haptic models of objects share information.},
author = {Newell, Fiona N and Ernst, Marc O and Tjan, Bosco S and Bu, Heinrich H},
journal = {Psychological Science},
keywords = {Folder - Haptic-Visual integration,Folder - Haptic-Visual integration - bio motivatio},
mendeley-tags = {Folder - Haptic-Visual integration,Folder - Haptic-Visual integration - bio motivatio},
number = {1},
pages = {37--42},
title = {{Research Article VIEWPOINT DEPENDENCE IN VISUAL AND HAPTIC}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=4794BD08F031C2A9CDFAF357F707D6F7?doi=10.1.1.126.5892{\&}rep=rep1{\&}type=pdf http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.126.5892},
volume = {12},
year = {2001}
}
@article{Ngiam2011,
abstract = {Deep networks have been successfully applied to unsupervised feature learning for single modalities (e.g., text, images or audio). In this work, we propose a novel application of deep networks to learn features over multiple modalities. We present a series of tasks for multimodal learning and show how to train deep networks that learn features to address these tasks. In particular, we demonstrate cross modality feature learning, where better features for one modality (e.g., video) can be learned ifmultiple modalities (e.g., audio and video) are present at feature learning time. Furthermore, we show how to learn a shared representation between modalities and evalu- ate it on a unique task, where the classifier is trained with audio-only data but tested with video-only data and vice-versa. Our mod- els are validated on the CUAVE and AVLet- ters datasets on audio-visual speech classifi- cation, demonstrating best published visual speech classification on AVLetters and effec- tive shared representation learning.},
annote = {Use two deep autoencoders (DAEs) to fuse video and audio by enforcing a joint hidden layer, and force it to recreate one modality from another, thus learning a joint distribution in the DAE's hidden layer. They claim that more robust representations are learnt for video when audio data is also provided. In particular, the multimodal system is more robust to noise than the unimodal version.
In their context, a joint highlevel descriptor (model fusion at the hidden layer) performs better than a joint lowlevel descriptor (feature fusion via input concatenation)},
author = {Ngiam, Jiquan and Khosla, Aditya and Kim, Mingyu and Nam, Juhan and Lee, Honglak and Ng, Andrew Y},
doi = {10.1145/2647868.2654931},
isbn = {9781450306195},
journal = {Proceedings of The 28th International Conference on Machine Learning (ICML)},
pages = {689--696},
title = {{Multimodal Deep Learning}},
year = {2011}
}
@article{Schmitz2014,
abstract = {For humans to accurately understand the world around them, multimodal integration is essential because it enhances perceptual precision and reduces ambiguity. Computational models replicating such human ability may contribute to the practical use of robots in daily human living environments; however, primarily because of scalability problems that conventional machine learning algorithms suffer from, sensory-motor information processing in robotic applications has typically been achieved via modal-dependent processes. In this paper, we propose a novel computational framework enabling the integration of sensory-motor time-series data and the self-organization of multimodal fused representations based on a deep learning approach. To evaluate our proposed model, we conducted two behavior-learning experiments utilizing a humanoid robot; the experiments consisted of object manipulation and bell-ringing tasks. From our experimental results, we show that large amounts of sensory-motor information, including raw RGB images, sound spectrums, and joint angles, are directly fused to generate higher-level multimodal representations. Further, we demonstrated that our proposed framework realizes the following three functions: (1) cross-modal memory retrieval utilizing the information complementation capability of the deep autoencoder; (2) noise-robust behavior recognition utilizing the generalization capability of multimodal features; and (3) multimodal causality acquisition and sensory-motor prediction based on the acquired causality. ?? 2014 Elsevier B.V. All rights reserved.},
annote = {Use deep autoencoders to seek a cross-modal representation of vision and propioception (including motion)},
author = {Noda, Kuniaki and Arie, Hiroaki and Suga, Yuki and Ogata, Tetsuya},
doi = {10.1016/j.robot.2014.03.003},
file = {:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schmitz et al. - 2014 - Tactile object recognition using deep learning and dropout.pdf:pdf;:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schmitz et al. - 2014 - Tactile object recognition using deep learning and dropout.html:html;:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Noda et al. - 2014 - Multimodal integration learning of robot behavior using deep neural networks.pdf:pdf},
isbn = {9781467363587},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Cross-modal memory retrieval,Deep learning,F/T sensors,Folder - from grasp,Multimodal integration,Object manipulation,Principal component analysis,Skin,TWENDY-ONE,Training,control engineering computing,deep learning,denoising autoencoder,dexterous manipulators,distributed sensors,distributed skin sensors,dropout,grasped objects recognition,joint angles,learning (artificial intelligence),multifingered hand,multimodal object recognition,object recognition,position relation,power grasping,tactile object recognition,tactile sensors,unknown orientation},
mendeley-tags = {F/T sensors,Folder - from grasp,Principal component analysis,Skin,TWENDY-ONE,Training,control engineering computing,deep learning,denoising autoencoder,dexterous manipulators,distributed sensors,distributed skin sensors,dropout,grasped objects recognition,joint angles,learning (artificial intelligence),multifingered hand,multimodal object recognition,object recognition,position relation,power grasping,tactile object recognition,tactile sensors,unknown orientation},
month = {nov},
number = {6},
pages = {721--736},
publisher = {Elsevier B.V.},
title = {{Multimodal integration learning of robot behavior using deep neural networks}},
url = {http://dx.doi.org/10.1016/j.robot.2014.03.003 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7041493},
volume = {62},
year = {2014}
}
@article{Noll1976,
abstract = {Imaging through atmospheric turbulence by systems with annular pupils is discussed using the Zernike annular polynomials. Fourier transforms of these polynomials are derived analytically to facilitate the calculation of variance and covariance of the aberration coefficients. Zernike annular shape functions are derived and used to calculate the Strehl ratio and the residual phase structure and mutual coherence functions when a certain number of modes are corrected using, say, a deformable mirror. Special cases of long- and short-exposure images are also considered. The results for systems with a circular pupil are obtained as a special case of the annular pupil.},
annote = {
Gives lkinear indexing for zernike polynomials
},
author = {Noll, Robert J.},
doi = {10.1364/JOSA.66.000207},
issn = {0030-3941},
journal = {Journal of the Optical Society of America},
month = {mar},
number = {3},
pages = {207},
pmid = {17164852},
title = {{Zernike polynomials and atmospheric turbulence}},
url = {http://www.osapublishing.org/viewmedia.cfm?uri=josa-66-3-207{\&}seq=0{\&}html=true$\backslash$nhttps://www.osapublishing.org/josa/abstract.cfm?uri=josa-66-3-207},
volume = {66},
year = {1976}
}
@inproceedings{Norouzi2009,
abstract = {In this paper we present a method for learning class-specific features for recognition. Recently a greedy layer-wise procedure was proposed to initialize weights of deep belief networks, by viewing each layer as a separate restricted Boltzmann machine (RBM). We develop the convolutional RBM (C-RBM), a variant of the RBM model in which weights are shared to respect the spatial structure of images. This framework learns a set of features that can generate the images of a specific object class. Our feature extraction model is a four layer hierarchy of alternating filtering and maximum subsampling. We learn feature parameters of the first and third layers viewing them as separate C-RBMs. The outputs of our feature extraction hierarchy are then fed as input to a discriminative classifier. It is experimentally demonstrated that the extracted features are effective for object detection, using them to obtain performance comparable to the state of the art on handwritten digit recognition and pedestrian detection.},
annote = {
Similar architectures to (Lee 2009 - RELATED) using stacked convolutional RBMs has been used for character recognition and person detection in images
},
archivePrefix = {arXiv},
arxivId = {1203.4416},
author = {Norouzi, Mohammad and Ranjbar, Mani and Mori, Greg},
booktitle = {2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2009},
doi = {10.1109/CVPRW.2009.5206577},
eprint = {1203.4416},
isbn = {9781424439935},
issn = {1063-6919},
keywords = {Boltzmann machines,C-RBM,Cellular neural networks,Computer vision,Feature extraction,Filters,Folder - convolutional,Large-scale systems,Neural networks,Nonhomogeneous media,belief networks,convolutional restricted Boltzmann machine,discriminative classifier,feature extraction model,handwritten digit recognition,images spatial structure,learning (artificial intelligence),machine learning,object detection,object recognition,pattern classification,pedestrian detection,shift invariant feature learning},
mendeley-tags = {Boltzmann machines,C-RBM,Cellular neural networks,Computer vision,Feature extraction,Filters,Folder - convolutional,Large-scale systems,Neural networks,Nonhomogeneous media,belief networks,convolutional restricted Boltzmann machine,discriminative classifier,feature extraction model,handwritten digit recognition,images spatial structure,learning (artificial intelligence),machine learning,object detection,object recognition,pattern classification,pedestrian detection,shift invariant feature learning},
month = {jun},
pages = {2735--2742},
title = {{Stacks of convolutional restricted boltzmann machines for shift-invariant feature learning}},
url = {http://ieeexplore.ieee.org/ielx5/5191365/5206488/05206577.pdf?tp={\&}arnumber=5206577{\&}isnumber=5206488 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5206577{\&}tag=1},
year = {2009}
}
@article{Okamura2001,
abstract = {The authors consider the detection of small surface features, such as cracks, bumps, and ridges, on the surface of an object during haptic exploration and dexterous manipulation. Surface feature definition and detection are essential for intelligent haptic exploration and modeling of unknown objects. First, the authors review the representation of object surface geometry and present definitions of features based on local surface curvature. These definitions depend on both the geometry of the robot fingertips and the object being explored. It is also shown that the trajectory traced by a round fingertip rolling or sliding over the object surface has some intrinsic properties that facilitate feature detection. Several algorithms for feature detection based on feature definitions are described and compared, and simulated and experimental results are presented for feature detection using a hemispherical fingertip equipped with a tactile sensor.},
annote = {
define features as areas where there is a surface curvature larger than the curvature of the fingertip sensor (assumed spherical). Propose a number of macrofeatures as a combination of features.
},
author = {Okamura, A. M.},
doi = {10.1177/02783640122068191},
isbn = {0278364012},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
keywords = {haptics,robot exploration,robot fingers,surface modeling,tactile sensing},
language = {en},
mendeley-tags = {haptics,robot exploration,robot fingers,surface modeling,tactile sensing},
month = {dec},
number = {12},
pages = {925--938},
pmid = {174018800001},
title = {{Feature Detection for Haptic Exploration with Robotic Fingers}},
url = {http://ijr.sagepub.com/content/20/12/925.short},
volume = {20},
year = {2001}
}
@article{Omata2004,
abstract = {A novel tactile sensor system, specifically designed to detect and quantify in real time tissue characteristics in a manner analogous to the human hand, is presented. To date, no modality that is capable of detecting the hardness and/or softness of tissues has been developed. The emerging requirement to build smart robotic manipulation for the next generation of virtual systems, incorporating hand-like-tactile feedback, continues to challenge developers in biomedical instrumentation. To satisfy this need we present a novel sensor system, consisting of a combination of a piezoelectric transducer (PZT) and a pressure sensor element. The system is designed with a feedback circuit, made with a disc-shaped PZT sensor element and a phase shift circuit acting as the oscillating circuit. Upon contact with a test object, this system responds to its physical properties by changing its resonance frequency in accordance with the object's acoustic impedance. It is suggested that this sensor may prove useful in applications involving robotics in the biomedical field and can be incorporated in the next generation of virtual operating systems. ?? 2004 Elsevier B.V. All rights reserved.},
annote = {
Uses a novel sensor which combines PZTs and pressure sensors to detect the softness of materials, by exploiting the change in vibration mode of the PZT.
},
author = {Omata, Sadao and Murayama, Yoshinobu and Constantinou, Christos E.},
doi = {10.1016/j.sna.2004.01.038},
isbn = {0924-4247},
issn = {09244247},
journal = {Sensors and Actuators, A: Physical},
keywords = {Robot hand,Surgery robotics,Tactile sensor,Tele-medical care,Virtual system,tactile sensor},
mendeley-tags = {Robot hand,Surgery robotics,Tele-medical care,Virtual system,tactile sensor},
month = {may},
number = {2-3},
pages = {278--285},
title = {{Real time robotic tactile sensor system for the determination of the physical properties of biomaterials}},
url = {http://www.sciencedirect.com/science/article/pii/S0924424704000731 http://www.sciencedirect.com/science/article/pii/S0924424704000731/pdf?md5=c3838cbe992304861751380ed5d0b01b{\&}pid=1-s2.0-S0924424704000731-main.pdf},
volume = {112},
year = {2004}
}
@article{Orbanz2015,
abstract = {The natural habitat of most Bayesian methods is data represented by exchangeable sequences of observations, for which de Finetti's theorem provides the theoretical foundation. Dirichlet process clustering, Gaussian process regression, and many other parametric and nonparametric Bayesian models fall within the remit of this framework; many problems arising in modern data analysis do not. This expository paper provides an introduction to Bayesian models of graphs, matrices, and other data that can be modeled by random structures. We describe results in probability theory that generalize de Finetti's theorem to such data and discuss the relevance of these results to nonparametric Bayesian modeling. With the basic ideas in place, we survey example models available in the literature; applications of such models include collaborative filtering, link prediction, and graph and network analysis. We also highlight connections to recent developments in graph theory and probability, and sketch the more general mathematical foundation of Bayesian methods for other types of data beyond sequences and arrays.},
archivePrefix = {arXiv},
arxivId = {1312.7857},
author = {Orbanz, Peter and Roy, Daniel M.},
doi = {10.1109/TPAMI.2014.2334607},
eprint = {1312.7857},
file = {:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Orbanz, Roy - 2015 - Bayesian models of graphs, arrays and other exchangeable random structures.pdf:pdf;:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Orbanz, Roy - 2015 - Bayesian models of graphs, arrays and other exchangeable random structures.html:html},
isbn = {0162-8828 VO - 37},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Analytical models,Arrays,Bayes methods,Bayesian nonparametrics,Data models,Exchangeable arrays,Folder - models - bayesian,Mathematical model,Random variables,graphs,hidden Markov models,networks,relational data},
mendeley-tags = {Analytical models,Arrays,Bayes methods,Bayesian nonparametrics,Data models,Exchangeable arrays,Folder - models - bayesian,Mathematical model,Random variables,graphs,hidden Markov models,networks,relational data},
month = {feb},
number = {2},
pages = {437--461},
pmid = {26353253},
title = {{Bayesian models of graphs, arrays and other exchangeable random structures}},
url = {http://ieeexplore.ieee.org/ielx7/34/7004096/06847223.pdf?tp={\&}arnumber=6847223{\&}isnumber=7004096 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6847223{\&}tag=1},
volume = {37},
year = {2015}
}
@inproceedings{Osuna1997,
abstract = {We investigate the problem of training a support vector machine (SVM) on a very large database in the case in which the number of support vectors is also very large. Training a SVM is equivalent to solving a linearly constrained quadratic programming (QP) problem in a number of variables equal to the number of data points. This optimization problem is known to be challenging when the number of data points exceeds few thousands. In previous work done by us as well as by other researchers, the strategy used to solve the large scale QP problem takes advantage of the fact that the expected number of support vectors is small ({\textless}3,000). Therefore, the existing algorithms cannot deal with more than a few thousand support vectors. In this paper we present a decomposition algorithm that is guaranteed to solve the QP problem and that does not make assumptions on the expected number of support vectors. In order to present the feasibility of our approach we consider a foreign exchange rate time series database with 110,000 data points that generates 100,000 support vectors},
author = {Osuna, E and Freund, R and Girosi, F},
booktitle = {Neural Networks for Signal Processing [1997] VII. Proceedings of the 1997 IEEE Workshop},
doi = {10.1109/NNSP.1997.622408},
file = {:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Osuna, Freund, Girosi - 1997 - An improved training algorithm for support vector machines.pdf:pdf;:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Osuna, Freund, Girosi - 1997 - An improved training algorithm for support vector machines.html:html},
isbn = {0-7803-4256-9},
issn = {1089-3555},
keywords = {Classification algorithms,Exchange rates,Folder - SVM large dataset,Large-scale systems,Minimization methods,Neural networks,Optimization,Polynomials,Support vector machine,Support vector machine classification,decomposition algorithm,financial data processing,financial data processing learning systems pattern,foreign exchange rate,learning systems,pattern classification,quadratic programming,support vector machines,time series,training algorithm,very large database,very large databases},
mendeley-tags = {Classification algorithms,Exchange rates,Folder - SVM large dataset,Large-scale systems,Minimization methods,Neural networks,Optimization,Polynomials,Support vector machine,Support vector machine classification,decomposition algorithm,financial data processing,foreign exchange rate,learning systems,pattern classification,quadratic programming,support vector machines,time series,training algorithm,very large database,very large databases},
month = {sep},
pages = {276--285},
title = {{An improved training algorithm for support vector machines}},
url = {http://ieeexplore.ieee.org/ielx3/4900/13509/00622408.pdf?tp={\&}arnumber=622408{\&}isnumber=13509 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=622408},
year = {1997}
}
@article{Parzen1962,
author = {Parzen, E},
doi = {10.1214/aoms/1177704472},
issn = {0003-4851},
journal = {Ann Math Stat},
pages = {1065--1076},
title = {{On estimation of a probability density function and mode}},
url = {http://www.researchgate.net/publication/224817392{\_}On{\_}estimation{\_}of{\_}a{\_}probability{\_}density{\_}function{\_}and{\_}mode},
volume = {33},
year = {1962}
}
@inproceedings{Petrovskaya2007,
abstract = {Humans are capable of manipulating objects solely based on the sense of touch. To study this capability in robots, we focus on touch based object localization. At each stage of exploration our goal is to estimate a Bayesian posterior based on measurements obtained thus far. The state space for object localization is six dimensional: three parameters for position and three for orientation of the object. When initial uncertainty is high (0.5m and 360 degrees), precise estimation of the posterior is computationally expensive. We propose an efficient technique that estimates the posterior in real time. The approach - termed Scaling Series - is based on importance sampling. It performs the estimation using a series of successive refinements, gradually scaling the precision from low to high. Our approach can be applied to a wide range of manipulation tasks. We demonstrate its portability on two applications: (1) picking up a box and (2) operating a door handle.},
annote = {
Use particle filters to recover the pose of an object, given the object's 3d mesh, using only tactile sensing. Processing can be done in real time.
},
author = {Petrovskaya, Anna and Khatib, Oussama and Thrun, Sebastian and Ng, Andrew Y},
booktitle = {Robotics Science and Systems, Robot Manipulation Workshop},
pages = {2--7},
title = {{Touch Based Perception for Object Manipulation}},
url = {http://projects.csail.mit.edu/manipulation/rss07/paper{\_}{\_}touch{\_}based{\_}perception{\_}for{\_}object{\_}manipulation{\_}{\_}petrovskaya.pdf http://cs.stanford.edu/groups/manips/images/pdfs/Petrovskaya{\_}2007{\_}RSS.pdf},
year = {2007}
}
@inproceedings{Petrovskaya2006,
abstract = {We consider the problem of autonomously estimating position and orientation of an object from tactile data. When initial uncertainty is high, estimation of all six parameters precisely is computationally expensive. We propose an efficient Bayesian approach that is able to estimate all six parameters in both unimodal and multimodal scenarios. The approach is termed scaling series sampling as it estimates the solution region by samples. It performs the search using a series of successive refinements, gradually scaling the precision from low to high. Our approach can be applied to a wide range of manipulation tasks. We demonstrate its portability on two applications: (1) manipulating a box and (2) grasping a door handle},
annote = {
Extract pose and location of an unknown object for the purposes of grasping and manipulation.
},
author = {Petrovskaya, Anna and Khatib, Oussama and Thrun, Sebastian and Ng, Andrew Y.},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ROBOT.2006.1641793},
isbn = {0780395069},
issn = {10504729},
keywords = {Bayes methods,Bayesian estimation,Bayesian methods,Computational efficiency,Fingers,Grasping,Parameter estimation,Robot sensing systems,Sonar,Uncertainty,autonomous object manipulation,manipulators,position control,position estimation,scaling series sampling,tactile sensors,telerobotics},
mendeley-tags = {Bayes methods,Bayesian estimation,Bayesian methods,Computational efficiency,Fingers,Grasping,Parameter estimation,Robot sensing systems,Sonar,Uncertainty,autonomous object manipulation,manipulators,position control,position estimation,scaling series sampling,tactile sensors,telerobotics},
month = {may},
pages = {707--714},
title = {{Bayesian estimation for autonomous object manipulation based on tactile sensors}},
url = {http://ieeexplore.ieee.org/ielx5/10932/34383/01641793.pdf?tp={\&}arnumber=1641793{\&}isnumber=34383 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1641793},
volume = {2006},
year = {2006}
}
@article{Pezzementi2011,
abstract = {This paper explores the connection between sensor-based perception and exploration in the context of haptic object identification. The proposed approach combines 1) object recognition from tactile appearance with 2) purposeful haptic exploration of unknown objects to extract appearance information. The recognition component brings to bear computer-vision techniques by viewing tactile-sensor readings as images. We present a bag-of-features framework that uses several tactile-image descriptors, some that are adapted from the vision domain and others that are novel, to estimate a probability distribution over object identity as an unknown object is explored. Haptic exploration is treated as a search problem in a continuous space to take advantage of sampling-based motion planning to explore the unknown object and construct its tactile appearance. Simulation experiments of a robot arm equipped with a haptic sensor at the end-effector provide promising validation, thereby indicating high accuracy in identifying complex shapes from tactile information gathered during exploration. The proposed approach is also validated by using readings from actual tactile sensors to recognize real objects.},
annote = {
digitacts (PPS)

----------

lower resulution tactile sensors might be better

----------

Uses simulations to compare various methods of feature extraction (vectorise, Moment Normalized, MN Translation Invariant, Polar Fourier). Latter 2 produce best results in obj recognition.
},
author = {Pezzementi, Zachary and Plaku, Erion and Reyda, Caitlin and Hager, Gregory D.},
doi = {10.1109/TRO.2011.2125350},
isbn = {1552-3098},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Animation and simulation,Feature extraction,Geometry,Surface treatment,appearance information extraction,bag-of-features framework,computer-vision techniques,end effectors,end-effector,force and tactile sensing,haptic exploration,haptic interfaces,haptic object identification,haptic sensor,object recognition,path planning,probability distribution,recognition,robot arm,robot vision,sampling-based motion planning,search problem,search problems,sensor-based exploration,sensor-based perception,statistical distributions,tactile sensors,tactile-image descriptors,tactile-object recognition},
mendeley-tags = {Animation and simulation,Feature extraction,Geometry,Surface treatment,appearance information extraction,bag-of-features framework,computer-vision techniques,end effectors,end-effector,force and tactile sensing,haptic exploration,haptic interfaces,haptic object identification,haptic sensor,object recognition,path planning,probability distribution,recognition,robot arm,robot vision,sampling-based motion planning,search problem,search problems,sensor-based exploration,sensor-based perception,statistical distributions,tactile sensors,tactile-image descriptors,tactile-object recognition},
number = {3},
pages = {473--487},
title = {{Tactile-object recognition from appearance information}},
url = {http://ieeexplore.ieee.org/ielx5/8860/5784192/05752871.pdf?tp={\&}arnumber=5752871{\&}isnumber=5784192 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5752871},
volume = {27},
year = {2011}
}
@inproceedings{Pfister2000,
abstract = {Surface elements (surfels) are a powerful paradigm to efficiently render complex geometric objects at interactive frame rates. Unlike classical surface discretizations, i.e., triangles or quadrilateral meshes, surfels are point primitives without explicit connectivity. Surfel attributes comprise depth, texture color, normal, and others. As a pre-process, an octree-based surfel representation of a geometric object is computed. During sampling, surfel positions and normals are optionally perturbed, and different levels of texture colors are prefiltered and stored per surfel. During rendering, a hierarchical forward warping algorithm projects surfels to a z-buffer. A novel method called visibility splatting determines visible surfels and holes in the z-buffer. Visible surfels are shaded using texture filtering, Phong illumination, and environment mapping using per-surfel normals. Several methods of image reconstruction, including supersampling, offer flexible speed-quality tradeoffs. Due to the simplicity of the operations, the surfel rendering pipeline is amenable for hardware implementation. Surfel objects offer complex shape, low rendering cost and high image quality, which makes them specifically suited for low-cost, real-time graphics, such as games.},
address = {New York, NY, USA},
annote = {
Present an efficient sampling and rendering system for 3D point-based surfaces.
},
author = {Pfister, Hanspeter and Zwicker, Matthias and van Baar, Jeroen and Gross, Markus},
booktitle = {Proceedings of the 27th annual conference on Computer graphics and interactive techniques  - SIGGRAPH '00},
doi = {10.1145/344779.344936},
isbn = {1581132085},
keywords = {rendering systems,texture mapping},
mendeley-tags = {rendering systems,texture mapping},
pages = {335--342},
publisher = {ACM Press/Addison-Wesley Publishing Co.},
series = {SIGGRAPH '00},
shorttitle = {Surfels},
title = {{Surfels}},
url = {http://portal.acm.org/citation.cfm?doid=344779.344936},
year = {2000}
}
@article{Platt1999,
abstract = {The output of a classifier should be a calibrated posterior probability to enable post-processing. Standard SVMs do not provide such probabilities. One method to create probabilities is to directly train a kernel classifier with a logit link function and a regularized maximum likelihood score. However, training with a maximum likelihood score will produce non-sparse kernel machines. Instead, we train an SVM, then train the parameters of an additional sigmoid function to map the SVM outputs into probabilities. This chapter compares classification error rate and likelihood scores for an SVM plus sigmoid versus a kernel method trained with a regularized likelihood error function. These methods are tested on three data-mining-style data sets. The SVM+sigmoid yields probabilities of comparable quality to the regularized maximum likelihood kernel method, while still retaining the sparseness of the SVM.},
author = {Platt, J},
doi = {10.1.1.41.1639},
file = {:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Platt - 1999 - Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods.html:html;:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Platt - 1999 - Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods.pdf:pdf},
isbn = {0262194481},
journal = {Advances in large margin classifiers},
keywords = {Folder - Probabilistical},
mendeley-tags = {Folder - Probabilistical},
number = {3},
pages = {61--74},
pmid = {1000183100},
title = {{Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods}},
url = {https://www.researchgate.net/profile/John{\_}Platt/publication/2594015{\_}Probabilistic{\_}Outputs{\_}for{\_}Support{\_}Vector{\_}Machines{\_}and{\_}Comparisons{\_}to{\_}Regularized{\_}Likelihood{\_}Methods/links/004635154cff5262d6000000.pdf?inViewer=0{\&}pdfJsDownload=0{\&}origin=publication{\_}detail},
volume = {10},
year = {1999}
}
@incollection{Joachims1999,
abstract = {An abstract is not available.},
address = {Cambridge, MA, USA},
author = {Platt, John C},
booktitle = {Advances in kernel methods: support vector learning},
editor = {Sch{\"{o}}lkopf, Bernhard and Burges, Christopher J. C. and Smola, Alexander J.},
isbn = {0-262-19416-3},
keywords = {Folder - SVM large dataset,gaussian,kernels,polinomial,smo,svm,weka},
mendeley-tags = {Folder - SVM large dataset},
pages = {185--208},
publisher = {MIT Press},
title = {{Advances in kernel methods}},
url = {http://portal.acm.org/citation.cfm?id=299105},
year = {1999}
}
@incollection{Platt1999a,
abstract = {An abstract is not available.},
address = {Cambridge, MA, USA},
author = {Platt, John C},
booktitle = {Advances in kernel methods: support vector learning},
editor = {Sch{\"{o}}lkopf, Bernhard and Burges, Christopher J. C. and Smola, Alexander J.},
isbn = {0-262-19416-3},
keywords = {Folder - SVM large dataset,gaussian,kernels,polinomial,smo,svm,weka},
mendeley-tags = {Folder - SVM large dataset},
pages = {185--208},
publisher = {MIT Press},
title = {{Advances in kernel methods}},
url = {http://portal.acm.org/citation.cfm?id=299105},
year = {1999}
}
@article{Prats2010,
abstract = {Whereas vision and force feedback-either at the wrist or at the joint level-for robotic manipulation purposes has received considerable attention in the literature, the benefits that tactile sensors can provide when combined with vision and force have been rarely explored. In fact, there are some situations in which vision and force feedback cannot guarantee robust manipulation. Vision is frequently subject to calibration errors, occlusions and outliers, whereas force feedback can only provide useful information on those directions that are constrained by the environment. In tasks where the visual feedback contains errors, and the contact configuration does not constrain all the Cartesian degrees of freedom, vision and force sensors are not sufficient to guarantee a successful execution. Many of the tasks performed in our daily life that do not require a firm grasp belong to this category. Therefore, it is important to develop strategies for robustly dealing with these situations. In this article, a new framework for combining tactile information with vision and force feedback is proposed and validated with the task of opening a sliding door. Results show how the vision-tactile-force approach outperforms vision-force and force-alone, in the sense that it allows to correct the vision errors at the same time that a suitable contact configuration is guaranteed. {\textcopyright} Springer Science+Business Media, LLC 2010.},
annote = {Use VVS (Virtual visual servoing) and multiple sensors to estimate pose of objects


Create a hierarchy of modalities, so that the most reliable reading is used.},
author = {Prats, Mario and Sanz, Pedro J. and {Del Pobil}, Angel P.},
doi = {10.1007/s10514-010-9192-1},
issn = {09295593},
journal = {Autonomous Robots},
keywords = {Artificial Intelligence (incl. Robotics),Computer Imaging- Vision- Pattern Recognition and,Control - Robotics- Mechatronics,Folder - Haptic-Visual integration,Robotics and automation,Sensor-based manipulation,Service robotics,Tactile sensing,service robotics,tactile sensing},
language = {en},
mendeley-tags = {Artificial Intelligence (incl. Robotics),Computer Imaging- Vision- Pattern Recognition and,Control - Robotics- Mechatronics,Folder - Haptic-Visual integration,Robotics and automation,Sensor-based manipulation,service robotics,tactile sensing},
month = {aug},
number = {2},
pages = {201--218},
title = {{Reliable non-prehensile door opening through the combination of vision, tactile and force feedback}},
url = {http://link.springer.com/article/10.1007/s10514-010-9192-1 http://link.springer.com/article/10.1007{\%}2Fs10514-010-9192-1 http://link.springer.com/content/pdf/10.1007{\%}2Fs10514-010-9192-1.pdf},
volume = {29},
year = {2010}
}
@misc{PressureProfileSystems,
author = {{Pressure Profile Systems}},
title = {{DigiTact II}},
url = {http://www.pressureprofile.com/digitacts-sensors},
urldate = {2014-09-24}
}
@article{Pruszynski2014,
abstract = {A fundamental feature of first-order neurons in the tactile system is that their distal axon branches in the skin and forms many transduction sites, yielding complex receptive fields with many highly sensitive zones. We found that this arrangement constitutes a peripheral neural mechanism that allows individual neurons to signal geometric features of touched objects. Specifically, we observed that two types of first-order tactile neurons that densely innervate the glabrous skin of the human fingertips signaled edge orientation via both the intensity and the temporal structure of their responses. Moreover, we found that the spatial layout of a neuron's highly sensitive zones predicted its sensitivity to particular edge orientations. We submit that peripheral neurons in the touch-processing pathway, as with peripheral neurons in the visual-processing pathway, perform feature extraction computations that are typically attributed to neurons in the cerebral cortex. Visual},
annote = {Tactile information encoding is necessary. It has recently been suggested that basic encodings such as edge orientation and even edge displacement are detected by humans in first order tactile neurons, not in the cerebral cortex.},
author = {Pruszynski, J Andrew and Johansson, Roland S.},
doi = {10.1038/nn.3804},
isbn = {1546-1726 (Electronic) 1097-6256 (Linking)},
issn = {15461726},
journal = {Nature Neuroscience},
keywords = {Folder - Haptic-Visual integration - bio motivatio},
language = {en},
mendeley-tags = {Folder - Haptic-Visual integration - bio motivatio},
month = {aug},
number = {10},
pages = {1404--1409},
pmid = {25174006},
title = {{Edge-orientation processing in first-order tactile neurons.}},
url = {http://dx.doi.org/10.1038/nn.3804},
volume = {17},
year = {2014}
}
@inproceedings{Quiroga2012,
abstract = {The scene flow describes the motion of each 3D point between two time steps. With the arrival of new depth sensors, as the Microsoft Kinect, it is now possible to compute scene flow with a single camera, with promising repercussion in a wide range of computer vision scenarios. We propose a novel method to compute a local scene flow by tracking in a Lucas-Kanade framework. Scene flow is estimated using a pair of aligned intensity and depth images but rather than computing a dense scene flow as in most previous methods, we get a set of 3D motion vectors by tracking surface patches. Assuming a 3D local rigidity of the scene, we propose a rigid translation flow model that allows solving directly for the scene flow by constraining the 3D motion field both in intensity and depth data. In our experimentation we achieve very encouraging results. Since this approach solves simultaneously for the 2D tracking and for the scene flow, it can be used for motion analysis in existing 2D tracking based methods or to define scene flow descriptors. {\textcopyright} 2013 Elsevier Inc. All rights reserved.},
annote = {
Tracks surface patches (ala L-Kanade) and looks for rigid translations.

----------

Assumes independent rigid moving 3D parts, but no smoothness.
},
author = {Quiroga, Julian and Devernay, Fr{\'{e}}d{\'{e}}ric and Crowley, James},
booktitle = {Journal of Visual Communication and Image Representation},
doi = {10.1016/j.jvcir.2013.03.018},
isbn = {9781467316118},
issn = {10473203},
keywords = {2D tracking based methods,3D motion estimation,3D point motion field,Brightness consistency,Cameras,Computer vision,Depth data,Folder - scene flow - dense scene flow,Image tracking,Image warping,Locally-rigid motion,Lucas-Kanade framework,Microsfot Kinect,Optical flow,Optical imaging,Optical sensors,Scene flow,Solid modeling,Tracking,Vectors,action recognition,computer vision scenarios,depth data,depth images,depth sensors,intensity data,intensity images,interactive devices,object recognition,object tracking,rigid translation flow model,scene flow computation,scene flow descriptors,single camera,surface patch tracking},
mendeley-tags = {2D tracking based methods,3D point motion field,Cameras,Computer vision,Folder - scene flow - dense scene flow,Lucas-Kanade framework,Microsfot Kinect,Optical imaging,Optical sensors,Solid modeling,Tracking,Vectors,action recognition,computer vision scenarios,depth data,depth images,depth sensors,intensity data,intensity images,interactive devices,object recognition,object tracking,rigid translation flow model,scene flow computation,scene flow descriptors,single camera,surface patch tracking},
month = {jun},
number = {1},
pages = {98--107},
title = {{Local scene flow by tracking in intensity and depth}},
url = {http://ieeexplore.ieee.org/ielx5/6230822/6238883/06239237.pdf?tp={\&}arnumber=6239237{\&}isnumber=6238883 http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6239237{\&}navigation=1},
volume = {25},
year = {2014}
}
@phdthesis{Rafla1991,
annote = {Obtain information about surface normals using vision and touch independently and classify objects accordingly.
Rafla in their PhD thesis developed a method to integrate tactile and visual range data to recreate surface equations analytically$\backslash$cite{\{}Rafla{\_}1991{\}}.},
author = {Rafla, Nader Iskander},
keywords = {Folder - Haptic-Visual integration},
mendeley-tags = {Folder - Haptic-Visual integration},
title = {{Visually guided tactile and force-torque sensing for object recognition and localization}},
url = {https://etd.ohiolink.edu/ap:10:0::NO:10:P10{\_}ETD{\_}SUBID:50815 https://etd.ohiolink.edu/ap:0:0:APPLICATION{\_}PROCESS=DOWNLOAD{\_}ETD{\_}SUB{\_}DOC{\_}ACCNUM:::F1501{\_}ID:case1059578946,},
year = {1991}
}
@inproceedings{Ratnasingam2011,
abstract = {In this paper a biologically inspired spiking neural network based haptic object recognition system is proposed. A number of different encoding schemes to convert the haptic measurements into spike train are proposed and investigated for haptic object recognition and compared with existing encoding schemes. The spiking neural network was trained using a supervised training approach that is based on the steepest descent algorithm. During the training, firing threshold of the hidden layer neurons were modified in such a way that the ability of the system in recognising different objects is maximised. A multiplexing scheme is used to convert the parallel spike train of the hidden layer into a serial stream. To convert the output spike train into a reliable feature that represents the shape of an object, moment of the spikes with respect to a reference time is calculated. A robot hand with three fingers that have the same number of degrees of freedom as the human fingers was used for testing the system. The hand was made to grasp different objects and the joint angles were recorded. These recorded angles were converted into spike train using different encoding schemes and applied as input to the network. Test results show that the performance of the system varies depending on the input encoding scheme and with the best encoring scheme the system can recognise 100{\%} of 7 different objects.},
annote = {
Compare 8 encoding schemes for a spiking feedforward neural network, trained to recognise 7 objects from the grasp contact of a robotic hand. Report accuracy of 100{\%}, which is obtained in one of the schemes. Would be interesting to see how it performs on a larger dataset.
},
author = {Ratnasingam, Sivalogeswaran and McGinnity, T. M.},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2011.6048508},
isbn = {9781612844541},
keywords = {Encoding,Joints,Robot sensing systems,Thumb,haptic interfaces},
mendeley-tags = {Encoding,Joints,Robot sensing systems,Thumb,haptic interfaces},
pages = {3446--3453},
title = {{A comparison of encoding schemes for haptic object recognition using a biologically plausible spiking neural network}},
url = {http://ieeexplore.ieee.org/ielx5/6034548/6094399/06094835.pdf?tp={\&}arnumber=6094835{\&}isnumber=6094399 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6094835},
year = {2011}
}
@inproceedings{Ren2013,
abstract = {Facial expression recognition remains a challenging problem especially when the face is partially corrupted or occluded. We propose using a new classification method, termed Sparse Representation based Classification (SRC), to accurately recognize expressions under these conditions. A test vector is representable as a linear combination of vectors from its own class and so its representation as a linear combination of all available training vectors is sparse. Efficient methods have been developed in the area of compressed sensing to recover this sparse representation. SRC gives state of the art performance on clean and noise corrupted images matching the recognition rate obtained using Gabor based features. When test images are occluded by square black blocks, SRC improves significantly on the performance obtained using Gabor features; SRC increases the recognition rate by 6.6{\%} when the block occlusion length is 30 and by 11.2{\%} when the block length is 40.},
annote = {
Learn features from data using KSVD. Combined with DPM it outperforms HOG.
},
archivePrefix = {arXiv},
arxivId = {chao-dyn/9411012},
author = {Ren, Xiaofeng and Ramanan, Deva},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer  Vision and Pattern Recognition},
doi = {10.1109/CVPR.2013.417},
eprint = {9411012},
isbn = {9781424442966},
issn = {10636919},
keywords = {Computational modeling,Detectors,Dictionaries,Feature Learning,Feature extraction,HOG feature,Histograms,K-SVD,Object Detection,Sparse Coding,Supervised Training,Training,dimension reduction,histogram of oriented gradient,histogram of sparse code,image coding,image representation,learning,learning (artificial intelligence),object detection,part-based model,root only model,sliding window framework,sparse representation,supervised training},
mendeley-tags = {Computational modeling,Detectors,Dictionaries,Feature Learning,Feature extraction,HOG feature,Histograms,K-SVD,Sparse Coding,Training,dimension reduction,histogram of oriented gradient,histogram of sparse code,image coding,image representation,learning,learning (artificial intelligence),object detection,part-based model,root only model,sliding window framework,sparse representation,supervised training},
month = {jun},
pages = {3246--3253},
pmid = {8637596},
primaryClass = {chao-dyn},
title = {{Histograms of sparse codes for object detection}},
url = {http://ieeexplore.ieee.org/ielx7/6596161/6618844/06619261.pdf?tp={\&}arnumber=6619261{\&}isnumber=6618844 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6619261{\&}tag=1},
year = {2013}
}
@inproceedings{RichardSocher2012,
annote = {
Use Convolutional Neural Nets and Recursive Neural Nets to learn features from RGBD data. Perform as well as state of the art (Bo et al., 2012)
},
author = {{Richard Socher and Brody Huval and Bharath Bhat and Christopher D. Manning and Andrew Y. Ng}},
booktitle = {Advances in Neural Information Processing Systems 25},
file = {:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Richard Socher and Brody Huval and Bharath Bhat and Christopher D. Manning and Andrew Y. Ng - 2012 - Convolutional-Recursive Deep Learn.html:html},
pages = {665----673},
title = {{Convolutional-Recursive Deep Learning for 3D Object Classification}},
url = {http://cs.stanford.edu/people/ang/?portfolio=convolutional-recursive-deep-learning-for-3d-object-classification},
year = {2012}
}
@incollection{Roke2011a,
abstract = {Skin deformation has been previously shown as vital for lump detection during the direct manipulation of an object. A deformation-based tactile feedback system is developed and presented that senses and displays this tactile information for palpation in tele-surgical applications. A biologically- inspired tactile sensor modelled from the human fingertip is used to obtain the skin deformation during interaction. It does this by measuring the displacement of the sensor's artificial intermediate epidermal ridges using a simple, computationally-efficient algorithm. The design of a previously-published tactile shape display is then recreated and improved for relaying this sensed information on to a human user's fingertip. This tactile display uses remote actuation to reduce the mass of the display, avoiding the issue of adding a large mass to a tele-operation interface. The tactors within the developed display exhibit 2.5 mm displacement, with a 2.5 mm spacing, 12 Hz bandwidth and a stiffness of 5.0 N/mm. A linear relationship is found between sensor deformation and tactor displacement and the spatial performance of the system is proven by successfully detecting lumps within artificial muscle tissue. This new deformation-based tactile system offers an intuitive sense of touch with minimal processing. {\textcopyright} 2011 Springer-Verlag Berlin Heidelberg.},
author = {Roke, Callum and Melhuish, Chris and Pipe, Tony and Drury, David and Chorley, Craig},
booktitle = {Taros},
editor = {Gro{\ss}, Roderich and Alboul, Lyuba and Melhuish, Chris and Witkowski, Mark and Prescott, Tony J. and Penders, Jacques},
isbn = {03029743 (ISSN); 9783642232312 (ISBN)},
keywords = {Algorithms,Artificial Intelligence (incl. Robotics),Artificial muscle,Deformation,Direct manipulation,Display devices,Epidermal ridge,Folder - TACTIP,Human users,Image Processing and Computer Vision,Linear relationships,Lump detection,Minimal processing,Sense of touch,Shape display,Skin deformation,Spatial performance,Tactile display,Tactile information,Tactile sensors,Tele-operations,Tissue,User Interfaces and Human Computer Interaction,robotics,sensors,tactile feedback},
mendeley-tags = {Artificial Intelligence (incl. Robotics),Folder - TACTIP,Image Processing and Computer Vision,User Interfaces and Human Computer Interaction},
month = {jan},
pages = {114--124},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Deformation-based tactile feedback using a biologically inspired sensor and an improved display}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-80052785425{\&}partnerID=40{\&}md5=1eef6be32edad9808075858dcea12af7},
volume = {6856 LNAI},
year = {2011}
}
@incollection{Roke2011,
abstract = {Skin deformation has been previously shown as vital for lump detection during the direct manipulation of an object. A deformation-based tactile feedback system is developed and presented that senses and displays this tactile information for palpation in tele-surgical applications. A biologically- inspired tactile sensor modelled from the human fingertip is used to obtain the skin deformation during interaction. It does this by measuring the displacement of the sensor's artificial intermediate epidermal ridges using a simple, computationally-efficient algorithm. The design of a previously-published tactile shape display is then recreated and improved for relaying this sensed information on to a human user's fingertip. This tactile display uses remote actuation to reduce the mass of the display, avoiding the issue of adding a large mass to a tele-operation interface. The tactors within the developed display exhibit 2.5 mm displacement, with a 2.5 mm spacing, 12 Hz bandwidth and a stiffness of 5.0 N/mm. A linear relationship is found between sensor deformation and tactor displacement and the spatial performance of the system is proven by successfully detecting lumps within artificial muscle tissue. This new deformation-based tactile system offers an intuitive sense of touch with minimal processing. {\textcopyright} 2011 Springer-Verlag Berlin Heidelberg.},
author = {Roke, Callum and Melhuish, Chris and Pipe, Tony and Drury, David and Chorley, Craig},
booktitle = {Taros},
editor = {Gro{\ss}, Roderich and Alboul, Lyuba and Melhuish, Chris and Witkowski, Mark and Prescott, Tony J. and Penders, Jacques},
isbn = {03029743 (ISSN); 9783642232312 (ISBN)},
keywords = {Algorithms,Artificial Intelligence (incl. Robotics),Artificial muscle,Deformation,Direct manipulation,Display devices,Epidermal ridge,Human users,Image Processing and Computer Vision,Linear relationships,Lump detection,Minimal processing,Sense of touch,Shape display,Skin deformation,Spatial performance,Tactile display,Tactile information,Tactile sensors,Tele-operations,Tissue,User Interfaces and Human Computer Interaction,robotics,sensors,tactile feedback},
mendeley-tags = {Artificial Intelligence (incl. Robotics),Image Processing and Computer Vision,User Interfaces and Human Computer Interaction},
month = {jan},
pages = {114--124},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Deformation-based tactile feedback using a biologically inspired sensor and an improved display}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-80052785425{\&}partnerID=40{\&}md5=1eef6be32edad9808075858dcea12af7},
volume = {6856 LNAI},
year = {2011}
}
@article{Roke2012,
abstract = {The ability to localise harder areas in soft tissues is often desired during robot-assisted surgical operations. A deformation-based tactile feedback system was tested for the detection of objects within soft tissues, after being chosen over common pressure-based designs. This system uses a biologically inspired sensor that offers a new finger-like approach to tactile sensing. A tactile shape display developed from previous successful designs was used to output the sensed tactile information. Using the tactile feedback system on a mechanical teleoperated device, test subjects palpated a number of artificial tissue models to locate objects of varying stiffness. The addition of the tactile feedback system improved the detection of the objects from 64{\%} to 98{\%}, reduced the localisation error from 18 to 11 mm, and also decreased the time the users spent palpating the tissue from 55 to 37 s. This demonstrates that a deformation-based tactile feedback system can be used to successfully locate hard embedded objects within soft tissue, with a significant improvement over force and visual feedback alone. During testing, it was found that the users were able to more accurately locate the softest embedded objects compared to stiffer ones. Reasons for this observation are discussed. ?? 2012 Elsevier B.V. All rights reserved.},
author = {Roke, Calum and Melhuish, Chris and Pipe, Tony and Drury, David and Chorley, Craig},
doi = {10.1016/j.robot.2012.05.002},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Folder - TACTIP,Tactile feedback display biologically inspired sen},
mendeley-tags = {Folder - TACTIP,Tactile feedback display biologically inspired sen},
month = {nov},
number = {11},
pages = {1442--1448},
title = {{Lump localisation through a deformation-based tactile feedback system using a biologically inspired finger sensor}},
url = {http://www.sciencedirect.com/science/article/pii/S0921889012000589 http://www.sciencedirect.com/science/article/pii/S0921889012000589/pdfft?md5=c452a010404ecb67bcd239a56d6bd835{\&}pid=1-s2.0-S0921889012000589-main.pdf},
volume = {60},
year = {2012}
}
@article{Rosenblatt1956,
abstract = {This note discusses some aspects of the estimation of the density function of a univariate probability distribution. All estimates of the density function satisfying relatively mild conditions are shown to be biased. The asymptotic mean square error of a particular class of estimates is evaluated.},
author = {Rosenblatt, Murray},
doi = {10.1214/aoms/1177728190},
isbn = {00034851},
issn = {0003-4851},
journal = {The Annals of Mathematical Statistics},
language = {EN},
month = {sep},
number = {3},
pages = {832--837},
title = {{Remarks on Some Nonparametric Estimates of a Density Function}},
url = {http://projecteuclid.org/euclid.aoms/1177728190},
volume = {27},
year = {1956}
}
@inproceedings{Russell2000,
annote = {
Uses geometric features such as lines and points, together with their evolution over time (named tactemes, since they are similar in concept to phonemes in speech recognition). Their accuracy recognising objects is high (83$\backslash${\%}), however the number of shapes is only 6 and they are very basic predefined geomtric solids (cylinder, cone, etc.).
},
author = {Russell, Ra},
booktitle = {Proceedings of the Australian Conference on Robotics {\ldots}},
pages = {93--98},
title = {{Object recognition by a 'smart' tactile sensor}},
url = {http://www.araa.asn.au/acra/acra2000/papers/paper17.pdf},
year = {2000}
}
@inproceedings{Sainath2013,
abstract = {Deep Convolutional Neural Networks (CNNs) are more powerful than Deep Neural Networks (DNN), as they are able to better reduce spectral variation in the input signal. This has also been confirmed experimentally, with CNNs showing improvements in word error rate (WER) between 4-12{\%} relative compared to DNNs across a variety of LVCSR tasks. In this paper, we describe different methods to further improve CNN performance. First, we conduct a deep analysis comparing limited weight sharing and full weight sharing with state-of-the-art features. Second, we apply various pooling strategies that have shown improvements in computer vision to an LVCSR speech task. Third, we introduce a method to effectively incorporate speaker adaptation, namely fMLLR, into log-mel features. Fourth, we introduce an effective strategy to use dropout during Hessian-free sequence training. We find that with these improvements, particularly with fMLLR and dropout, we are able to achieve an additional 2-3{\%} relative improvement in WER on a 50-hour Broadcast News task over our previous best CNN baseline. On a larger 400-hour BN task, we find an additional 4-5{\%} relative improvement over our previous best CNN baseline.},
annote = {
introduces dropout regularization
},
archivePrefix = {arXiv},
arxivId = {1309.1501},
author = {Sainath, Tara N. and Kingsbury, Brian and Mohamed, Abdel Rahman and Dahl, George E. and Saon, George and Soltau, Hagen and Beran, Tomas and Aravkin, Aleksandr Y. and Ramabhadran, Bhuvana},
booktitle = {2013 IEEE Workshop on Automatic Speech Recognition and Understanding, ASRU 2013 - Proceedings},
doi = {10.1109/ASRU.2013.6707749},
eprint = {1309.1501},
isbn = {9781479927562},
issn = {15206149},
pages = {315--320},
pmid = {10463930},
title = {{Improvements to deep convolutional neural networks for LVCSR}},
year = {2013}
}
@inproceedings{Zhang2006,
abstract = {Recently, methods based on local image features have shown promise for texture and object recognition tasks. This paper presents a large-scale evaluation of an approach that represents images as distributions (signatures or histograms) of features extracted from a sparse set of keypoint locations and learns a Support Vector Machine classifier with kernels based on two effective measures for comparing distributions, the Earth Mover's Distance and the ÷2 distance. We first evaluate the performance of our approach with different keypoint detectors and descriptors, as well as different kernels and classifiers. We then conduct a comparative evaluation with several state-of-the-art recognition methods on 4 texture and 5 object databases. On most of these databases, our implementation exceeds the best reported results and achieves comparable performance on the rest. Finally, we investigate the influence of background correlations on recognition performance.},
annote = {
Introduces Bag-of-features
},
author = {Schmid, Cordelia},
booktitle = {Pattern Recognition},
doi = {10.1007/s11263-006-9794-4},
isbn = {0-7695-2646-2},
keywords = {Databases,Detectors,Earth,Feature extraction,Folder - TOR by BOF,Histograms,Kernel,Large-scale systems,Support vector machine classification,object recognition,support vector machines},
mendeley-tags = {Databases,Detectors,Earth,Feature extraction,Folder - TOR by BOF,Histograms,Kernel,Large-scale systems,Support vector machine classification,object recognition,support vector machines},
pages = {0--7},
shorttitle = {Local Features and Kernels for Classification of T},
title = {{Local Features and Kernels for Classi cation of Texture and Object Categories: A Comprehensive Study}},
url = {http://ieeexplore.ieee.org/ielx5/10922/34371/01640452.pdf?tp={\&}arnumber=1640452{\&}isnumber=34371 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1640452},
year = {2006}
}
@article{Schmidhuber2015,
abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning {\&} evolutionary computation, and indirect search for short programs encoding deep and large networks.},
annote = {Comment: 88 pages, 888 references},
archivePrefix = {arXiv},
arxivId = {1404.7828},
author = {Schmidhuber, J{\"{u}}rgen},
doi = {10.1016/j.neunet.2014.09.003},
eprint = {1404.7828},
file = {:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schmidhuber - 2015 - Deep Learning in neural networks An overview.pdf:pdf;:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schmidhuber - 2015 - Deep Learning in neural networks An overview.html:html},
isbn = {0893-6080},
issn = {18792782},
journal = {Neural Networks},
keywords = {Computer Science - Learning,Computer Science - Neural and Evolutionary Computi,Deep learning,Evolutionary computation,Folder - Deep learning,Reinforcement learning,Supervised learning,Unsupervised learning},
mendeley-tags = {Computer Science - Learning,Computer Science - Neural and Evolutionary Computi,Folder - Deep learning},
month = {jan},
pages = {85--117},
shorttitle = {Deep Learning in Neural Networks},
title = {{Deep Learning in neural networks: An overview}},
url = {http://arxiv.org/abs/1404.7828 http://www.arxiv.org/pdf/1404.7828.pdf},
volume = {61},
year = {2015}
}
@inproceedings{Schmitz2014a,
abstract = {Recognizing grasped objects with tactile sensors is beneficial in many situations, as other sensor information like vision is not always reliable. In this paper, we aim for multimodal object recognition by power grasping of objects with an unknown orientation and position relation to the hand. Few robots have the necessary tactile sensors to reliably recognize objects: in this study the multifingered hand of TWENDY-ONE is used, which has distributed skin sensors covering most of the hand, 6 axis F/T sensors in each fingertip, and provides information about the joint angles. Moreover, the hand is compliant. When using tactile sensors, it is not clear what kinds of features are useful for object recognition. Recently, deep learning has shown promising results. Nevertheless, deep learning has rarely been used in robotics and to our best knowledge never for tactile sensing, probably because it is difficult to gather many samples with tactile sensors. Our results show a clear improvement when using a denoising autoencoder with dropout compared to traditional neural networks. Nevertheless, a higher number of layers did not prove to be beneficial.},
author = {Schmitz, Alexander and Bansho, Yusuke and Noda, Kuniaki and Iwata, Hiroyasu and Ogata, Tetsuya and Sugano, Shigeki},
booktitle = {2014 IEEE-RAS International Conference on Humanoid Robots},
doi = {10.1109/HUMANOIDS.2014.7041493},
isbn = {978-1-4799-7174-9},
issn = {21640580},
keywords = {Dexterous manipulators,F/t sensors,Power grasping,Principal component analysis,TWENDY-ONE,Tactile sensors,Training,control engineering computing,deep learning,denoising autoencoder,distributed sensors,distributed skin sensors,dropout,grasped objects recognition,joint angles,learning (artificial intelligence),multifingered hand,multimodal object recognition,object recognition,position relation,skin,tactile object recognition,unknown orientation},
pages = {1044--1050},
title = {{Tactile object recognition using deep learning and dropout}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7041493},
year = {2014}
}
@inproceedings{Schmitz2014,
abstract = {Recognizing grasped objects with tactile sensors is beneficial in many situations, as other sensor information like vision is not always reliable. In this paper, we aim for multimodal object recognition by power grasping of objects with an unknown orientation and position relation to the hand. Few robots have the necessary tactile sensors to reliably recognize objects: in this study the multifingered hand of TWENDY-ONE is used, which has distributed skin sensors covering most of the hand, 6 axis F/T sensors in each fingertip, and provides information about the joint angles. Moreover, the hand is compliant. When using tactile sensors, it is not clear what kinds of features are useful for object recognition. Recently, deep learning has shown promising results. Nevertheless, deep learning has rarely been used in robotics and to our best knowledge never for tactile sensing, probably because it is difficult to gather many samples with tactile sensors. Our results show a clear improvement when using a denoising autoencoder with dropout compared to traditional neural networks. Nevertheless, a higher number of layers did not prove to be beneficial.},
annote = {Use a twendy-one hand with over 300 sensors to recognise objects from grasp information, using deep learning, DAE pretraining with dropout.},
author = {Schmitz, Alexander and Bansho, Yusuke and Noda, Kuniaki and Iwata, Hiroyasu and Ogata, Tetsuya and Sugano, Shigeki},
booktitle = {2014 IEEE-RAS International Conference on Humanoid Robots},
doi = {10.1109/HUMANOIDS.2014.7041493},
file = {:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schmitz et al. - 2014 - Tactile object recognition using deep learning and dropout.pdf:pdf;:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schmitz et al. - 2014 - Tactile object recognition using deep learning and dropout.html:html},
isbn = {978-1-4799-7174-9},
issn = {21640580},
keywords = {Dexterous manipulators,F/T sensors,F/t sensors,Folder - from grasp,Power grasping,Principal component analysis,Skin,TWENDY-ONE,Tactile sensors,Training,control engineering computing,deep learning,denoising autoencoder,dexterous manipulators,distributed sensors,distributed skin sensors,dropout,grasped objects recognition,joint angles,learning (artificial intelligence),multifingered hand,multimodal object recognition,object recognition,position relation,power grasping,skin,tactile object recognition,tactile sensors,unknown orientation},
mendeley-tags = {F/T sensors,Folder - from grasp,Principal component analysis,Skin,TWENDY-ONE,Training,control engineering computing,deep learning,denoising autoencoder,dexterous manipulators,distributed sensors,distributed skin sensors,dropout,grasped objects recognition,joint angles,learning (artificial intelligence),multifingered hand,multimodal object recognition,object recognition,position relation,power grasping,tactile object recognition,tactile sensors,unknown orientation},
month = {nov},
pages = {1044--1050},
title = {{Tactile object recognition using deep learning and dropout}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7041493},
year = {2014}
}
@inproceedings{Schmitz2014b,
abstract = {Recognizing grasped objects with tactile sensors is beneficial in many situations, as other sensor information like vision is not always reliable. In this paper, we aim for multimodal object recognition by power grasping of objects with an unknown orientation and position relation to the hand. Few robots have the necessary tactile sensors to reliably recognize objects: in this study the multifingered hand of TWENDY-ONE is used, which has distributed skin sensors covering most of the hand, 6 axis F/T sensors in each fingertip, and provides information about the joint angles. Moreover, the hand is compliant. When using tactile sensors, it is not clear what kinds of features are useful for object recognition. Recently, deep learning has shown promising results. Nevertheless, deep learning has rarely been used in robotics and to our best knowledge never for tactile sensing, probably because it is difficult to gather many samples with tactile sensors. Our results show a clear improvement when using a denoising autoencoder with dropout compared to traditional neural networks. Nevertheless, a higher number of layers did not prove to be beneficial.},
annote = {Achieves 88{\%} accuracy over 20 objects (including difficult objects such as similar bottles) using a fusion of propioception and many tactile inputs distributed over a robotic hand which grasps the object.},
author = {Schmitz, Alexander and Bansho, Yusuke and Noda, Kuniaki and Iwata, Hiroyasu and Ogata, Tetsuya and Sugano, Shigeki},
booktitle = {2014 IEEE-RAS International Conference on Humanoid Robots},
doi = {10.1109/HUMANOIDS.2014.7041493},
isbn = {978-1-4799-7174-9},
issn = {21640580},
keywords = {Dexterous manipulators,F/t sensors,Power grasping,Principal component analysis,TWENDY-ONE,Tactile sensors,Training,control engineering computing,deep learning,denoising autoencoder,distributed sensors,distributed skin sensors,dropout,grasped objects recognition,joint angles,learning (artificial intelligence),multifingered hand,multimodal object recognition,object recognition,position relation,skin,tactile object recognition,unknown orientation},
pages = {1044--1050},
title = {{Tactile object recognition using deep learning and dropout}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7041493},
year = {2014}
}
@inproceedings{Schneider2009,
abstract = {In this paper, we present a novel approach for identifying objects using touch sensors installed in the finger tips of a manipulation robot. Our approach operates on low-resolution intensity images that are obtained when the robot grasps an object. We apply a bag-of-words approach for object identification. By means of unsupervised clustering on training data, our approach learns a vocabulary from tactile observations which is used to generate a histogram codebook. The histogram codebook models distributions over the vocabulary and is the core identification mechanism. As the objects are larger than the sensor, the robot typically needs multiple grasp actions at different positions to uniquely identify an object. To reduce the number of required grasp actions, we apply a decision-theoretic framework that minimizes the entropy of the probabilistic belief about the type of the object. In our experiments carried out with various industrial and household objects, we demonstrate that our approach is able to discriminate between a large set of objects. We furthermore show that using our approach, a robot is able to distinguish visually similar objects that have different elasticity properties by using only the information from the touch sensor. ER -},
annote = {
two fingered grasps to apply a bag-of-features approach to recognition of household and industrial objects. The pose of the object is known and fixed, and the number of words is chosen manually as part of the optimisation. They provide an interesting information theoretic approach for maximum expected information gain to inform grasping position. Using MAP they obtain an accuracy of 84.6{\%} in recognition.
},
author = {Schneider, A and Sturm, J and Stachniss, C and Reisert, M and Burkhardt, H and Burgard, W},
booktitle = {Intelligent Robots and Systems, 2009. IROS 2009. IEEE/RSJ International Conference on DOI - 10.1109/IROS.2009.5354648},
doi = {10.1109/IROS.2009.5354648},
isbn = {978-1-4244-3803-7},
keywords = {Fingers,Histograms,Intelligent robots,Robot sensing systems,Robustness,Sensor arrays,Service robots,Skin,Vocabulary,bag-of-features,bag-of-words approach,decision theory,decision-theoretic framework,histogram codebook,manipulation robot,manipulators,object identification,probabilistic belief,probability,tactile sensor,tactile sensors,touch sensor,unsupervised clustering,unsupervised clustering VL -,unsupervised learning},
mendeley-tags = {Fingers,Histograms,Intelligent robots,Robot sensing systems,Robustness,Sensor arrays,Service robots,Skin,Vocabulary,bag-of-features,bag-of-words approach,decision theory,decision-theoretic framework,histogram codebook,manipulation robot,manipulators,object identification,probabilistic belief,probability,tactile sensor,tactile sensors,touch sensor,unsupervised clustering,unsupervised learning},
pages = {243--248},
pmid = {5354648},
title = {{Object identification with tactile sensors using bag-of-features}},
url = {10.1109/IROS.2009.5354648},
year = {2009}
}
@book{Leen2001,
abstract = {The annual Neural Information Processing Systems (NIPS) conference is the flagship meeting on neural computation and machine learning. It draws a diverse group of attendees—physicists, neuroscientists, mathematicians, statisticians, and computer scientists—interested in theoretical and applied aspects of modeling, simulating, and building neural-like or intelligent systems. The presentations are interdisciplinary, with contributions in algorithms, learning theory, cognitive science, neuroscience, brain imaging, vision, speech and signal processing, reinforcement learning, and applications. Only twenty-five percent of the papers submitted are accepted for presentation at NIPS, so the quality is exceptionally high. This volume contains the papers presented at the December 2006 meeting, held in Vancouver.},
author = {Sch{\"{o}}lkopf, Bernhard and Platt, John and Hofmann, Thomas},
isbn = {0262195682},
keywords = {Computers / Neural Networks},
language = {en},
mendeley-tags = {Computers / Neural Networks},
pages = {1690},
publisher = {MIT Press},
shorttitle = {Advances in Neural Information Processing Systems },
title = {{Advances in Neural Information Processing Systems 19: Proceedings of the ...}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=Tbn1l9P1220C{\&}pgis=1},
year = {2007}
}
@inproceedings{Schopfer2007,
abstract = {We present a database of 2D pressure profile time series as a testbed for tactile object and surface recognition. The tactile database captures the surfaces of household and toy objects by moving a 2D pressure sensor mounted to an industrial robot arm around the objects using real-time trajectory calculation. Thus, it represents different "views" of the objects in a similar way as the well known Columbia Object Image Library (COIL) captures different views of an object by a camera. As a first application, objects in the database are classified using a neural network architecture.},
annote = {
Created a haptic database by sensing the contour of small objects, and showed a preliminar approach at classification using PCA (via NNs) and supervised neural nets on the features.

----------

Obtains good results on classification, even robust to small translations of the object. However it is not resilient to changes in the angle of the object pose, mostly due to the sensor type.

----------

uses AEV (activity equalization VQ) (
},
author = {Sch{\"{o}}pfer, Matthias and Ritter, Helge and Heidemann, Gunther},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ROBOT.2007.363539},
isbn = {1424406021},
issn = {10504729},
keywords = {2D pressure profile time series,2D pressure sensor,Cameras,Image databases,Libraries,Neural networks,Pattern recognition,Robot sensing systems,Robot tactile systems,Robot vision systems,Service robots,Tactile systems (non-biological),Tactile systems (nonbiological),Testing,Toy industry,camera,image database,image recognition,industrial robot arm,manipulators,neural nets,neural network architecture,pattern recognition,robot tactile systems,surface recognition,tactile database,tactile object,tactile sensors,time series,trajectory calculation,visual databases},
mendeley-tags = {2D pressure profile time series,2D pressure sensor,Cameras,Image databases,Libraries,Neural networks,Robot sensing systems,Robot vision systems,Service robots,Tactile systems (non-biological),Testing,Toy industry,camera,image database,image recognition,industrial robot arm,manipulators,neural nets,neural network architecture,pattern recognition,robot tactile systems,surface recognition,tactile database,tactile object,tactile sensors,time series,trajectory calculation,visual databases},
pages = {1517--1522},
title = {{Acquisition and application of a tactile database}},
url = {http://ieeexplore.ieee.org/ielx5/4209048/4209049/04209303.pdf?tp={\&}arnumber=4209303{\&}isnumber=4209049 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4209303},
year = {2007}
}
@article{Simon2015,
abstract = {Part models of object categories are essential for chal- lenging recognition tasks, where differences in categories are subtle and only reflected in appearances of small parts of the object. We present an approach that is able to learn part models in a completely unsupervised manner, without part annotations and even without given bound- ing boxes during learning. The key idea is to find constel- lations of neural activation patterns computed using con- volutional neural networks. In our experiments, we out- perform existing approaches for fine-grained recognition on the CUB200-2011, Oxford PETS, and Oxford Flowers dataset in case no part or bounding box annotations are available and achieve state-of-the-art performance for the Stanford Dog dataset. We also show the benefits of neu- ral constellation models as a data augmentation technique for fine-tuning. Furthermore, our paper unites the areas of generic and fine-grained classification, since our approach is suitable for both scenarios. The source code of our method is available online at http://www.inf-cv. uni-jena.de/part{\_}discovery. 1.},
archivePrefix = {arXiv},
arxivId = {1504.0828},
author = {Simon, Marcel and Rodner, Erik},
doi = {10.3150/13-BEJ559},
eprint = {1504.0828},
file = {:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Simon, Rodner - 2015 - Neural Activation Constellations Unsupervised Part Model Discovery with Convolutional Networks.pdf:pdf;:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Simon, Rodner - 2015 - Neural Activation Constellations Unsupervised Part Model Discovery with Convolutional Networks.html:html},
issn = {1350-7265},
journal = {Bernoulli},
keywords = {Computer Science - Computer Vision and Pattern Rec,Folder - using CNN},
mendeley-tags = {Computer Science - Computer Vision and Pattern Rec,Folder - using CNN},
month = {apr},
number = {1},
pages = {1--37},
shorttitle = {Neural Activation Constellations},
title = {{Neural Activation Constellations: Unsupervised Part Model Discovery with Convolutional Networks}},
url = {arXiv:1504.08289v1$\backslash$nhttp://projecteuclid.org/euclid.bj/1426597062},
volume = {21},
year = {2015}
}
@incollection{Simon2014,
abstract = {Current fine-grained classification approaches often rely on a robust localization of object parts to extract localized feature representations suitable for discrimination. However, part localization is a challenging task due to the large variation of appearance and pose. In this paper, we show how pre-trained convolutional neural networks can be used for robust and efficient object part discovery and localization without the necessity to actually train the network on the current dataset. Our approach called "part detector discovery" (PDD) is based on analyzing the gradient maps of the network outputs and finding activation centers spatially related to annotated semantic parts or bounding boxes. This allows us not just to obtain excellent performance on the CUB200-2011 dataset, but in contrast to previous approaches also to perform detection and bird classification jointly without requiring a given bounding box annotation during testing and ground-truth parts during training. The code is available at http://www.inf-cv.uni-jena.de/part{\_}discovery and https://github.com/cvjena/PartDetectorDisovery.},
annote = {
Calculate the derivative of a high level activation layer with respect to the input pixels. High values determine pixels which are influential for a particular channel. These can be used as part localisation proposals.
},
archivePrefix = {arXiv},
arxivId = {1411.3159},
author = {Simon, Marcel and Rodner, Erik and Denzler, Joachim},
booktitle = {Accv2014},
doi = {10.1007/978-3-319-16808-1},
editor = {Cremers, Daniel and Reid, Ian and Saito, Hideo and Yang, Ming-Hsuan},
eprint = {1411.3159},
file = {:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Simon, Rodner, Denzler - 2014 - Part Detector Discovery in Deep Convolutional Neural Networks.pdf:pdf;:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Simon, Rodner, Denzler - 2014 - Part Detector Discovery in Deep Convolutional Neural Networks.html:html},
isbn = {9783319168081},
issn = {10459227},
keywords = {Artificial Intelligence (incl. Robotics),Folder - convolutional - part detection,Health Informatics,Image Processing and Computer Vision,Information Storage and Retrieval,Information Systems Applications (incl. Internet),pattern recognition},
language = {en},
mendeley-tags = {Artificial Intelligence (incl. Robotics),Folder - convolutional - part detection,Health Informatics,Image Processing and Computer Vision,Information Storage and Retrieval,Information Systems Applications (incl. Internet),pattern recognition},
month = {nov},
pages = {162--177},
publisher = {Springer International Publishing},
series = {Lecture Notes in Computer Science},
title = {{Part Detector Discovery in Deep Convolutional Neural Networks}},
url = {http://link.springer.com/chapter/10.1007/978-3-319-16808-1{\_}12 http://link.springer.com/content/pdf/10.1007{\%}2F978-3-319-16808-1{\_}12.pdf},
volume = {9004},
year = {2014}
}
@article{Sinapov2011,
abstract = {This paper proposes a method for interactive surface recognition and surface categorization by a humanoid robot using a vibrotactile sensory modality. The robot was equipped with an artificial fingernail that had a built-in three-axis accelerometer. The robot interacted with 20 different surfaces by performing five different exploratory scratching behaviors on them. Surface-recognition models were learned by coupling frequency-domain analysis of the vibrations detected by the accelerometer with machine learning algorithms, such as support vector machine (SVM) and k-nearest neighbors (k-NN). The results show that by applying several different scratching behaviors on a test surface, the robot can recognize surfaces better than with any single behavior alone. The robot was also able to estimate a measure of similarity between any two surfaces, which was used to construct a grounded hierarchical surface categorization. {\textcopyright} 2011 IEEE.},
annote = {
Uses frequency-domain analysis on vibration information from scratching. Feed feature vector to a SVM or k-NN to recognise and classify surfaces based on a range of Exploratory Behaviours. Estimates a hierarchical structure for the surfaces.
},
author = {Sinapov, Jivko and Sukhoy, Vladimir and Sahai, Ritika and Stoytchev, Alexander},
doi = {10.1109/TRO.2011.2127130},
isbn = {1552-3098},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Behavior-based systems,Feature extraction,Frequency-domain analysis,Robot sensing systems,accelerometers,built-in three-axis accelerometer,force and tactile sensing,humanoid robot,humanoid robots,interactive surface recognition,learning and adaptive systems,machine learning algorithms,recognition,support vector machines,surface categorization,tactile sensors,vibration control,vibrotactile recognition,vibrotactile sensory modality},
mendeley-tags = {Behavior-based systems,Feature extraction,Frequency-domain analysis,Robot sensing systems,accelerometers,built-in three-axis accelerometer,force and tactile sensing,humanoid robot,humanoid robots,interactive surface recognition,learning and adaptive systems,machine learning algorithms,recognition,support vector machines,surface categorization,tactile sensors,vibration control,vibrotactile recognition,vibrotactile sensory modality},
number = {3},
pages = {488--497},
title = {{Vibrotactile recognition and categorization of surfaces by a humanoid robot}},
url = {http://ieeexplore.ieee.org/ielx5/8860/5784192/05752872.pdf?tp={\&}arnumber=5752872{\&}isnumber=5784192 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5752872},
volume = {27},
year = {2011}
}
@inproceedings{Soh2012,
abstract = {In this work, we are primarily concerned with robotic systems that learn online and continuously from multi-variate data-streams. Our first contribution is a new recursive kernel, which we have integrated into a sparse Gaussian Process to yield the Spatio-Temporal Online Recursive Kernel Gaussian Process (STORK-GP). This algorithm iteratively learns from time-series, providing both predictions and uncertainty estimates. Experiments on benchmarks demonstrate that our method achieves high accuracies relative to state-of-the-art methods. Second, we contribute an online tactile classifier which uses an array of STORK-GP experts. In contrast to existing work, our classifier is capable of learning new objects as they are presented, improving itself over time. We show that our approach yields results comparable to highly-optimised offline classification methods. Moreover, we conducted experiments with human subjects in a similar online setting with true-label feedback and present the insights gained.},
annote = {
Online fast and accuracte learning of object models using tactile sensors from an iCub hand, using STORK-GP (Spatio-Temporal Online Recursive Kernel - GP). Propose active exploration.
},
author = {Soh, Harold and Su, Yanyu and Demiris, Yiannis},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2012.6385992},
isbn = {9781467317375},
issn = {21530858},
keywords = {Benchmark testing,Gaussian processes,Kernel,Robot sensing systems,STORK-GP experts,Standards,dexterous manipulators,haptic interfaces,humanoid robots,iterative learning,learning (artificial intelligence),multivariate data streams,online learning,online tactile classifier,pattern classification,predictions,recursive estimation,robotic systems,sparse Gaussian Process,spatiotemporal online recursive kernel Gaussian pr,spatiotemporal phenomena,tactile classification,time series,true-label feedback,uncertain systems,uncertainty estimates},
mendeley-tags = {Benchmark testing,Gaussian processes,Kernel,Robot sensing systems,STORK-GP experts,Standards,dexterous manipulators,haptic interfaces,humanoid robots,iterative learning,learning (artificial intelligence),multivariate data streams,online learning,online tactile classifier,pattern classification,predictions,recursive estimation,robotic systems,sparse Gaussian Process,spatiotemporal online recursive kernel Gaussian pr,spatiotemporal phenomena,tactile classification,time series,true-label feedback,uncertain systems,uncertainty estimates},
pages = {4489--4496},
title = {{Online spatio-temporal Gaussian process experts with application to tactile classification}},
url = {http://ieeexplore.ieee.org/ielx5/6363628/6385431/06385992.pdf?tp={\&}arnumber=6385992{\&}isnumber=6385431 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6385992{\&}tag=1},
year = {2012}
}
@incollection{Sole-Ribalta2009,
abstract = {This article presents a structural and probabilistic framework for representing a class of attributed graphs with only one structure. The aim of this article is to define a new model, called Structurally-Defined Random Graphs. This structure keeps together statistical and structural information to increase the capacity of the model to discern between attributed graphs within or outside the class. Moreover, we define the match probability of an attributed graph respect to our model that can be used as a dissimilarity measure. Our model has the advantage that does not incorporate application dependent parameters such as edition costs. The experimental validation on a TC-15 database shows that our model obtains higher recognition results, when there is moderate variability of the class elements, than several structural matching algorithms. Indeed in our model fewer comparisons are needed. {\textcopyright} 2009 Springer Berlin Heidelberg.},
author = {Sol{\'{e}}-Ribalta, Albert and Serratosa, Francesc},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-02124-4_17},
editor = {Torsello, Andrea and Escolano, Francisco and Brun, Luc},
file = {:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sol{\'{e}}-Ribalta, Serratosa - 2009 - A structural and semantic probabilistic model for matching and representing a set of graphs.html:html},
isbn = {3642021239},
issn = {03029743},
keywords = {Artificial Intelligence (incl. Robotics),Computer Graphics,Computer Imaging- Vision- Pattern Recognition and,Discrete Mathematics in Computer Science,Folder - attributed graphs,Graph Matching,Graph clustering,Graph matching,Graph synthesis,Image Processing and Computer Vision,Probabilistic model,Semantic relations,Structural relations,graph clustering,graph synthesis,pattern recognition,probabilistic model,semantic relations,structural relations},
language = {en},
mendeley-tags = {Artificial Intelligence (incl. Robotics),Computer Graphics,Computer Imaging- Vision- Pattern Recognition and,Discrete Mathematics in Computer Science,Folder - attributed graphs,Graph Matching,Image Processing and Computer Vision,graph clustering,graph synthesis,pattern recognition,probabilistic model,semantic relations,structural relations},
pages = {164--173},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{A structural and semantic probabilistic model for matching and representing a set of graphs}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-02124-4{\_}17 http://link.springer.com/content/pdf/10.1007{\%}2F978-3-642-02124-4{\_}17.pdf},
volume = {5534 LNCS},
year = {2009}
}
@incollection{NIPS2012_4683,
annote = {Uses a stack of binary, replicated softmax (text) and gaussian (image) RBMs to jointly model images and their tags. A simple logistic regression is performed on the top (joint) layer for classification. It is shown to be superior to simple approaches such as SVM or LDA on direct concatenation of features, as well as more complex models such as a DBN and an autoencoder.
There is a marked benefit in being able to use unlabeled data due to the unsupervised nature of RBMs.
Also, because the joint probabililty space is modelled, absent modalities can be "imagined" by drawing from the conditional distribution using a sampler.},
author = {Srivastava, Nitish and Salakhutdinov, Ruslan R},
booktitle = {Advances in Neural Information Processing Systems 25},
editor = {Pereira, F and Burges, C J C and Bottou, L and Weinberger, K Q},
file = {:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Irie et al. - 2013 - A bayesian approach to multimodal visual dictionary learning.pdf:pdf},
pages = {2222--2230},
publisher = {Curran Associates, Inc.},
title = {{Multimodal Learning with Deep Boltzmann Machines}},
url = {http://papers.nips.cc/paper/4683-multimodal-learning-with-deep-boltzmann-machines.pdf},
year = {2012}
}
@article{Stansfield1992,
abstract = {In this paper we present a series of haptic exploritory procedures, or EPs, implemented for a multi-fingered, articulated, sensate robot hand. These EPs are designed to extract specifice tactile and kinesthetic information from and object via their purposive invocation by an intellegent robotic system. Taken together, they form an active robotic touch perception system to be used both in extracting information about the environment for internal representation and in acquiring grasps for manipulation. The haptic system presented utilizes an integrated robotic system consisting of a PUMA 560 robot arm, a JPL/Stanford robot hand, with joint torque sensing in the fingers, a wrist force/torque sensor, and a 256 element, spatially-resolved fingertip tactile array, We describe the EPs implemented for this system and provide experimental results which illustrate how they function and how the information with they extract may be used. In addition to the sensate hand and arm, the robot also contains structured-lighting vision and a Prolog-based reasoning system capable of grasp generation and object categorization. We present a set of simple tasks which show how both grasping and recognition may be enhanced by the addition of active touch perception.},
annote = {
JPL/Stanford robot hand.

----------

Use predefined exploratory procedures (EPs) to sense tactile features: weight, hardness (binary hard/soft), texture, temperature, edges, local shape, size, solidity.

----------

Object representation via rules.
},
author = {Stansfield, Sharon a.},
doi = {10.1017/S0263574700005828},
isbn = {1111111111},
issn = {0263-5747},
journal = {Robotica},
keywords = {Haptic procedure,Intelligent robot,Robot hand,Touch system,haptic procedure,intelligent,robot hand},
mendeley-tags = {Haptic procedure,Intelligent robot,Robot hand,Touch system},
number = {06},
pages = {497},
title = {{Haptic Perception with an Articulated, Sensate Robot Hand}},
url = {http://journals.cambridge.org/action/displayFulltext?type=1{\&}fid=4448784{\&}jid=ROB{\&}volumeId=10{\&}issueId=06{\&}aid=4448776 http://journals.cambridge.org/action/displayFulltext?type=1{\&}fid=4448784{\&}jid=ROB{\&}volumeId=10{\&}issueId=06{\&}aid=4448776{\&}toPdf=true},
volume = {10},
year = {1992}
}
@inproceedings{Steder2011,
abstract = {In this paper we address the topic of feature extraction in 3D point cloud data for object recognition and pose identification. We present a novel interest keypoint extraction method that operates on range images generated from arbitrary 3D point clouds, which explicitly considers the borders of the objects identified by transitions from foreground to background. We furthermore present a feature descriptor that takes the same information into account. We have implemented our approach and present rigorous experiments in which we analyze the individual components with respect to their repeatability and matching capabilities and evaluate the usefulness for point feature based object detection methods.},
annote = {
NARF (Normal Aligned Radial Feature) uses edge information (discriminating between object, shadow and veil edges) and 3D distance between neighbouring points to create a robust feature which holds information about the variability of stable surface patches.


More robust than spin images},
author = {Steder, Bastian and Rusu, Radu Bogdan and Konolige, Kurt and Burgard, Wolfram},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2011.5980187},
isbn = {9781612843865},
issn = {10504729},
keywords = {3D point cloud data,3D range scans,Data mining,Estimation,Feature extraction,Noise,Robustness,Three dimensional displays,feature descriptor,matching capabilities,object boundaries,object detection,object recognition,point feature based object detection methods,point feature extraction,pose estimation,pose identification,range images,repeatability capabilities},
mendeley-tags = {3D point cloud data,3D range scans,Data mining,Estimation,Feature extraction,Noise,Robustness,Three dimensional displays,feature descriptor,matching capabilities,object boundaries,object detection,object recognition,point feature based object detection methods,point feature extraction,pose estimation,pose identification,range images,repeatability capabilities},
pages = {2601--2608},
title = {{Point feature extraction on 3D range scans taking into account object boundaries}},
url = {http://ieeexplore.ieee.org/ielx5/5967842/5979525/05980187.pdf?tp={\&}arnumber=5980187{\&}isnumber=5979525 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5980187},
year = {2011}
}
@article{Steinwart2003,
abstract = {Support vector machines (SVMs) construct decision functions that are linear combinations of kernel evaluations on the training set. The samples with non-vanishing coefficients are called support vectors. In this work we establish lower (asymptotical) bounds on the number of support vectors. On our way we prove several results which are of great importance for the understanding of SVMs. In particular, we describe to which "limit" SVM decision functions tend, discuss the corresponding notion of convergence and provide some results on the stability of SVMs using subdifferential calculus in the associated reproducing kernel Hilbert space.},
annote = {
Number of support vectors grows linearly with the number of training samples.
},
author = {Steinwart, Ingo},
file = {:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Steinwart - 2003 - {\{}S{\}}parseness of {\{}S{\}}upport {\{}V{\}}ector {\{}M{\}}achines.pdf:pdf},
issn = {1532-4435},
journal = {{\{}J{\}}ournal of {\{}M{\}}achine {\{}L{\}}earning {\{}R{\}}esearch (JMLR)},
keywords = {kernel,sparsity},
month = {dec},
pages = {1071--1105},
title = {{{\{}S{\}}parseness of {\{}S{\}}upport {\{}V{\}}ector {\{}M{\}}achines}},
url = {http://dl.acm.org/citation.cfm?id=945365.964289 http://dl.acm.org/ft{\_}gateway.cfm?id=964289{\&}type=pdf},
volume = {4},
year = {2003}
}
@inproceedings{Sturm2010,
abstract = {Many tasks that would be of benefit to users in domestic environments require that robots manipulate articulated objects such as doors and drawers. In this paper, we present a novel approach that simultaneously estimates the kinematic model of an articulated object based on the trajectory described by the robot's end effector, and uses this model to predict the future trajectory of the end effector. One advantage of our approach is that the robot can directly use these predictions to generate an equilibrium point control path for operating the mechanism. Additionally, our approach can improve these predictions based on previously learned articulation models. We have implemented and tested our approach on a real mobile manipulator. Through 40 trials, we show that the robot can reliably open various household objects, including cabinet doors, sliding doors, office drawers, and a dishwasher. Furthermore, we demonstrate that using the information from previous interactions as a prior significantly improves the prediction accuracy.},
annote = {
Learns relationship between multiple rigid parts of an articulated object by maximising the likelihood of a tree which represents the connection types between parts, given observations of all the parts in many valid poses. Uses GPs for relationships that are not simple (revolute, linear, rigid, pivot)

----------

Available as ros package: http://wiki.ros.org/articulation

----------

robot opeerates a few domestic articulated objects (dishwasher, drawer, etc)
},
author = {Sturm, J. and Jain, a. and Stachniss, C. and Kemp, C.C. and Burgard, W.},
booktitle = {Intelligent Robots and Systems (IROS), 2010 IEEE/RSJ International Conference on},
doi = {10.1109/IROS.2010.5653813},
isbn = {978-1-4244-6674-0},
issn = {2153-0858},
keywords = {articulated object,articulation model,cabinet door,dishwasher,end effectors,equilibrium point control path,kinematic model,manipulator kinematics,mobile manipulator,office drawer,path planning,prediction accuracy,sliding door},
mendeley-tags = {articulated object,articulation model,cabinet door,dishwasher,end effectors,equilibrium point control path,kinematic model,manipulator kinematics,mobile manipulator,office drawer,path planning,prediction accuracy,sliding door},
month = {oct},
pages = {2739--2744},
title = {{Operating articulated objects based on experience}},
url = {http://ieeexplore.ieee.org/ielx5/5639431/5648787/05653813.pdf?tp={\&}arnumber=5653813{\&}isnumber=5648787 http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5653813{\&}navigation=1},
year = {2010}
}
@inproceedings{Sturm2010a,
abstract = {Service robots deployed in domestic environments generally need the capability to deal with articulated objects such as doors and drawers in order to fulfill certain mobile manipulation tasks. This however, requires, that the robots are able to perceive the articulation models of such objects. In this paper, we present an approach for detecting, tracking, and learning articulation models for cabinet doors and drawers without using artificial markers. Our approach uses a highly efficient and sampling-based approach to rectangle detection in depth images obtained from a self-developed active stereo system. The robot can use the generative models learned for the articulated objects to estimate their articulation type, their current configuration, and to make predictions about possible configurations not observed before. We present experiments carried out on real data obtained from our active stereo system. The results demonstrate that our technique is able to learn accurate articulation models. We furthermore provide a detailed error analysis based on ground truth data obtained in a motion capturing studio.},
author = {Sturm, J{\"{u}}rgen and Konolige, Kurt and Stachniss, Cyrill and Burgard, Wolfram},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ROBOT.2010.5509985},
isbn = {9781424450381},
issn = {10504729},
keywords = {Cameras,Image segmentation,Iterative algorithms,Predictive models,Robot vision systems,Robotics and automation,Service robots,USA Councils,articulation models,artificial markers,cabinet doors,drawers,error analysis,ground truth data,home automation,household environments,mobile manipulation tasks,motion capturing studio,object detection,rectangle detection,robot vision,self developed active stereo system,stereo image processing,vision based detection},
mendeley-tags = {Cameras,Image segmentation,Iterative algorithms,Predictive models,Robot vision systems,Robotics and automation,Service robots,USA Councils,articulation models,artificial markers,cabinet doors,drawers,error analysis,ground truth data,home automation,household environments,mobile manipulation tasks,motion capturing studio,object detection,rectangle detection,robot vision,self developed active stereo system,stereo image processing,vision based detection},
month = {may},
pages = {362--368},
title = {{Vision-based detection for learning articulation models of cabinet doors and drawers in household environments}},
url = {http://ieeexplore.ieee.org/xpls/icp.jsp?arnumber=5509985},
year = {2010}
}
@inproceedings{Sturm2010b,
abstract = {Service robots deployed in domestic environments generally need the capability to deal with articulated objects such as doors and drawers in order to fulfill certain mobile manipulation tasks. This however, requires, that the robots are able to perceive the articulation models of such objects. In this paper, we present an approach for detecting, tracking, and learning articulation models for cabinet doors and drawers without using artificial markers. Our approach uses a highly efficient and sampling-based approach to rectangle detection in depth images obtained from a self-developed active stereo system. The robot can use the generative models learned for the articulated objects to estimate their articulation type, their current configuration, and to make predictions about possible configurations not observed before. We present experiments carried out on real data obtained from our active stereo system. The results demonstrate that our technique is able to learn accurate articulation models. We furthermore provide a detailed error analysis based on ground truth data obtained in a motion capturing studio.},
annote = {
Fits rectangles to the planes and tracks them to define rigid parts of articulated objects whose kinematics are being learnt.

----------

RANSAC like method to fit planes to the pointcloud. Removes points that are fitted to a plane and continues.

----------

Enhanced stereo (with pattern projection) to achieve high quality reconstruction (2mm error within a metre)


 },
author = {Sturm, J{\"{u}}rgen and Konolige, Kurt and Stachniss, Cyrill and Burgard, Wolfram},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ROBOT.2010.5509985},
isbn = {9781424450381},
issn = {10504729},
keywords = {Cameras,Image segmentation,Iterative algorithms,Predictive models,Robot vision systems,Robotics and automation,Service robots,USA Councils,articulation models,artificial markers,cabinet doors,drawers,error analysis,ground truth data,home automation,household environments,mobile manipulation tasks,motion capturing studio,object detection,rectangle detection,robot vision,self developed active stereo system,stereo image processing,vision based detection},
mendeley-tags = {Cameras,Image segmentation,Iterative algorithms,Predictive models,Robot vision systems,Robotics and automation,Service robots,USA Councils,articulation models,artificial markers,cabinet doors,drawers,error analysis,ground truth data,home automation,household environments,mobile manipulation tasks,motion capturing studio,object detection,rectangle detection,robot vision,self developed active stereo system,stereo image processing,vision based detection},
month = {may},
pages = {362--368},
title = {{Vision-based detection for learning articulation models of cabinet doors and drawers in household environments}},
url = {http://ieeexplore.ieee.org/ielx5/5501116/5509124/05509985.pdf?tp={\&}arnumber=5509985{\&}isnumber=5509124 http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5509985{\&}navigation=1},
year = {2010}
}
@article{Sturm2011,
abstract = {Robots operating in domestic environments generally need to interact with articulated objects, such as doors, cabinets, dishwashers or fridges. In this work, we present a novel, probabilistic framework for modeling articulated objects as kinematic graphs. Vertices in this graph correspond to object parts, while edges between them model their kinematic relationship. In particular, we present a set of parametric and non-parametric edge models and how they can robustly be estimated from noisy pose observations. We furthermore describe how to estimate the kinematic structure and how to use the learned kinematic models for pose prediction and for robotic manipulation tasks. We finally present how the learned models can be generalized to new and previously unseen objects. In various experiments using real robots with different camera systems as well as in simulation, we show that our approach is valid, accurate and efficient. Further, we demonstrate that our approach has a broad set of applications, in particular for the emerging fields of mobile manipulation and service robotics.},
author = {Sturm, J{\"{u}}rgen and Stachniss, Cyrill and Burgard, Wolfram},
doi = {10.1613/jair.3229},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
month = {may},
number = {2},
pages = {477--526},
title = {{A probabilistic framework for learning kinematic models of articulated objects}},
url = {http://dl.acm.org/citation.cfm?id=2051237.2051252},
volume = {41},
year = {2011}
}
@article{Sun2016,
annote = {Classify objects using vision and touch independently, aiming to find a good grasping plan.

Objects: various 7-10},
author = {Sun, Fuchun and Liu, Chunfang and Huang, Wenbing and Zhang, Jianwei},
doi = {10.1109/TSMC.2016.2524059},
file = {:C$\backslash$:/Users/tadeo/Google Drive/phd/pdf/07448968 (1).pdf:pdf},
issn = {2168-2216},
journal = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
pages = {1--11},
title = {{Object Classification and Grasp Planning Using Visual and Tactile Sensing}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7448968},
year = {2016}
}
@inproceedings{Taddeucci1997,
abstract = {This paper presents an integrated approach to tactile perception, both in terms of data acquisition and data interpretation. In humans, touch sensing is implemented through a number of different sensing elements embedded in the skin. The interpretation of perceived data to the level of detection of basic features, such as material, shape of surface, shape of contact, is achieved by integrating the different sensorial inputs at a low level, with no involvement of high level cognitive processes. The approach we propose in this paper follows this anthropomorphic model of tactile perception, by including, on one hand, a miniature fingertip integrating different sensors and, on the other hand, a parallel data interpretation module implemented through a fuzzy neural-network which processes all the different inputs at the same level. The paper describes the characteristics of the integrated fingertip sensor and of the neuro-fuzzy system, and discusses experimental results achieved during exploratory tasks on a set of common objects are discussed in detail},
annote = {
Integrates vision, tact, heat and vibration sensors using a multi layer neural network. Classifies 14 different objects


Preprogrammed interaction. Images taken and classified in advance.

Sensor NNs are trained independently, taking hours to converge.},
author = {Taddeucci, D and Laschi, C and Lazzarini, R and Magni, R and Dario, P and Starita, A},
booktitle = {Proceedings of International Conference on Robotics and Automation},
doi = {10.1109/ROBOT.1997.606759},
isbn = {0-7803-3612-7},
keywords = {Anthropomorphism,Computer vision,Folder - Haptic-Visual integration,Fuzzy neural networks,Humans,Sensor phenomena and characterization,Sensor systems,Shape,Skin,anthropomorphic model,data acquisition,data interpretation,fingertip sensor,fuzzy neural nets,fuzzy neural-network,integrated tactile perception,knowledge representation,pattern recognition,robotics,robots,tactile sensors,touch sensing},
mendeley-tags = {Anthropomorphism,Computer vision,Folder - Haptic-Visual integration,Fuzzy neural networks,Humans,Sensor phenomena and characterization,Sensor systems,Shape,Skin,anthropomorphic model,data acquisition,data interpretation,fingertip sensor,fuzzy neural nets,fuzzy neural-network,integrated tactile perception,knowledge representation,pattern recognition,robotics,robots,tactile sensors,touch sensing},
number = {April},
pages = {3100--3105},
title = {{An approach to integrated tactile perception}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=606759},
volume = {4},
year = {1997}
}
@inproceedings{Takamuku2008,
abstract = {Skin is an essential component of artificial hands. It enables the use of object affordance for recognition and control, but due to its intrinsic locality and low density of current tactile sensors, stable and proper manual contacts with the objects are indispensable. Recently, design of hand structure have shown to be effective for adaptive grasping. However, such adaptive design are only introduced to the fingers in existing works of haptics and their role in recognition remains unclear. This paper introduces the design of the Bionic Hand; an anthropomorphic hand with adaptive design introduced to the whole hand and fully covered with sensitive skin. The experiment shows that anthropomorphic design of hand structure enables robust haptic recognition by convergence of object contact conditions into stable representative states through repetitive grasping. The structure of the human hand is found to solve the issue of narrowing down the sensor space for haptic object recognition by morphological computation.},
annote = {
Demonstrates that using a anthropomorphic hand, objects grasped tend to converge to a number of limited poses and perform a preliminary analysis using SOMs which suggests that sensory information converges as well, which reduces the problem of pose-independence via manipulation.
},
author = {Takamuku, Shinya and Fukuda, Atsushi and Hosoda, Koh},
booktitle = {2008 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS},
doi = {10.1109/IROS.2008.4651175},
isbn = {9781424420582},
keywords = {Fingers,Humans,Joints,Sensors,Skin,Strain,adaptive grasping,anthropomorphic hand structure design,anthropomorphic skin-covered hand,artificial hands,artificial limbs,bionic hand,dexterous manipulators,haptic interfaces,haptic object recognition,repetitive grasping,robust haptic recognition,tactile sensors},
mendeley-tags = {Fingers,Humans,Joints,Sensors,Skin,Strain,adaptive grasping,anthropomorphic hand structure design,anthropomorphic skin-covered hand,artificial hands,artificial limbs,bionic hand,dexterous manipulators,haptic interfaces,haptic object recognition,repetitive grasping,robust haptic recognition,tactile sensors},
pages = {3212--3217},
title = {{Repetitive grasping with anthropomorphic skin-covered hand enables robust haptic recognition}},
url = {http://ieeexplore.ieee.org/ielx5/4637508/4650570/04651175.pdf?tp={\&}arnumber=4651175{\&}isnumber=4650570 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4651175},
year = {2008}
}
@inproceedings{Tan2013,
abstract = {We present a novel real-time monocular SLAM system which can robustly work in dynamic environments. Different to the traditional methods, our system allows parts of the scene to be dynamic or the whole scene to gradually change. The key contribution is that we propose a novel online keyframe representation and updating method to adaptively model the dynamic environments, where the appearance or structure changes can be effectively detected and handled. We reliably detect the changed features by projecting them from the keyframes to current frame for appearance and structure comparison. The appearance change due to occlusions also can be reliably detected and handled. The keyframes with large changed areas will be replaced by newly selected frames. In addition, we propose a novel prior-based adaptive RANSAC algorithm (PARSAC) to efficiently remove outliers even when the inlier ratio is rather low, so that the camera pose can be reliably estimated even in very challenging situations. Experimental results demonstrate that the proposed system can robustly work in dynamic environments and outperforms the state-of-the-art SLAM systems (e.g. PTAM).},
author = {Tan, Wei and Liu, Haomin and Dong, Zilong and Zhang, Guofeng and Bao, Hujun},
booktitle = {2013 IEEE International Symposium on Mixed and Augmented Reality, ISMAR 2013},
doi = {10.1109/ISMAR.2013.6671781},
isbn = {9781479928699},
keywords = {Cameras,Feature extraction,I.4.8 [Image Processing and Computer Vision],Multimedia Information Systems. Artificial, augmen,PARSAC,Real-time systems,Robustness,SLAM (robots),Scene Analysis.Tracking; H.5.1 [Information Interf,Solid modeling,dynamic environments,iterative methods,online keyframe representation method,prior-based adaptive RANSAC algorithm,robot vision,robust monocular SLAM,simultaneous localization and mapping,three-dimensional displays},
mendeley-tags = {Cameras,Feature extraction,PARSAC,Real-time systems,Robustness,SLAM (robots),Solid modeling,dynamic environments,iterative methods,online keyframe representation method,prior-based adaptive RANSAC algorithm,robot vision,robust monocular SLAM,simultaneous localization and mapping,three-dimensional displays},
month = {oct},
pages = {209--218},
pmid = {6671781},
title = {{Robust monocular SLAM in dynamic environments}},
url = {http://ieeexplore.ieee.org/xpls/icp.jsp?arnumber=6671781},
year = {2013}
}
@inproceedings{Tan2013a,
abstract = {We present a novel real-time monocular SLAM system which can robustly work in dynamic environments. Different to the traditional methods, our system allows parts of the scene to be dynamic or the whole scene to gradually change. The key contribution is that we propose a novel online keyframe representation and updating method to adaptively model the dynamic environments, where the appearance or structure changes can be effectively detected and handled. We reliably detect the changed features by projecting them from the keyframes to current frame for appearance and structure comparison. The appearance change due to occlusions also can be reliably detected and handled. The keyframes with large changed areas will be replaced by newly selected frames. In addition, we propose a novel prior-based adaptive RANSAC algorithm (PARSAC) to efficiently remove outliers even when the inlier ratio is rather low, so that the camera pose can be reliably estimated even in very challenging situations. Experimental results demonstrate that the proposed system can robustly work in dynamic environments and outperforms the state-of-the-art SLAM systems (e.g. PTAM).},
annote = {
Make SLAM robust to moving large objects by tracking SIFT features to detect camera pose and removing moving points from the SLAM.
},
author = {Tan, Wei and Liu, Haomin and Dong, Zilong and Zhang, Guofeng and Bao, Hujun},
booktitle = {2013 IEEE International Symposium on Mixed and Augmented Reality, ISMAR 2013},
doi = {10.1109/ISMAR.2013.6671781},
isbn = {9781479928699},
keywords = {Cameras,Feature extraction,I.4.8 [Image Processing and Computer Vision],Multimedia Information Systems. Artificial, augmen,PARSAC,Real-time systems,Robustness,SLAM (robots),Scene Analysis.Tracking; H.5.1 [Information Interf,Solid modeling,dynamic environments,iterative methods,online keyframe representation method,prior-based adaptive RANSAC algorithm,robot vision,robust monocular SLAM,simultaneous localization and mapping,three-dimensional displays},
mendeley-tags = {Cameras,Feature extraction,PARSAC,Real-time systems,Robustness,SLAM (robots),Solid modeling,dynamic environments,iterative methods,online keyframe representation method,prior-based adaptive RANSAC algorithm,robot vision,robust monocular SLAM,simultaneous localization and mapping,three-dimensional displays},
month = {oct},
pages = {209--218},
pmid = {6671781},
title = {{Robust monocular SLAM in dynamic environments}},
url = {http://ieeexplore.ieee.org/ielx7/6661626/6671745/06671781.pdf?tp={\&}arnumber=6671781{\&}isnumber=6671745 http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6671781{\&}navigation=1},
year = {2013}
}
@incollection{Tang2013,
abstract = {We propose a feature, the Histogram of Oriented Normal Vectors (HONV), designed specifically to capture local geometric characteristics for object recognition with a depth sensor. Through our derivation, the normal vector orientation represented as an ordered pair of azimuthal angle and zenith angle can be easily computed from the gradients of the depth image. We form the HONV as a concatenation of local histograms of azimuthal angle and zenith angle. Since the HONV is inherently the local distribution of the tangent plane orientation of an object surface, we use it as a feature for object detection/classification tasks. The object detection experiments on the standard RGB-D dataset [1] and a self-collected Chair-D dataset show that the HONV significantly outperforms traditional features such as HOG on the depth image and HOG on the intensity image, with an improvement of 11.6{\%} in average precision. For object classification, the HONV achieved 5.0{\%} improvement over state-of-the-art approaches.},
annote = {HONV: introduces Histogram of Oriented Normal Vectors, a 3d feature which can be easily recovered from a depth image.----------Gives a proof for normal = $\backslash$vector3{\{}-$\backslash$dd{\{}d(x,y){\}}{\{}x{\}}{\}}{\{}-$\backslash$dd{\{}d(x,y){\}}{\{}y{\}}{\}}{\{}1{\}}, which can be discretised as 0.5*[d(x+1,y)-d(x-1,y); d(x,y+1)-d(x,y-1); 1]},
author = {Tang, Shuai and Wang, Xiaoyu and Lv, Xutao and Han, Tony X and Keller, James and He, Zhihai and Skubic, Marjorie and Lao, Shihong},
doi = {10.1007/978-3-642-37444-9_41},
editor = {Lee, Kyoung Mu and Matsushita, Yasuyuki and Rehg, James M. and Hu, Zhanyi},
isbn = {978-3-642-37443-2},
keywords = {Artificial Intelligence (incl. Robotics),Health Informatics,Image Processing and Computer Vision,pattern recognition},
mendeley-tags = {Artificial Intelligence (incl. Robotics),Health Informatics,Image Processing and Computer Vision,pattern recognition},
month = {jan},
pages = {525--538},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Histogram of Oriented Normal Vectors for Object Recognition with a Depth Sensor}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-37444-9{\_}41 http://link.springer.com/content/pdf/10.1007{\%}2F978-3-642-37444-9{\_}41.pdf},
year = {2013}
}
@incollection{Taylor2010,
abstract = {We address the problem of learning good features for understanding video data. We introduce a model that learns latent representations of image sequences from pairs of successive images. The convolutional architecture of our model allows it to scale to realistic image sizes whilst using a compact parametrization. In experiments on the NORB dataset, we show our model extracts latent "flow fields" which correspond to the transformation between the pair of input frames. We also use our model to extract low-level motion features in a multi-stage architecture for action recognition, demonstrating competitive performance on both the KTH and Hollywood2 datasets.},
address = {Berlin, Heidelberg},
author = {Taylor, Graham W and Fergus, Rob and Lecun, Yann and Bregler, Christoph},
booktitle = {Computer Vision – ECCV 2010},
doi = {10.1007/978-3-642-15567-3_11},
isbn = {978-3-642-15566-6},
issn = {03029743},
keywords = {Folder - convolutional,activity recognition,con-,convolutional nets,optical flow,restricted boltzmann machines,unsupervised learning,video analysis,volutional nets},
mendeley-tags = {Folder - convolutional,activity recognition,convolutional nets,optical flow,restricted boltzmann machines,unsupervised learning,video analysis},
pages = {140--153},
publisher = {Springer-Verlag},
series = {ECCV'10},
title = {{of Spatio-temporal Features}},
url = {http://dl.acm.org/citation.cfm?id=1888212.1888225},
year = {2010}
}
@article{Tenzer2012,
author = {Tenzer, Yaroslav and Jentoft, Leif and Howe, Robert},
doi = {10.1109/MRA.2014.2310152},
issn = {10709932},
journal = {IEEE R{\&}A Magazine},
number = {c},
pages = {2013},
title = {{{\textless}p{\textgreater}Inexpensive and Easily Customized Tactile Array Sensors using MEMS Barometers Chips{\textless}/p{\textgreater}}},
url = {https://biorobotics.harvard.edu/pubs/2012/journal/2012{\_}YTenzer{\_}BarometricSensors.pdf},
volume = {21},
year = {2012}
}
@article{Trinh2011,
abstract = {We describe a top-down object detection and segmentation approach that uses a skeleton-based shape model and that works directly on real images. The approach is based on three components. First, we propose a fragment-based generative model for shape that is based on the shock graph and has minimal dependency among its shape fragments. The model is capable of generating a wide variation of shapes as instances of a given object category. Second, we develop a progressive selection mechanism to search among the generated shapes for the category instances that are present in the image. The search begins with a large pool of candidates identified by a dynamic programming (DP) algorithm and progressively reduces it in size by applying series of criteria, namely, local minimum criterion, extent of shape overlap, and thresholding of the objective function to select the final object candidates. Third, we propose the Partitioned Chamfer Matching (PCM) measure to capture the support of image edges for a hypothesized shape. This measure overcomes the shortcomings of the Oriented Chamfer Matching and is robust against spurious edges, missing edges, and accidental alignment between the image edges and the shape boundary contour. We have evaluated our approach on the ETHZ dataset and found it to perform well in both object detection and object segmentation tasks.},
author = {Trinh, Nhon H. and Kimia, Benjamin B.},
doi = {10.1007/s11263-010-0412-0},
file = {:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Trinh, Kimia - 2011 - Skeleton search Category-specific object recognition and segmentation using a skeletal shape model.pdf:pdf;:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Trinh, Kimia - 2011 - Skeleton search Category-specific object recognition and segmentation using a skeletal shape model.html:html},
isbn = {1-901725-39-1},
issn = {09205691},
journal = {International Journal of Computer Vision},
keywords = {Artificial Intelligence (incl. Robotics),Categorization,Computer Imaging- Vision- Pattern Recognition and,Dynamic programming,Folder - constellation-star models - geometry over,Generative shape model,Image Processing and Computer Vision,Recognition,Segmentation,Shock graph,Skeleton,Top-down modeling,pattern recognition,recognition},
language = {en},
mendeley-tags = {Artificial Intelligence (incl. Robotics),Categorization,Computer Imaging- Vision- Pattern Recognition and,Dynamic programming,Folder - constellation-star models - geometry over,Generative shape model,Image Processing and Computer Vision,Segmentation,Shock graph,Skeleton,Top-down modeling,pattern recognition,recognition},
month = {jan},
number = {2},
pages = {215--240},
shorttitle = {Skeleton Search},
title = {{Skeleton search: Category-specific object recognition and segmentation using a skeletal shape model}},
url = {http://link.springer.com/article/10.1007/s11263-010-0412-0 http://link.springer.com/content/pdf/10.1007{\%}2Fs11263-010-0412-0.pdf},
volume = {94},
year = {2011}
}
@article{Tsochantaridis2005,
abstract = {Learning general functional dependencies between arbitrary input and output spaces is one of the key challenges in computational intelligence. While recent progress in machine learning has mainly focused on designing flexible and powerful input representations, this paper addresses the complementary issue of designing classification algorithms that can deal with more complex outputs, such as trees, sequences, or sets. More generally, we consider problems involving multiple dependent output variables, structured output spaces, and classification problems with class attributes. In order to accomplish this, we propose to appropriately generalize the well-known notion of a separation margin and derive a corresponding maximum-margin formulation. While this leads to a quadratic program with a potentially prohibitive, i.e. exponential, number of constraints, we present a cutting plane algorithm that solves the optimization problem in polynomial time for a large class of problems. The proposed method has important applications in areas such as computational biology, natural language processing, information retrieval/extraction, and optical character recognition. Experiments from various domains involving different types of output spaces emphasize the breadth and generality of our approach.},
author = {Tsochantaridis, Ioannis and Joachims, Thorsten and Hofmann, Thomas and Altun, Yasemin},
doi = {10.1007/s10994-008-5071-9},
file = {:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tsochantaridis et al. - 2005 - Large Margin Methods for Structured and Interdependent Output Variables.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research (JMLR)},
keywords = {Folder - SSVMs,maximum margin markove network},
mendeley-tags = {Folder - SSVMs},
month = {dec},
pages = {1453--1484},
title = {{Large Margin Methods for Structured and Interdependent Output Variables}},
url = {http://dl.acm.org/citation.cfm?id=1046920.1088722 http://www.google.co.uk/url?sa=t{\&}rct=j{\&}q={\&}esrc=s{\&}source=web{\&}cd=1{\&}ved=0CCgQFjAA{\&}url=http{\%}3A{\%}2F{\%}2Fwww.jmlr.org{\%}2Fpapers{\%}2Fvolume6{\%}2Ftsochantaridis05a{\%}2Ftsochantaridis05a.pdf{\&}ei=6mKIVPnlEOXV7QaHmYHQBw{\&}usg=AFQ},
volume = {6},
year = {2005}
}
@article{Tucker1958,
abstract = {Abstract The inter-battery method of factor analysis was devised to provide information relevant to the stability of factors over different selections of tests. Two batteries of tests, postulated to depend on the same common factors, but not parallel tests, are given to one ...$\backslash$n},
author = {Tucker, Ledyard R.},
doi = {10.1007/BF02289009},
isbn = {0033-3123},
issn = {00333123},
journal = {Psychometrika},
keywords = {Assessment- Testing and Evaluation,Folder - Haptic-Visual integration,Psychometrics,Statistical Theory and Methods,Statistics for Social Science- Behavorial Science-},
language = {en},
mendeley-tags = {Assessment- Testing and Evaluation,Folder - Haptic-Visual integration,Psychometrics,Statistical Theory and Methods,Statistics for Social Science- Behavorial Science-},
month = {jun},
number = {2},
pages = {111--136},
title = {{An inter-battery method of factor analysis}},
url = {http://link.springer.com/article/10.1007/BF02289009 http://link.springer.com/article/10.1007{\%}2FBF02289009},
volume = {23},
year = {1958}
}
@article{Vasconcelos2011,
abstract = {Cortical areas that directly receive sensory inputs from the thalamus were long thought to be exclusively dedicated to a single modality, originating separate labeled lines. In the past decade, however, several independent lines of research have demonstrated cross-modal responses in primary sensory areas. To investigate whether these responses represent behaviorally relevant information, we carried out neuronal recordings in the primary somatosensory cortex (S1) and primary visual cortex (V1) of rats as they performed whisker-based tasks in the dark. During the free exploration of novel objects, V1 and S1 responses carried comparable amounts of information about object identity. During execution of an aperture tactile discrimination task, tactile recruitment was slower and less robust in V1 than in S1. However, V1 tactile responses correlated significantly with performance across sessions. Altogether, the results support the notion that primary sensory areas have a preference for a given modality but can engage in meaningful cross-modal processing depending on task demand.},
annote = {Experiments on rats show that it is likely haptics and visual object recognition are related intrinsically, reading the response of parts of the brain dedicated to of visual and tactile object exploration.},
author = {Vasconcelos, Nivaldo and Pantoja, Janaina and Belchior, Hindiael and Caixeta, F{\'{a}}bio Viegas and Faber, Jean and Freire, Marco Aurelio M and Cota, Vin{\'{i}}cius Rosa and {Anibal de Macedo}, Edson and Laplagne, Diego a and Gomes, Herman Martins and Ribeiro, Sidarta},
doi = {10.1073/pnas.1102780108},
isbn = {1091-6490 (Electronic)$\backslash$r0027-8424 (Linking)},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Folder - Haptic-Visual integration - bio motivatio,computer grid,distributed processing,multielectrode,multisensory integration,pattern classification},
language = {en},
mendeley-tags = {Folder - Haptic-Visual integration - bio motivatio,computer grid,distributed processing,multielectrode,multisensory integration,pattern classification},
month = {sep},
number = {37},
pages = {15408--15413},
pmid = {21876148},
title = {{Cross-modal responses in the primary visual cortex encode complex objects and correlate with tactile discrimination.}},
url = {http://www.pnas.org/content/108/37/15408 http://www.ncbi.nlm.nih.gov/pubmed/21876148 http://www.pnas.org/content/108/37/15408.full http://www.pnas.org/content/108/37/15408.full.pdf},
volume = {108},
year = {2011}
}
@inproceedings{Vedula1999a,
abstract = {Just as optical flow is the two-dimensional motion of points in an image, scene flow is the three-dimensional motion of points in the world. The fundamental difficulty with optical flow is that only the normal flow can be computed directly from the image measurements, without some form of smoothing or regularization. In this paper, we begin by showing that the same fundamental limitation applies to scene flow; however, many cameras are used to image the scene. There are then two choices when computing scene flow: 1) perform the regularization in the images or 2) perform the regularization on the surface of the object in the scene. In this paper, we choose to compute scene flow using regularization in the images. We describe three algorithms, the first two for computing scene flow from optical flows and the third for constraining scene tructure from the inconsistencies in multiple optical flows.},
author = {Vedula, Sundar and Baker, Simon and Rander, Peter and Collins, Robert and Kanade, Takeo},
booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
doi = {10.1109/TPAMI.2005.63},
isbn = {0-7695-0164-8},
issn = {01628828},
keywords = {3D scene flow,Cameras,Ear,Folder - scene flow - dense scene flow,Layout,Motion estimation,Neutron spin echo,Normal flow,Optical computing,Optical flow,Read only memory,Scene flow,Smoothing methods,The brightness constancy constraint,Three-dimensional dense nonrigid motion,Three-dimensional normal flow,camera,classification,image motion analysis,image sequences,linear algorithms,optical flow,regularization,robots,smoothing,three-dimensional motion field,three-dimensional scene flow,two-dimensional motion field},
mendeley-tags = {3D scene flow,Cameras,Ear,Folder - scene flow - dense scene flow,Layout,Motion estimation,Neutron spin echo,Optical computing,Read only memory,Smoothing methods,camera,classification,image motion analysis,image sequences,linear algorithms,optical flow,regularization,robots,smoothing,three-dimensional motion field,three-dimensional scene flow,two-dimensional motion field},
number = {3},
pages = {475--480},
pmid = {15747803},
title = {{Three-dimensional scene flow}},
url = {http://ieeexplore.ieee.org/ielx5/6412/17141/00790293.pdf?tp={\&}arnumber=790293{\&}isnumber=17141 http://ieeexplore.ieee.org/xpl/abstractCitations.jsp?tp={\&}arnumber=790293{\&}contentType=Conference+Publications{\#}abstractCitations},
volume = {27},
year = {2005}
}
@inproceedings{Vedula1999,
abstract = {Just as optical flow is the two-dimensional motion of points in an image, scene flow is the three-dimensional motion of points in the world. The fundamental difficulty with optical flow is that only the normal flow can be computed directly from the image measurements, without some form of smoothing or regularization. In this paper, we begin by showing that the same fundamental limitation applies to scene flow; however, many cameras are used to image the scene. There are then two choices when computing scene flow: 1) perform the regularization in the images or 2) perform the regularization on the surface of the object in the scene. In this paper, we choose to compute scene flow using regularization in the images. We describe three algorithms, the first two for computing scene flow from optical flows and the third for constraining scene tructure from the inconsistencies in multiple optical flows.},
annote = {
Introduces dense scene flow, as 2d-{\textgreater}3d flow problem. Gives solutions for 3 cases, full accurate structure knowledge, binocular correspondence, no knowledge.

----------

stereo vision

----------

{\textless}span style="font-size: 100{\%}; font-family: arial,sans,sans-serif;"{\textgreater}Three-dimensional scene flow: introduces 3d dense scene flow (going from 2D flow to 3d flow)
},
author = {Vedula, Sundar and Baker, Simon and Rander, Peter and Collins, Robert and Kanade, Takeo},
booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
doi = {10.1109/TPAMI.2005.63},
isbn = {0-7695-0164-8},
issn = {01628828},
keywords = {3D scene flow,Cameras,Ear,Layout,Motion estimation,Neutron spin echo,Normal flow,Optical computing,Optical flow,Read only memory,Scene flow,Smoothing methods,The brightness constancy constraint,Three-dimensional dense nonrigid motion,Three-dimensional normal flow,camera,classification,image motion analysis,image sequences,linear algorithms,optical flow,regularization,robots,smoothing,three-dimensional motion field,three-dimensional scene flow,two-dimensional motion field},
mendeley-tags = {3D scene flow,Cameras,Ear,Layout,Motion estimation,Neutron spin echo,Optical computing,Read only memory,Smoothing methods,camera,classification,image motion analysis,image sequences,linear algorithms,optical flow,regularization,robots,smoothing,three-dimensional motion field,three-dimensional scene flow,two-dimensional motion field},
number = {3},
pages = {475--480},
pmid = {15747803},
title = {{Three-dimensional scene flow}},
url = {http://ieeexplore.ieee.org/ielx5/6412/17141/00790293.pdf?tp={\&}arnumber=790293{\&}isnumber=17141 http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp={\&}arnumber=790293{\&}contentType=Conference+Publications},
volume = {27},
year = {2005}
}
@inproceedings{Wangsiripitak2009,
abstract = {To work at video rate, the maps that monocular SLAM builds are bound to be sparse, making them sensitive to the erroneous inclusion of moving points and to the deletion of valid points through temporary occlusion. This paper describes the parallel implementation of monoSLAM with a 3D object tracker, allowing reasoning about moving objects and occlusion. The SLAM process provides the object tracker with information to register objects to the map's frame, and the object tracker allows the marking of features, either those on objects, or those created by their occluding edges, or those occluded by objects. Experiments are presented to verify the recovered geometry and to indicate the impact on camera pose in monoSLAM of including and avoiding moving features.},
annote = {
sparse


 
----------

Extends monocular SLAM by tracking convex hull of moving objects. Thus ignoring moving features and not looking for features that are believed to be occluded by a moving object.
},
author = {Wangsiripitak, Somkiat and Murray, David W.},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ROBOT.2009.5152290},
isbn = {9781424427895},
issn = {10504729},
keywords = {3D object tracker,Airborne radar,Cameras,Geometry,Laser radar,Layout,Radar tracking,Radiofrequency identification,Robot vision systems,SLAM (robots),Sonar navigation,Wearable sensors,image sensors,occluding edges,path planning,simultaneous localization and mapping,visual SLAM},
mendeley-tags = {3D object tracker,Airborne radar,Cameras,Geometry,Laser radar,Layout,Radar tracking,Radiofrequency identification,Robot vision systems,SLAM (robots),Sonar navigation,Wearable sensors,image sensors,occluding edges,path planning,simultaneous localization and mapping,visual SLAM},
month = {may},
pages = {375--380},
pmid = {5152290},
title = {{Avoiding moving outliers in visual slam by tracking moving objects}},
url = {http://ieeexplore.ieee.org/ielx5/5076472/5152175/05152290.pdf?tp={\&}arnumber=5152290{\&}isnumber=5152175 http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5152290{\&}navigation=1},
year = {2009}
}
@inproceedings{Weber2000,
abstract = {We propose a method to learn heterogeneous models of object$\backslash$nclasses for visual recognition. The training images contain a$\backslash$npreponderance of clutter and learning is unsupervised. Our models$\backslash$nrepresent objects as probabilistic constellations of rigid parts$\backslash$n(features). The variability within a class is represented by a join$\backslash$nprobability density function on the shape of the constellation and the$\backslash$nappearance of the parts. Our method automatically identifies distinctive$\backslash$nfeatures in the training set. The set of model parameters is then$\backslash$nlearned using expectation maximization. When trained on different,$\backslash$nunlabeled and unsegmented views of a class of objects, each component of$\backslash$nthe mixture model can adapt to represent a subset of the views.$\backslash$nSimilarly, different component models can also {\&}ldquo;specialize{\&}rdquo;$\backslash$non sub-classes of an object class. Experiments on images of human heads,$\backslash$nleaves from different species of trees, and motor-cars demonstrate that$\backslash$nthe method works well over a wide variety of objects},
author = {Weber, M. and Welling, M. and Perona, P.},
booktitle = {Proceedings IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No.PR00662)},
doi = {10.1109/CVPR.2000.854754},
file = {:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Weber, Welling, Perona - 2000 - Towards automatic discovery of object categories.pdf:pdf;:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Weber, Welling, Perona - 2000 - Towards automatic discovery of object categories.html:html},
isbn = {0-7695-0662-3},
issn = {1063-6919},
keywords = {Clutter,Detectors,Electrical capacitance tomography,Folder - constellation-star models,Footwear,Head,Humans,Labeling,Marine animals,Probability density function,Shape,heterogeneous models,learning,object categories,object classes,object detection,object recognition,training images,unsupervised,unsupervised learning,visual recognition},
mendeley-tags = {Clutter,Detectors,Electrical capacitance tomography,Folder - constellation-star models,Footwear,Head,Humans,Labeling,Marine animals,Probability density function,Shape,heterogeneous models,learning,object categories,object classes,object detection,object recognition,training images,unsupervised,unsupervised learning,visual recognition},
pages = {101--108},
title = {{Towards automatic discovery of object categories}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=854754},
volume = {2},
year = {2000}
}
@article{Wedel2011,
abstract = {Building upon recent developments in optical flow and stereo matching estimation, we propose a variational framework for the estimation of stereoscopic scene flow, i.e., the motion of points in the three-dimensional world from stereo image sequences. The proposed algorithm takes into account image pairs from two consecutive times and computes both depth and a 3D motion vector associated with each point in the image. In contrast to previous works, we partially decouple the depth estimation from the motion estimation, which has many practical advantages. The variational formulation is quite flexible and can handle both sparse or dense disparity maps. The proposed method is very efficient; with the depth map being computed on an FPGA, and the scene flow computed on the GPU, the proposed algorithm runs at frame rates of 20 frames per second on QVGA images (320240 pixels). Furthermore, we present solutions to two important problems in scene flow estimation: violations of intensity consistency between input images, and the uncertainty measures for the scene flow result.},
annote = {
Dense stereo correspondence

----------

Decouples stereo vision -{\textgreater} depth and depth flow -{\textgreater} scene flow to achieve online scene flow
},
author = {Wedel, Andreas and Brox, Thomas and Vaudrey, Tobi and Rabe, Clemens and Franke, Uwe and Cremers, Daniel},
doi = {10.1007/s11263-010-0404-0},
issn = {09205691},
journal = {International Journal of Computer Vision},
keywords = {3D motion,3D reconstruction,Artificial Intelligence (incl. Robotics),Computer Imaging- Vision- Pattern Recognition and,Folder - scene flow - dense scene flow,Image Processing and Computer Vision,Motion segmentation,Scene flow,Structure from motion,pattern recognition},
language = {en},
mendeley-tags = {3D motion,3D reconstruction,Artificial Intelligence (incl. Robotics),Computer Imaging- Vision- Pattern Recognition and,Folder - scene flow - dense scene flow,Image Processing and Computer Vision,Motion segmentation,Scene flow,Structure from motion,pattern recognition},
month = {oct},
number = {1},
pages = {29--51},
title = {{Stereoscopic scene flow computation for 3D motion understanding}},
url = {http://link.springer.com/article/10.1007/s11263-010-0404-0 http://link.springer.com/article/10.1007{\%}2Fs11263-010-0404-0},
volume = {95},
year = {2011}
}
@inproceedings{Weiss2005,
abstract = { Tactile sensors systems are very important for today's service robotics. Designed as a holohedral cover for a robot, they are suitable for collision detection when working in unstructured environments, for human-machine interaction or, with a high resolution, as object sensors enabling dexterous hands for reactive gripping. In this paper, we explain construction and working principle of resistive tactile sensor cells. The latter is based on the change of the electrical resistance between a conductive polymer and at least two electrodes. For this effect, we formulate a model to describe the dependence between the sensor's electrical resistance and the applied load. The model enables further improvements of resistive tactile sensor cells.},
annote = {
introduces DSA 9205

{\textless}table border="0" cellspacing="0"{\textgreater}{\textless}colgroup width="85"{\textgreater}{\textless}/colgroup{\textgreater}
{\textless}tbody{\textgreater}

{\textless}td height="16" align="LEFT"{\textgreater}DSA 9205 (Weiss 2005)

{\textless}/tbody{\textgreater}
},
author = {Weiss, K. and Worn, H.},
booktitle = {IEEE International Conference Mechatronics and Automation, 2005},
doi = {10.1109/ICMA.2005.1626593},
isbn = {0-7803-9044-X},
issn = {0022-3727},
keywords = {Capacitive sensors,Electric resistance,Electrodes,Optical polymers,Optical scattering,Optical sensors,Robot sensing systems,Service robots,Ultrafast optics,collision detection,dexterous manipulators,holohedral cover,human-machine interaction,intelligent sensors,object sensors,reactive gripping,resistive tactile sensor cells,service robotics,tactile sensors},
mendeley-tags = {Capacitive sensors,Electric resistance,Electrodes,Optical polymers,Optical scattering,Optical sensors,Robot sensing systems,Service robots,Ultrafast optics,collision detection,dexterous manipulators,holohedral cover,human-machine interaction,intelligent sensors,object sensors,reactive gripping,resistive tactile sensor cells,service robotics,tactile sensors},
number = {July},
pages = {471--476},
title = {{The working principle of resistive tactile sensor cells}},
url = {http://ieeexplore.ieee.org/ielx5/10831/34146/01626593.pdf?tp={\&}arnumber=1626593{\&}isnumber=34146 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1626593{\&}tag=1},
volume = {1},
year = {2005}
}
@article{Wettels2008,
abstract = {The performance of robotic and prosthetic hands in unstructured environments is severely limited by their having little or no tactile information compared to the rich tactile feedback of the human hand. We are developing a novel, robust tactile sensor array that mimics the mechanical properties and distributed touch receptors of the human fingertip. It consists of a rigid core surrounded by a weakly conductive fluid contained within an elastomeric skin. The sensor uses the deformable properties of the finger pad as part of the transduction process. Multiple electrodes are mounted on the surface of the rigid core and connected to impedance-measuring circuitry safely embedded within the core. External forces deform the fluid path around the electrodes, resulting in a distributed pattern of impedance changes containing information about those forces and the objects that applied them. Here we describe means to optimize the dynamic range of individual electrode sensors by texturing the inner surface of the silicone skin. Forces ranging from 0.1 to 30 N produced impedances ranging from 5 to 1000 k$\Omega$. Spatial resolution (below 2 mm) and frequency response (above 50 Hz) appeared to be limited only by the viscoelastic properties of the silicone elastomeric skin.},
annote = {
The BioTac is capable of sensing pressure distribution via changes in impedance measured at internal electrodes as the internal fluid that resides between them and the deformable outer is displaced due to contact. It is also capable of measuring heat transfer between itself and the contact object, which gives information about the material being touched. A dedicated fluid pressure sensor is used to sense total contact pressure, and the vibrations in this total pressure can be analysed to infer texture, when stroking a surface.
},
author = {Wettels, Nicholas and Santos, Veronica J. and Johansson, Roland S. and Loeb, Gerald E.},
doi = {10.1163/156855308X314533},
isbn = {0169-1864},
issn = {0169-1864},
journal = {Advanced Robotics},
number = {8},
pages = {829--849},
pmid = {258222500004},
title = {{Biomimetic Tactile Sensor Array}},
url = {http://www.tandfonline.com/doi/abs/10.1163/156855308X314533},
volume = {22},
year = {2008}
}
@inproceedings{Winstone2013,
abstract = {A versatile tactile sensor and artificial fingertip which exploits compliant materials and optical tracking. In comparison to previous MEMS sensors, the TACTIP device is especially suited to tasks for which humans use their fingertips; examples include object manipulation, contact sensing, pressure sensing and shear force detection. This is achieved whilst main- taining a high level of robustness. Previous development of the TACTIP device has proven the devices capability to measure force interaction and identify shape through edge detection. Here we present experimental re- sults which confirm the ability to also identify textures. This is achieved by measuring the vibration of the in-built human-like skin features in relation to textured profiles. Modifications to the mechanical design of the TACTIP are explored to increase the sensitivity to finer textured profiles. The results show that a contoured outer skin, similar to a finger print, increases the sensitivity of the device.},
annote = {record the vibration of the papillae and perform frequency analysis to classify artificial textures. Accuracy is improved significantly by adding an artificial fingerprint to the TACTIP},
author = {Winstone, Benjamin and Griffiths, Gareth and Pipe, Tony and Melhuish, Chris and Rossiter, Jonathon},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-39802-5-28},
file = {:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Winstone et al. - 2013 - TACTIP - Tactile fingertip device, texture analysis through optical tracking of skin features.pdf:pdf},
isbn = {9783642398018},
issn = {03029743},
pages = {323--334},
title = {{TACTIP - Tactile fingertip device, texture analysis through optical tracking of skin features}},
volume = {8064 LNAI},
year = {2013}
}
@inproceedings{Wolf2004,
abstract = {We propose an on-line algorithm for simultaneous localization and mapping of dynamic environments. Our algorithm is capable of differentiating static and dynamic parts of the environment and representing them appropriately on the map. Our approach is.....},
annote = {
laser range finder


 
----------

Maintains a static scene voxel grid and a dynamic scene voxel grid, with probabilities of occupancy. They are upgraded based on sensor readings. Most occupancies are assumed static. Dynamic detection is based on the disappearance of occupancy.


Occupancy is detected using a SICK lidar.

Localization is performed by simultaneous location and tracking of landmarks.},
author = {Wolf, D and Sukhatme, G},
booktitle = {Robotics and Automation, 2004. Proceedings. ICRA '04. 2004 IEEE International Conference on},
doi = {10.1109/ROBOT.2004.1308004},
isbn = {0780382323},
issn = {1050-4729},
keywords = {Bayesian methods,Computer science,Embedded system,Performance evaluation,Robot localization,Testing,dynamic environment mapping,mobile robots,object detection,occupancy grids,online algorithm,path planning,physical robots,simultaneous localization and mapping,static environment,static landmarks},
mendeley-tags = {Bayesian methods,Computer science,Embedded system,Performance evaluation,Robot localization,Testing,dynamic environment mapping,mobile robots,object detection,occupancy grids,online algorithm,path planning,physical robots,simultaneous localization and mapping,static environment,static landmarks},
month = {apr},
pages = {1301 ---- 1307 Vol.2},
title = {{Online simultaneous localization and mapping in dynamic environments}},
url = {http://ieeexplore.ieee.org/search/srchabstract.jsp?arnumber=1308004{\&}isnumber=29025{\&}punumber=9126{\&}k2dockey=1308004@ieeecnfs},
volume = {2},
year = {2004}
}
@article{Wu2013a,
abstract = {Recent years have witnessed extensive studies on distance metric learning (DML) for improving similarity search in multimedia information retrieval tasks. Despite their successes, most existing DML methods suffer from two critical limitations: (i) they typically attempt to learn a linear distance function on the input feature space, in which the assumption of linearity limits their capacity of measuring the similarity on complex patterns in real-world applications; (ii) they are often designed for learning distance metrics on uni-modal data, which may not effectively handle the similarity measures for multimedia objects with multimodal representations. To address these limitations, in this paper, we propose a novel framework of online multimodal deep similarity learning (OMDSL), which aims to optimally integrate multiple deep neural networks pretrained with stacked denoising autoencoder. In particular, the proposed framework explores a unified two-stage online learning scheme that consists of (i) learning a flexible nonlinear transformation function for each individual modality, and (ii) learning to find the optimal combination of multiple diverse modalities simultaneously in a coherent process. We conduct an extensive set of experiments to evaluate the performance of the proposed algorithms for multimodal image retrieval tasks, in which the encouraging results validate the effectiveness of the proposed technique.},
annote = {... or even taking different visual features are different modalities},
author = {Wu, Pengcheng and Hoi, Steven C.H. and Xia, Hao and Zhao, Peilin and Wang, Dayong and Miao, Chunyan},
doi = {10.1145/2502081.2502112},
isbn = {9781450324045},
journal = {Proceedings of the 21st ACM international conference on Multimedia - MM '13},
keywords = {deep learning,distance metric learning,image retrieval,online learning,similarity learning},
number = {iii},
pages = {153--162},
title = {{Online multimodal deep similarity learning with application to image retrieval}},
url = {http://doi.acm.org/10.1145/2502081.2502112$\backslash$nhttp://dl.acm.org/citation.cfm?doid=2502081.2502112},
year = {2013}
}
@article{Wu2013b,
abstract = {Recent years have witnessed extensive studies on distance metric learning (DML) for improving similarity search in multimedia information retrieval tasks. Despite their successes, most existing DML methods suffer from two critical limitations: (i) they typically attempt to learn a linear distance function on the input feature space, in which the assumption of linearity limits their capacity of measuring the similarity on complex patterns in real-world applications; (ii) they are often designed for learning distance metrics on uni-modal data, which may not effectively handle the similarity measures for multimedia objects with multimodal representations. To address these limitations, in this paper, we propose a novel framework of online multimodal deep similarity learning (OMDSL), which aims to optimally integrate multiple deep neural networks pretrained with stacked denoising autoencoder. In particular, the proposed framework explores a unified two-stage online learning scheme that consists of (i) learning a flexible nonlinear transformation function for each individual modality, and (ii) learning to find the optimal combination of multiple diverse modalities simultaneously in a coherent process. We conduct an extensive set of experiments to evaluate the performance of the proposed algorithms for multimodal image retrieval tasks, in which the encouraging results validate the effectiveness of the proposed technique.},
author = {Wu, Pengcheng and Hoi, Steven C.H. and Xia, Hao and Zhao, Peilin and Wang, Dayong and Miao, Chunyan},
doi = {10.1145/2502081.2502112},
isbn = {9781450324045},
journal = {Proceedings of the 21st ACM international conference on Multimedia - MM '13},
keywords = {deep learning,distance metric learning,image retrieval,online learning,similarity learning},
number = {iii},
pages = {153--162},
title = {{Online multimodal deep similarity learning with application to image retrieval}},
url = {http://doi.acm.org/10.1145/2502081.2502112$\backslash$nhttp://dl.acm.org/citation.cfm?doid=2502081.2502112},
year = {2013}
}
@incollection{Wu2014,
abstract = {Visual object classification and detection are major problems in contemporary computer vision. State-of-art algorithms allow thousands of visual objects to be learned and recognized, under a wide range of variations including lighting changes, occlusion, point of view and different object instances. Only a small fraction of the literature addresses the problem of variation in depictive styles (photographs, drawings, paintings etc.). This is a challenging gap but the ability to process images of all depictive styles and not just photographs has potential value across many applications. In this paper we model visual classes using a graph with multiple labels on each node; weights on arcs and nodes indicate relative importance (salience) to the object description. Visual class models can be learned from examples from a database that contains photographs, drawings, paintings etc. Experiments show that our representation is able to improve upon Deformable Part Models for detection and Bag of Words models for classification.},
author = {Wu, Qi and Cai, Hongping and Hall, Peter},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-10584-0_21},
editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
isbn = {9783319105833},
issn = {16113349},
keywords = {Artificial Intelligence (incl. Robotics),Computer Graphics,Deformable Models,Graph Matching,Image Processing and Computer Vision,Multi-labeled Graph,Object Recognition,object recognition,pattern recognition},
language = {en},
mendeley-tags = {Artificial Intelligence (incl. Robotics),Computer Graphics,Deformable Models,Graph Matching,Image Processing and Computer Vision,Multi-labeled Graph,object recognition,pattern recognition},
month = {jan},
number = {PART 7},
pages = {313--328},
publisher = {Springer International Publishing},
series = {Lecture Notes in Computer Science},
title = {{Learning graphs to model visual objects across different depictive styles}},
url = {http://link.springer.com/chapter/10.1007/978-3-319-10584-0{\_}21 http://link.springer.com/content/pdf/10.1007{\%}2F978-3-319-10584-0{\_}21.pdf},
volume = {8695 LNCS},
year = {2014}
}
@incollection{Wu2014a,
abstract = {Visual object classification and detection are major problems in contemporary computer vision. State-of-art algorithms allow thousands of visual objects to be learned and recognized, under a wide range of variations including lighting changes, occlusion, point of view and different object instances. Only a small fraction of the literature addresses the problem of variation in depictive styles (photographs, drawings, paintings etc.). This is a challenging gap but the ability to process images of all depictive styles and not just photographs has potential value across many applications. In this paper we model visual classes using a graph with multiple labels on each node; weights on arcs and nodes indicate relative importance (salience) to the object description. Visual class models can be learned from examples from a database that contains photographs, drawings, paintings etc. Experiments show that our representation is able to improve upon Deformable Part Models for detection and Bag of Words models for classification.},
annote = {It has been argued [Cai et al, The cross-depiction...] that a possible reason for this, is that looking at various low-dimensional representations of such a variety of depictions, the difference between images of a same object class of different depictive styles tends to be larger than the difference between images of different object classes of the same style. In particular, photograph images tend to occupy a narrow region in feature space compared to art images. This results in poor classification accuracy for a variety of method when training is performed on photos and testing is performed on art.
Part-based models have played a growing role in computer vision and have given good results in cross-depictive classification. To date, only one method, Multi-labelled-graphs, is resilient to the “train on photos, test on art” challenge described above},
author = {Wu, Qi and Cai, Hongping and Hall, Peter},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-10584-0_21},
editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
file = {:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu, Cai, Hall - 2014 - Learning graphs to model visual objects across different depictive styles.pdf:pdf;:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu, Cai, Hall - 2014 - Learning graphs to model visual objects across different depictive styles.html:html},
isbn = {9783319105833},
issn = {16113349},
keywords = {Artificial Intelligence (incl. Robotics),Computer Graphics,Deformable Models,Folder - cross-depiction,Graph Matching,Image Processing and Computer Vision,Multi-labeled Graph,Object Recognition,object recognition,pattern recognition},
language = {en},
mendeley-tags = {Artificial Intelligence (incl. Robotics),Computer Graphics,Deformable Models,Folder - cross-depiction,Graph Matching,Image Processing and Computer Vision,Multi-labeled Graph,object recognition,pattern recognition},
month = {sep},
number = {PART 7},
pages = {313--328},
publisher = {Springer International Publishing},
series = {Lecture Notes in Computer Science},
title = {{Learning graphs to model visual objects across different depictive styles}},
url = {http://link.springer.com/chapter/10.1007/978-3-319-10584-0{\_}21 http://link.springer.com/chapter/10.1007{\%}2F978-3-319-10584-0{\_}21 http://link.springer.com/content/pdf/10.1007{\%}2F978-3-319-10584-0{\_}21.pdf},
volume = {8695 LNCS},
year = {2014}
}
@inproceedings{Wu2012,
author = {Wu, Qi and Hall, Peter},
booktitle = {Procedings of the British Machine Vision Conference 2012},
doi = {10.5244/C.26.45},
isbn = {1-901725-46-4},
language = {en},
pages = {45.1--45.12},
publisher = {British Machine Vision Association},
title = {{Prime Shapes in Natural Images}},
url = {http://www.bmva.org/bmvc/2012/BMVC/paper045/index.html},
year = {2012}
}
@inproceedings{Xu2013,
abstract = {In order to endow robots with human-like tactile sensory abilities, they must be provided with tactile sensors and intelligent algorithms to select and control useful exploratory movements and interpret data from all available sensors. Current robotic systems do not possess such sensors or algorithms. In this study we integrate multimodal tactile sensing (force, vibration and temperature) from the BioTac{\textregistered} with a Shadow Dexterous Hand and program the robot to make exploratory movements similar to those humans make when identifying objects by their compliance, texture, and thermal properties. Signal processing strategies were developed to provide measures of these perceptual properties. When identifying an object, exploratory movements are intelligently selected using a process we have previously developed called Bayesian exploration [1], whereby exploratory movements that provide the most disambiguation between likely candidates of objects are automatically selected. The exploration algorithm was augmented with reinforcement learning whereby its internal representations of objects evolved according to its cumulative experience with them. This allowed the algorithm to compensate for drift in the performance of the anthropomorphic robot hand and the ambient conditions of testing, improving accuracy while reducing the number of exploratory movements required to identify an object. The robot correctly identified 10 different objects on 99 out of 100 presentations.},
address = {Karlsruhe, Germany},
annote = {chooses the EP which maximally differentiates between currently likely candidates.----------biotac},
author = {Xu, Danfei and Loeb, Gerald E. and Fishel, Jeremy A.},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2013.6631001},
isbn = {9781467356411},
issn = {10504729},
pages = {3056--3061},
title = {{Tactile identification of objects using Bayesian exploration}},
url = {file:///home/tadeo/x-drive/conferences/ICRA2013/media/files/papers{\_}videos/0049.pdf},
year = {2013}
}
@incollection{Xue2008,
abstract = {Support Vector Machine (SVM) is one of the most popular classifiers in pattern recognition, which aims to find a hyperplane that can separate two classes of samples with the maximal margin. As a result, traditional SVM usually more focuses on the scatter between classes, but neglects the different data distributions within classes which are also vital for an optimal classifier in different real-world problems. Recently, using as much structure information hidden in a given dataset as possible to help improve the generalization ability of a classifier has yielded a class of effective large margin classifiers, typically as Structured Large Margin Machine (SLMM). SLMM is generally derived by optimizing a corresponding objective function using SOCP, and thus in contrast to SVM developed from optimizing a QP problem, it, though more effective in classification performance, has the following shortcomings: 1) large time complexity; 2) lack of sparsity of solution, and 3) poor scalability to the size of the dataset. In this paper, still following the above line of the research, we develop a novel algorithm, termed as Structural Support Vector Machine (SSVM), by directly embedding the structural information into the SVM objective function rather than using as the constraints into SLMM, in this way, we achieve: 1) to overcome the above three shortcomings; 2) empirically better than or comparable generalization to SLMM, and 3) theoretically and empirically better generalization than SVM.},
annote = {The structure can be given a priori or retrieved with an unsupervised clustering algorithm. This is incorporated into the objective function as opposed to the constrains (as is the case with SLMM). Solution requires solving a QP using standard SVM framework which is generally faster than solving a (second order cone programming (SOCP)) for SSLM.----------Indtroduces SSVMs, which embed structure information about the data into the objective function fo the SVM, and are proven to better at generalising classficiation than SVMs.----------Inspired in SSLMs [Yeung 07]},
author = {Xue, Hui and Chen, Songcan and Yang, Qiang},
doi = {10.1007/978-3-540-87732-5_56},
editor = {Sun, Fuchun and Zhang, Jianwei and Tan, Ying and Cao, Jinde and Yu, Wen},
file = {:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xue, Chen, Yang - 2008 - Structural Support Vector Machine.pdf:pdf},
keywords = {Artificial Intelligence (incl. Robotics),Computation by Abstract Devices,Data Mining and Knowledge Discovery,Folder - SSVMs,Image Processing and Computer Vision,Rademacher complexity,Simulation and Modeling,Structural information,Support vector machine,and qiang yang 2,hui xue 1,pattern recognition,songcan chen 1,uctural support vector machine},
language = {en},
mendeley-tags = {Artificial Intelligence (incl. Robotics),Computation by Abstract Devices,Data Mining and Knowledge Discovery,Folder - SSVMs,Image Processing and Computer Vision,Rademacher complexity,Simulation and Modeling,Structural information,Support vector machine,pattern recognition},
month = {jan},
number = {60773061},
pages = {501--511},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Structural Support Vector Machine}},
url = {http://link.springer.com/chapter/10.1007/978-3-540-87732-5{\_}56 http://link.springer.com/content/pdf/10.1007{\%}2F978-3-540-87732-5{\_}56.pdf},
year = {2008}
}
@inproceedings{Yang2015,
annote = {Use weighted KNN to adjudicated a class label given vectors representing the tactile and visual input, obtaining a higher accuracy when both are combined rather than either used alone.

18 Objects
10 Images
10 Tactile sequences},
author = {Yang, Jingwei and Liu, Huaping and Sun, Fuchun and Gao, Meng},
booktitle = {2015 IEEE International Conference on Robotics and Biomimetics (ROBIO)},
doi = {10.1109/ROBIO.2015.7419024},
file = {:C$\backslash$:/Users/tadeo/Google Drive/phd/pdf/07419024.pdf:pdf},
isbn = {978-1-4673-9675-2},
keywords = {Haptic System,Robot Vision,Sensing},
month = {dec},
pages = {1746--1751},
publisher = {IEEE},
title = {{Object recognition using tactile and image information}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7419024},
year = {2015}
}
@article{Yeung2007,
abstract = {This paper proposes a new large margin classifier—the structured large margin machine (SLMM)—that is sensitive to the structure of the data distribution. The SLMM approach incorporates the merits of “structured” learning models, such as radial basis function networks and Gaussian mixture models, with the advantages of “unstructured” large margin learning schemes, such as support vector machines and maxi-min margin machines. We derive the SLMM model from the concepts of “structured degree” and “homospace”, based on an analysis of existing structured and unstructured learning models. Then, by using Ward's agglomerative hierarchical clustering on input data (or data mappings in the kernel space) to extract the underlying data structure, we formulate SLMM training as a sequential second order cone programming. Many promising features of the SLMM approach are illustrated, including its accuracy, scalability, extensibility, and noise tolerance. We also demonstrate the theoretical importance of the SLMM model by showing that it generalizes existing approaches, such as SVMs and M4s, provides novel insight into learning models, and lays a foundation for conceiving other “structured” classifiers. Editor:},
annote = {Uses clusters for optimisation. These are used to calculate the WMD between data and the decision hyperplane. This is used as a restriction in the optimisation problem.},
author = {Yeung, Daniel S. and Wang, Defeng and Ng, Wing W Y and Tsang, Eric C C and Wang, Xizhao},
doi = {10.1007/s10994-007-5015-9},
file = {:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yeung et al. - 2007 - Structured large margin machines Sensitive to data distributions.pdf:pdf;:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yeung et al. - 2007 - Structured large margin machines Sensitive to data distributions.html:html},
issn = {08856125},
journal = {Machine Learning},
keywords = {Agglomerative hierarchical clustering,Folder - SSVMs,Homospace,Large margin learning,Second order cone programming (SOCP),Structured learning,Weighted Mahalanobis distance (WMD)},
language = {en},
mendeley-tags = {Folder - SSVMs},
month = {aug},
number = {2},
pages = {171--200},
shorttitle = {Structured large margin machines},
title = {{Structured large margin machines: Sensitive to data distributions}},
url = {http://link.springer.com/article/10.1007/s10994-007-5015-9 http://link.springer.com/article/10.1007{\%}2Fs10994-007-5015-9 http://link.springer.com/content/pdf/10.1007{\%}2Fs10994-007-5015-9.pdf},
volume = {68},
year = {2007}
}
@article{Zernike1934,
abstract = {Auf Grundlage der A b b eschen Beugungstheorie der optischen Abbildung wird das Aussehen eines Hohlspiegels mit willk{\"{u}}rlich verlaufenden kleinen Abweichungen beim Foucaul tschen Schneidenverfahren und beim neuen Phasenkontrastverfahren berechnet. Es werden die orthogonalen “Kreisf{\"{a}}chenpolynome” gefunden und auf die Beugungserscheinungen beim kreisf{\"{o}}rmigen Spiegel angewendet.},
author = {Zernike, F.},
doi = {10.1016/S0031-8914(34)80259-5},
isbn = {0031-8914},
issn = {00318914},
journal = {Physica},
keywords = {diffraction,optics},
mendeley-tags = {diffraction,optics},
month = {may},
number = {7-12},
pages = {689--704},
title = {{Beugungstheorie des schneidenver-fahrens und seiner verbesserten form, der phasenkontrastmethode}},
url = {http://www.sciencedirect.com/science/article/pii/S0031891434802595},
volume = {1},
year = {1934}
}
@inproceedings{Zhang2010,
abstract = {Bag-of-Words (BOW) models have recently become popular for the task of object recognition, owing to their good performance and simplicity. Much work has been proposed over the years to improve the BOW model, where the Spatial Pyramid Matching technique is the most notable. In this work, we propose three novel techniques to capture more refined spatial information between image features than that provided by the Spatial Pyramids. Our techniques demonstrate a performance gain over the Spatial Pyramid representation of the BOW model.},
author = {Zhang, Edmond and Mayo, Michael},
booktitle = {International Conference Image and Vision Computing New Zealand},
doi = {10.1109/IVCNZ.2010.6148795},
file = {:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Mayo - 2010 - Improving bag-of-words model with spatial information.html:html},
isbn = {9781424496303},
issn = {21512191},
keywords = {BOW model,Bags-of-words,Folder - constellation-star models - geometry over,Histograms,Object Recognition,SIFT Keypoints,Shape,Spatial Pyramid Matching,Training,Vectors,Visualization,Vocabulary,bag-of-words model,image features,information retrieval,object recognition,spatial information,spatial pyramid matching technique,spatial pyramid representation},
mendeley-tags = {BOW model,Bags-of-words,Folder - constellation-star models - geometry over,Histograms,SIFT Keypoints,Shape,Spatial Pyramid Matching,Training,Vectors,Visualization,Vocabulary,bag-of-words model,image features,information retrieval,object recognition,spatial information,spatial pyramid matching technique,spatial pyramid representation},
month = {nov},
pages = {1--8},
title = {{Improving bag-of-words model with spatial information}},
url = {http://ieeexplore.ieee.org/xpls/icp.jsp?arnumber=6148795{\#}fig{\_}1},
year = {2010}
}
@inproceedings{Zhu2010,
abstract = {We present a latent hierarchical structural learning method for object detection. An object is represented by a mixture of hierarchical tree models where the nodes represent object parts. The nodes can move spatially to allow both local and global shape deformations. The models can be trained discriminatively using latent structural SVM learning, where the latent variables are the node positions and the mixture component. But current learning methods are slow, due to the large number of parameters and latent variables, and have been restricted to hierarchies with two layers. In this paper we describe an incremental concave-convex procedure (iCCCP) which allows us to learn both two and three layer models efficiently. We show that iCCCP leads to a simple training algorithm which avoids complex multi-stage layer-wise training, careful part selection, and achieves good performance without requiring elaborate initialization. We perform object detection using our learnt models and obtain performance comparable with state-of-the-art methods when evaluated on challenging public PASCAL datasets. We demonstrate the advantages of three layer hierarchies - outperforming Felzenszwalb et al.'s two layer models on all 20 classes.},
author = {Zhu, Long and Chen, Yuanhao and Yuille, Alan and Freeman, William},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2010.5540096},
file = {:C$\backslash$:/Users/tadeo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhu et al. - 2010 - Latent hierarchical structural learning for object detection.html:html},
isbn = {9781424469840},
issn = {10636919},
keywords = {Computer vision,Context modeling,Folder - SSVMs - Applications,Joining processes,Performance evaluation,SVM learning,Shape,Statistics,Training data,concave programming,convex programming,global shape deformations,hierarchical tree models,incremental concave-convex procedure,latent hierarchical structural learning method,learning (artificial intelligence),learning systems,local shape deformations,mixture component,node positions,object detection,support vector machines,trees (mathematics)},
mendeley-tags = {Computer vision,Context modeling,Folder - SSVMs - Applications,Joining processes,Performance evaluation,SVM learning,Shape,Statistics,Training data,concave programming,convex programming,global shape deformations,hierarchical tree models,incremental concave-convex procedure,latent hierarchical structural learning method,learning (artificial intelligence),learning systems,local shape deformations,mixture component,node positions,object detection,support vector machines,trees (mathematics)},
month = {jun},
pages = {1062--1069},
title = {{Latent hierarchical structural learning for object detection}},
url = {http://ieeexplore.ieee.org/xpls/icp.jsp?arnumber=5540096},
year = {2010}
}
@misc{Zienkiewicz2013,
abstract = {Abstract We present a technique whereby a single camera can be used as a high precision visual odometry sensor in a range of practical settings using simple, computationally efficient techniques. Taking advantage of the local planarity of common floor surfaces, we use real- ...},
author = {Zienkiewicz, Jacek and Lukierski, Robert and Davison, Andrew},
booktitle = {Procedings of the British Machine Vision Conference 2013},
doi = {10.5244/C.27.95},
isbn = {1-901725-49-9},
pages = {94.1--94.11},
title = {{Dense, Auto-Calibrating Visual Odometry from a Downward-Looking Camera}},
url = {http://www.bmva.org/bmvc/2013/Papers/paper0095/index.html},
urldate = {2014-10-09},
year = {2013}
}
